{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import cv2\n",
    "import math\n",
    "import time\n",
    "import json\n",
    "import carla\n",
    "import heapq\n",
    "import torch\n",
    "import random\n",
    "import requests\n",
    "import kagglehub\n",
    "import transformers\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision import models\n",
    "from collections import Counter\n",
    "from sklearn.decomposition import PCA\n",
    "from skimage.morphology import skeletonize\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from huggingface_hub import hf_hub_download\n",
    "from transformers.image_transforms import rgb_to_id\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from transformers import SegformerForSemanticSegmentation, SegformerConfig, CLIPProcessor, CLIPModel, AutoModelForSeq2SeqLM, AutoTokenizer, AutoImageProcessor, DetrForObjectDetection, DetrForSegmentation, TimeSeriesTransformerForPrediction, TimeSeriesTransformerConfig, pipeline\n",
    "\n",
    "from torchvision import transforms as A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A1 = \"VL pretrained-M fine-tuned with semi-disentangled outputs\"\n",
    "A2 = \"Future Trajectory Prediction by LSTM using Multi Modal Inputs\"\n",
    "A3 = \"Future Waypoint Prediction using BEV representations and GPS mechanism\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ALGORITHM 1: VL pretrained-M fine-tuned with semi-disentangled outputs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "semantic_label_classes = [\"Traffic Sign\", \"Building\", \"Fence\", \"Other\", \"Pedestrian\", \"Pole\", \"Road Line\", \"Road\", \"Sidewalk\", \"Vegetation\", \"Car\", \"Wall\", \"Unlabeled\"]\n",
    "\n",
    "root = \"datasets\"\n",
    "images_dir = f\"{root}/images\"\n",
    "labels_dir = f\"{root}/labels\"\n",
    "waypoint_file = \"A1_dataset.json\"\n",
    "batch_size = 2\n",
    "\n",
    "skip = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Creation (IMAGE to EMBEDDING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = kagglehub.dataset_download(\"albertozorzetto/carla-densely-annotated-driving-dataset\")\n",
    "# print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Download the CARLA dataset using the Kaggle API\n",
    "# dataset_name = \"albertozorzetto/carla-densely-annotated-driving-dataset\"\n",
    "# destination_path = \"/content/datasets\"\n",
    "\n",
    "# import os\n",
    "# # Install Kaggle API\n",
    "# !pip install kaggle --upgrade\n",
    "# # Upload kaggle.json for authentication\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "# # Move kaggle.json to the proper directory\n",
    "# !mkdir -p ~/.kaggle\n",
    "# !mv kaggle.json ~/.kaggle/\n",
    "# !chmod 600 ~/.kaggle/kaggle.json\n",
    "# # Verify Kaggle is set up correctly\n",
    "# !kaggle datasets list\n",
    "# !kaggle datasets download -d {dataset_name} -p {destination_path} --unzip\n",
    "# print(\"Path to dataset files:\", destination_path)\n",
    "\n",
    "# import tarfile\n",
    "# images_path = os.path.join(destination_path, \"images\")\n",
    "# labels_path = os.path.join(destination_path, \"labels\")\n",
    "# # Create directories for extracted files\n",
    "# os.makedirs(images_path, exist_ok=True)\n",
    "# os.makedirs(labels_path, exist_ok=True)\n",
    "# # Extract and manage .tar files\n",
    "# tar_files = [f for f in os.listdir(destination_path) if f.endswith('.tar')]\n",
    "# for tar_file in tar_files:\n",
    "#     tar_path = os.path.join(destination_path, tar_file)\n",
    "#     print(f\"Extracting {tar_path}...\")\n",
    "#     with tarfile.open(tar_path) as tar:\n",
    "#         # Determine the folder to extract based on the tar file name\n",
    "#         if \"images\" in tar_file.lower():\n",
    "#             tar.extractall(path=images_path)\n",
    "#         elif \"labels\" in tar_file.lower():\n",
    "#             tar.extractall(path=labels_path)\n",
    "#         else:\n",
    "#             print(f\"Skipping unknown .tar file: {tar_file}\")\n",
    "#     os.remove(tar_path)  # Optional: Remove the .tar file after extraction\n",
    "# print(\"All .tar files have been extracted.\")\n",
    "# print(f\"Images folder: {images_path}\")\n",
    "# print(f\"Labels folder: {labels_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Saving the {way points} for passing to the LLM*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triangular_condition(x_check, y_check):\n",
    "    margin = 250\n",
    "    n = 5\n",
    "    if margin <= x_check <= 800 - margin:\n",
    "        y_inc = (n * x_check / 8) if x_check < 400 else ((n * 100) - (n * x_check / 8))\n",
    "        if y_check <= y_inc:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_main_waypoint(waypoints, skeleton):\n",
    "    ego_pos = [400,600]\n",
    "    if len(waypoints) == 0:\n",
    "        return None\n",
    "\n",
    "    filtered_points = []\n",
    "    for x,y in waypoints:\n",
    "        if triangular_condition(x,600 - y):\n",
    "            # print(x,y)\n",
    "            filtered_points.append((x,y)) \n",
    "\n",
    "    # plt.imshow(skeleton, cmap=\"gray\")\n",
    "    # plt.grid(True)\n",
    "    # for idx in range(0, len(filtered_points)):\n",
    "    #     x,y = filtered_points[idx]\n",
    "    #     plt.scatter(x, y, c=\"blue\", marker=\"o\")\n",
    "    # plt.show()\n",
    "\n",
    "    min_d_idx = None\n",
    "    min_d = float('inf')\n",
    "    for idx in range(0, len(filtered_points)):\n",
    "        d = (((x - ego_pos[0]) ** 2) + ((y - ego_pos[1]) ** 2)) ** (1 / 2)\n",
    "        if min_d > d:\n",
    "            min_d_idx = idx\n",
    "            min_d = d\n",
    "\n",
    "    if filtered_points == []:\n",
    "        return ego_pos\n",
    "    for val in filtered_points:\n",
    "        if val[0] > 390 and val[0] < 410:\n",
    "            return float(val[0]), float(val[1])\n",
    "    if len(filtered_points) <=2:\n",
    "        return float(ego_pos[0]), float(ego_pos[1])\n",
    "    mean_x = sum(ele[0] for ele in filtered_points) / len(filtered_points)\n",
    "    mean_y = sum(ele[1] for ele in filtered_points) / len(filtered_points)\n",
    "    return float(mean_x), float(mean_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_waypoints(label, num_waypoints):\n",
    "    if len(label.shape) == 3:\n",
    "        label = cv2.cvtColor(label, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    road_mask = (label == 90).astype(np.uint8)\n",
    "    \n",
    "    contours, _ = cv2.findContours(road_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if not contours:\n",
    "        return []\n",
    "    \n",
    "    largest_contour = max(contours, key=cv2.contourArea)\n",
    "    binary_image = (road_mask > 0).astype(np.uint8)\n",
    "    skeleton = skeletonize(binary_image).astype(np.uint8)\n",
    "    \n",
    "    y_coords, x_coords = np.where(skeleton > 0)\n",
    "    path_points = np.column_stack((x_coords, y_coords))\n",
    "    \n",
    "    x_min, x_max = path_points[:, 0].min(), path_points[:, 0].max()\n",
    "    y_min, y_max = path_points[:, 1].min(), path_points[:, 1].max()\n",
    "\n",
    "    x_range = (x_max - x_min) * 0.25\n",
    "    y_range = (y_max - y_min) * 0.25\n",
    "\n",
    "    # path_points = path_points[\n",
    "    #     (path_points[:, 0] > x_min + x_range) & (path_points[:, 0] < x_max - x_range) &\n",
    "    #     (path_points[:, 1] > y_min + y_range) & (path_points[:, 1] < y_max - y_range)\n",
    "    # ]\n",
    "\n",
    "    if len(path_points) == 0:\n",
    "        return []\n",
    "\n",
    "    sampled_indices = np.linspace(0, len(path_points) - 1, num_waypoints, dtype=int)\n",
    "    waypoints = path_points[sampled_indices]\n",
    "\n",
    "    num_selected_waypoints = min(25, len(path_points))\n",
    "    sampled_indices = np.linspace(0, len(path_points) - 1, num_selected_waypoints, dtype=int)\n",
    "    waypoints = path_points[sampled_indices]\n",
    "    \n",
    "    # print(\"Min/Max path points:\", path_points.min(), path_points.max())\n",
    "\n",
    "    # plt.imshow(skeleton, cmap=\"gray\")\n",
    "    # plt.scatter(x_coords, y_coords, c=\"red\", marker=\"o\")\n",
    "    # plt.show()\n",
    "\n",
    "    main_waypoint = get_main_waypoint(waypoints, skeleton)\n",
    "    # print(\"Main Waypoint:\", main_waypoint)\n",
    "\n",
    "    waypoints = main_waypoint\n",
    "\n",
    "    # print(main_waypoint[0], main_waypoint[1])\n",
    "\n",
    "    # if len(waypoints) > 0:\n",
    "    #     plt.figure(figsize=(10, 10))\n",
    "    #     plt.imshow(label, cmap=\"gray\")\n",
    "    #     x = waypoints[0]\n",
    "    #     y = waypoints[1]\n",
    "    #     plt.scatter(x, y, c='red', marker='o')\n",
    "    #     plt.text(x, y, f'({x:.1f}, {y:.1f})', fontsize=8, color='yellow', ha='right', va='bottom')\n",
    "    #     plt.title(\"Extracted Waypoints\")\n",
    "    #     plt.show()\n",
    "\n",
    "    return waypoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(labels_dir, output_dir, num_waypoints):\n",
    "\n",
    "    for i in range(0, 28):\n",
    "        k = \"0\" if i <= 9 else \"\"\n",
    "        curr_folder_labels = f\"{labels_dir}/Video_0{k}{i}\"\n",
    "        labels_path = [file for file in os.listdir(curr_folder_labels) if file.endswith(\".png\")]\n",
    "        labels_path.sort()\n",
    "        labels_path = labels_path[::skip]\n",
    "\n",
    "        for label_file in labels_path:\n",
    "            label_path = os.path.join(curr_folder_labels, label_file)\n",
    "            label = Image.open(label_path)\n",
    "            label_np = np.array(label)\n",
    "            waypoints = generate_waypoints(label_np, num_waypoints=num_waypoints)\n",
    "            if waypoints == []:\n",
    "                waypoints = [400,600]\n",
    "            output_label_path = os.path.join(output_dir, label_file)\n",
    "            dataset_waypoints = [{\n",
    "                \"label\": output_label_path,\n",
    "                \"waypoints\": waypoints\n",
    "            }]\n",
    "            # break\n",
    "\n",
    "            with open(os.path.join(output_dir, waypoint_file), 'a') as wf:\n",
    "                json.dump(dataset_waypoints, wf)\n",
    "                wf.write(\"\\n\")\n",
    "                # break\n",
    "\n",
    "        if(i<27):\n",
    "            print(f\"\\rProgress: {((i + 1) / 0.27):.2f} %\", end = \"\")\n",
    "        # break\n",
    "\n",
    "    print(f\"\\nDone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_dataset(labels_dir, root, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_the_wps_A1(labels_dir, output_dir, num_waypoints):\n",
    "    dataset_waypoints = []\n",
    "    folder_name = f\"{labels_dir}/Video_000\"\n",
    "    file_1 = os.path.join(folder_name, \"v000_0000.png\")\n",
    "\n",
    "    folder_name = f\"{labels_dir}/Video_007\"\n",
    "    file_2 = os.path.join(folder_name, \"v007_0019.png\")\n",
    "\n",
    "    generate_waypoints(np.array(Image.open(file_1)), num_waypoints = num_waypoints)\n",
    "    generate_waypoints(np.array(Image.open(file_2)), num_waypoints = num_waypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_the_wps_A1(labels_dir, root, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "getting dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, images_dir):\n",
    "        self.data = []\n",
    "\n",
    "        for i in range(0, 28):\n",
    "            k = \"0\" if i <= 9 else \"\"\n",
    "            curr_folder_images = f\"{images_dir}/Video_0{k}{i}\"\n",
    "            images_path = [file for file in os.listdir(curr_folder_images) if file.endswith(\".png\")]\n",
    "            images_path.sort()\n",
    "            images_path = images_path[::skip]\n",
    "\n",
    "            for image_file in images_path:\n",
    "                image_path = os.path.join(curr_folder_images, image_file)\n",
    "                image = Image.open(image_path)\n",
    "                image_embeddings = processor(text=semantic_label_classes, images=image, return_tensors=\"pt\", padding=True)\n",
    "                ei = image_embeddings['pixel_values']\n",
    "\n",
    "                with open(os.path.join(root, waypoint_file), \"r\") as wf:\n",
    "                    data_list = wf.read().splitlines()\n",
    "                    for data in data_list:\n",
    "                        data = json.loads(data)[0]\n",
    "                        if data['label'] == f\"datasets\\\\{image_file}\":\n",
    "                            corresponding_waypoints = data['waypoints']\n",
    "                            cw = corresponding_waypoints\n",
    "                            # print('match found')\n",
    "                            self.data.append((ei,cw))\n",
    "                            break\n",
    "                break # for first image in all folders\n",
    "            # break\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        embeddings, waypoints = self.data[idx]\n",
    "        return embeddings, waypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split indices for train and validation\n",
    "dataset = SegmentationDataset(images_dir)\n",
    "remaining_indices, test_indices = train_test_split(range(len(dataset)), test_size=0.1, random_state=42)\n",
    "\n",
    "test_dataset = Subset(dataset, test_indices)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, drop_last=True)\n",
    "\n",
    "train_indices, val_indices = train_test_split(remaining_indices, test_size=0.3, random_state=42)\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(pixel_values):\n",
    "    mini = pixel_values.min()\n",
    "    maxi = pixel_values.max()\n",
    "    pixel_values = (pixel_values - mini) / (maxi - mini)\n",
    "    return pixel_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OUR VIT WP MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VitWPModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VitWPModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(512)\n",
    "        self.conv3 = nn.Conv2d(512, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "        self.conv5 = nn.Conv2d(512, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 512)\n",
    "        self.fc2 = nn.Linear(512, 2)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        b = pixel_values.shape[0]\n",
    "        x = torch.relu(self.conv1(pixel_values))\n",
    "        x = self.bn1(x)\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        \n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.bn2(x)\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = self.bn3(x)\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "\n",
    "        x = torch.relu(self.conv4(x))\n",
    "        x = self.bn4(x)\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "\n",
    "        x = torch.relu(self.conv5(x))\n",
    "        x = self.bn5(x)\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        output = self.fc2(x)\n",
    "        return output.view(b,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_train = VitWPModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*--training set ups--*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wp_generation(pixel_values, model):\n",
    "    pixel_values = pixel_values.squeeze(1)\n",
    "    outputs = model(pixel_values)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model_to_train, train_loader, optimizer, criterion):\n",
    "    model_to_train.train()\n",
    "    running_loss = 0.0\n",
    "    running_accuracy = 0.0\n",
    "\n",
    "    for data in train_loader:\n",
    "        p_val = data[0]\n",
    "        target = torch.stack(data[1], dim=1)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = wp_generation(p_val, model_to_train)\n",
    "        loss = criterion(outputs.float(), target.float())\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # break\n",
    "    return running_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model_to_validate, val_loader, optimizer, criterion):\n",
    "    model_to_validate.eval()\n",
    "    running_loss = 0.0\n",
    "    running_accuracy = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            p_val = data[0]\n",
    "            target = torch.stack(data[1], dim=1)\n",
    "\n",
    "            outputs = wp_generation(p_val, model_to_validate)\n",
    "            loss = criterion(outputs.float(), target.float())\n",
    "            running_loss += loss.item()\n",
    "            # break\n",
    "    return running_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model_to_test, dataloader, optimizer, criterion):\n",
    "    model_to_test.eval()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            p_val = data[0]\n",
    "            target = torch.stack(data[1], dim=1)\n",
    "\n",
    "            outputs = wp_generation(p_val, model_to_test)\n",
    "            print(outputs)\n",
    "            print(target)\n",
    "            loss = criterion(outputs.float(), target.float())\n",
    "            running_loss += loss.item()\n",
    "            break\n",
    "    return running_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_save = best_model_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_to_save, 'models/VitWPModel.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_model = model_to_save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ALGORITHM 2: Future Trajectory Prediction by LSTM using Multi Modal Inputs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_processor = AutoImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "detr_model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "\n",
    "root = \"datasets\"\n",
    "images_dir = f\"{root}/images\"\n",
    "labels_dir = f\"{root}/labels\"\n",
    "bev_file = \"A2_dataset_m.json\"\n",
    "batch_size = 1\n",
    "\n",
    "skip = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Creation (IMAGE to TEMPORAL DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "image > depth map and object detection > bird's eye view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_depth_map(image):\n",
    "    estimator = pipeline(task=\"depth-estimation\", model=\"Intel/dpt-large\")\n",
    "    result = estimator(images=image)\n",
    "    depth_map = result['predicted_depth']\n",
    "    return depth_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_depth_map(depth_map, cord_x = 400, cord_y = 75, c = 'white'):\n",
    "    depth_map = depth_map.max() - depth_map\n",
    "    height, width = depth_map.shape\n",
    "    depth_at_cord = depth_map[cord_y-1, cord_x-1]\n",
    "    print(f\"Depth at co-ordinate ({cord_x}, {cord_y}) of the image: {depth_at_cord:.2f} meters\")\n",
    "    plt.scatter(cord_x, cord_y, color=c, marker='x')\n",
    "    plt.imshow(depth_map, cmap = 'inferno')\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def object_detection(image):\n",
    "    inputs = image_processor(images=image, return_tensors=\"pt\")\n",
    "    outputs = detr_model(**inputs)\n",
    "    target_sizes = torch.tensor([image.size[::-1]])\n",
    "    objects = image_processor.post_process_object_detection(outputs, threshold=0.9, target_sizes=target_sizes)[0]\n",
    "\n",
    "    return objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def future_positions(objects, alpha = 0.1):\n",
    "    trajectories = []\n",
    "    \n",
    "    for obj in objects:\n",
    "        smoothed_positions = [obj]\n",
    "        for t in range(0, len(obj)):\n",
    "            smoothed_pos = alpha * obj[t] + (1 - alpha) * smoothed_positions[-1]\n",
    "            smoothed_positions.append(smoothed_pos)\n",
    "        future_positions = [smoothed_positions[-1]]\n",
    "        trajectories.append(future_positions)\n",
    "\n",
    "    new = []\n",
    "    for i in trajectories:\n",
    "        new.append(i[0].detach().numpy())\n",
    "\n",
    "    trajectories = torch.tensor(new, dtype = torch.float32)\n",
    "        \n",
    "    return trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def depth_map_to_bev(depth_map, objects, trajectories):\n",
    "    f_x = 1000\n",
    "    f_y = 1000\n",
    "    c_x = 400\n",
    "    c_y = 300\n",
    "    bev_size = 200\n",
    "    scale_factor = 15\n",
    "\n",
    "    height, width = depth_map.shape\n",
    "    bev_map = np.zeros((bev_size, bev_size), dtype=np.float32)\n",
    "\n",
    "    x_coords, y_coords = np.meshgrid(np.arange(width), np.arange(height))\n",
    "\n",
    "    depth = depth_map[y_coords, x_coords].detach().numpy()\n",
    "    X = (x_coords - c_x) * depth / f_x\n",
    "    Y = (y_coords - c_y) * depth / f_y\n",
    "\n",
    "    bev_x = np.clip((X * scale_factor).astype(int), 0, bev_size - 1)\n",
    "    bev_y = np.clip((Y * scale_factor).astype(int), 0, bev_size - 1)\n",
    "\n",
    "    for i in range(height * width):\n",
    "        bev_map[bev_y.flat[i], bev_x.flat[i]] = max(bev_map[bev_y.flat[i], bev_x.flat[i]], depth.flat[i])\n",
    "\n",
    "    for obj, traj in zip(objects, trajectories):\n",
    "        x_min, y_min, x_max, y_max = obj\n",
    "        x_min, y_min, x_max, y_max = x_min.item(), y_min.item(), x_max.item(), y_max.item()\n",
    "\n",
    "        traj_x_min, traj_y_min, traj_x_max, traj_y_max = traj\n",
    "        traj_x_min, traj_y_min, traj_x_max, traj_y_max = traj_x_min.item(), traj_y_min.item(), traj_x_max.item(), traj_y_max.item()\n",
    "\n",
    "        obj_depths = depth_map[int(y_min):int(y_max), int(x_min):int(x_max)].detach().numpy()\n",
    "        if obj_depths.size > 0:\n",
    "            mean_depth = np.mean(obj_depths)\n",
    "        else:\n",
    "            mean_depth = 0\n",
    "        \n",
    "        X_min = (x_min - c_x) * mean_depth / f_x\n",
    "        X_max = (x_max - c_x) * mean_depth / f_x\n",
    "        Y_min = (y_min - c_y) * mean_depth / f_y\n",
    "        Y_max = (y_max - c_y) * mean_depth / f_y\n",
    "        Z = mean_depth\n",
    "        bev_x_min = int(100 + X_min * scale_factor)\n",
    "        bev_x_max = int(100 + X_max * scale_factor)\n",
    "        bev_y_min = int(100 + Y_min * scale_factor)\n",
    "        bev_y_max = int(100 + Y_max * scale_factor)\n",
    "\n",
    "        traj_X_min = (traj_x_min - c_x) * mean_depth / f_x\n",
    "        traj_X_max = (traj_x_max - c_x) * mean_depth / f_x\n",
    "        traj_Y_min = (traj_y_min - c_y) * mean_depth / f_y\n",
    "        traj_Y_max = (traj_y_max - c_y) * mean_depth / f_y\n",
    "        traj_Z = mean_depth\n",
    "        traj_bev_x_min = int(100 + traj_X_min * scale_factor)\n",
    "        traj_bev_x_max = int(100 + traj_X_max * scale_factor)\n",
    "        traj_bev_y_min = int(100 + traj_Y_min * scale_factor)\n",
    "        traj_bev_y_max = int(100 + traj_Y_max * scale_factor)\n",
    "\n",
    "        mean_depth = torch.tensor(mean_depth, dtype = torch.float32)\n",
    "        if mean_depth > 4:\n",
    "            bev_map[bev_y_min-10:bev_y_max+10, bev_x_min-10:bev_x_max+10] = 0 # bev_map.max() - mean_depth\n",
    "            bev_map[traj_bev_y_min-10:traj_bev_y_max+10, traj_bev_x_min-10:traj_bev_x_max+10] = 0 # bev_map.max() - mean_depth\n",
    "\n",
    "    bev_map = np.clip(bev_map, 0, 255)\n",
    "    bev_map = bev_map.astype(np.uint8)\n",
    "\n",
    "    return bev_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_bev(bev_map):\n",
    "    bev_map = bev_map.max() - bev_map\n",
    "    bev_map = cv2.normalize(bev_map, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    bev_map = np.uint8(bev_map)\n",
    "    plt.imshow(bev_map, cmap=\"jet\", interpolation='nearest')\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_waypoint(bev_map):\n",
    "    bev_map = bev_map.max() - bev_map\n",
    "    bev_map = cv2.normalize(bev_map, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    bev_map = np.uint8(bev_map)\n",
    "    x_mid = 100\n",
    "    dist_from_x_mid = float('inf')\n",
    "\n",
    "    filtered_ones = []\n",
    "    for x in range(0,200,4):\n",
    "        for y in range(0,200,4):\n",
    "            b = bev_map[y,x]\n",
    "            if b < 80:\n",
    "                dist_from_x_mid = abs(x_mid - x)\n",
    "                x_for_filtering = x\n",
    "                y_for_filtering = y\n",
    "                # plt.scatter(x_for_filtering * 4, y_for_filtering * 3, color = 'black', marker='x')\n",
    "                filtered_ones.append([y_for_filtering, x_for_filtering])\n",
    "                # break\n",
    "\n",
    "    fof_cam = []\n",
    "    for i in filtered_ones:\n",
    "        fy = i[0]\n",
    "        fx = i[1]\n",
    "        fof_cam.append(bev_map[fy, fx])\n",
    "    fof_center = []\n",
    "    for i in range(0, len(filtered_ones)):\n",
    "        curr_x = filtered_ones[i][1]\n",
    "        fof_center.append(abs(x_mid - curr_x))\n",
    "\n",
    "    mini = min(fof_center)\n",
    "    mini_idxs = []\n",
    "    mini_val = float('inf')\n",
    "    for i in range(0, len(fof_center)):\n",
    "        if fof_center[i] <= mini_val:\n",
    "            mini_val = fof_center[i]\n",
    "    for i in range(0, len(fof_center)):\n",
    "        if(fof_center[i] == mini_val):\n",
    "            mini_idxs.append(i)\n",
    "    \n",
    "    max_x = -float('inf')\n",
    "    for i in mini_idxs:\n",
    "        cords = filtered_ones[i]\n",
    "        if cords[1] > max_x:\n",
    "            max_x = cords[1]\n",
    "            sel_y = cords[0]\n",
    "    cord_x = max_x\n",
    "    cord_y = sel_y\n",
    "\n",
    "    x_above = y_for_filtering\n",
    "    \n",
    "    # plt.scatter(cord_x * 4, cord_y * 3, color = 'black', marker='x')\n",
    "    # plt.imshow(depth_map.max() - depth_map, cmap = 'inferno')\n",
    "    # plt.grid(True)\n",
    "    # plt.show()\n",
    "\n",
    "    return cord_x * 4, cord_y * 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Saving BEV details*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bev_for_A2(image):\n",
    "    # depth_map = image_to_depth_map(image)\n",
    "    # objects = object_detection(image)\n",
    "    # trajectories = future_positions(objects['boxes'])\n",
    "    # bev_map = depth_map_to_bev(depth_map, objects['boxes'], trajectories)\n",
    "    # print(tuple(bev_map).shape)\n",
    "    bev_map = bev_generation(image, image_to_bev_model)\n",
    "    return bev_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_waypoints(file, image_name):\n",
    "    with open(os.path.join('datasets','A1_dataset.json'), 'r') as rf:\n",
    "        for data in rf:\n",
    "            data = json.loads(data)\n",
    "            if data[0]['label'] == image_name:\n",
    "                x, y = data[0]['waypoints']\n",
    "                val = [x, y]\n",
    "                # print(f\"match found: {val}\")\n",
    "                return val\n",
    "\n",
    "    return [400, 600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_bev_dataset(images_dir, output_dir):\n",
    "\n",
    "    for i in range(0, 28):\n",
    "        k = \"0\" if i <= 9 else \"\"\n",
    "        curr_folder_images = f\"{images_dir}/Video_0{k}{i}\"\n",
    "        images_path = [file for file in os.listdir(curr_folder_images) if file.endswith(\".png\")]\n",
    "        images_path.sort()\n",
    "        images_path = images_path[::skip]\n",
    "\n",
    "        for image_file in images_path:\n",
    "            image_path = os.path.join(curr_folder_images, image_file)\n",
    "            image = Image.open(image_path)\n",
    "            image_np = np.array(image)\n",
    "\n",
    "            bev_map = generate_bev_for_A2(image)\n",
    "            wp_x, wp_y = generate_waypoints(image_path, f\"datasets\\\\{image_file}\")\n",
    "            wp_x = wp_x / 4\n",
    "            wp_y = wp_y / 3\n",
    "\n",
    "            output_image_path = os.path.join(curr_folder_images, image_file)\n",
    "            bevs = ({\n",
    "                \"image\": output_image_path,\n",
    "                \"bev_map\": bev_map.tolist(),\n",
    "                \"way_point\": [wp_x, wp_y]\n",
    "            })\n",
    "            # break\n",
    "\n",
    "            with open(os.path.join(output_dir, \"A2_dataset_m.json\"), 'a') as wf:\n",
    "                json.dump(bevs, wf)\n",
    "                wf.write(\"\\n\")\n",
    "            break\n",
    "\n",
    "        if(i<27):\n",
    "            print(f\"\\rProgress: {((i + 1) / 0.27):.2f} %\", end = \"\")\n",
    "        # break\n",
    "\n",
    "    print(f\"\\nDone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_bev_dataset(images_dir, root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_the_wps_A2(images_dir, output_dir):\n",
    "\n",
    "    folder_name = f\"{images_dir}/Video_000\"\n",
    "    file_1 = os.path.join(folder_name, \"v000_0000.png\")\n",
    "    bev_1 = generate_bev_for_A2(Image.open(file_1))\n",
    "    wp_x, wp_y = generate_waypoints(file_1, \"datasets\\\\v000_0000.png\")\n",
    "    wp_x = wp_x / 4\n",
    "    wp_y = wp_y / 3\n",
    "\n",
    "    plt.figure(figsize = (4,4))\n",
    "    plt.imshow(bev_1)\n",
    "    plt.scatter(wp_x, wp_y, c = 'red', marker = 'o')\n",
    "    plt.show()\n",
    "    \n",
    "    output_image_path = os.path.join(folder_name, \"v000_0000.png\")\n",
    "    bevs_1 = [{\n",
    "        \"image\": output_image_path,\n",
    "        \"bev_map\": bev_1.tolist(),\n",
    "        \"way_point\": [wp_x, wp_y]\n",
    "    }]\n",
    "\n",
    "    folder_name = f\"{images_dir}/Video_007\"\n",
    "    file_2 = os.path.join(folder_name, \"v007_0019.png\")\n",
    "    bev_2 = generate_bev_for_A2(Image.open(file_2))\n",
    "    wp_x, wp_y = generate_waypoints(file_2, \"datasets\\\\v007_0019.png\")\n",
    "    wp_x = wp_x / 4\n",
    "    wp_y = wp_y / 3\n",
    "\n",
    "    plt.figure(figsize = (4,4))\n",
    "    plt.imshow(bev_2)\n",
    "    plt.scatter(wp_x, wp_y, c = 'red', marker = 'o')\n",
    "    plt.show()\n",
    "    \n",
    "    output_image_path = os.path.join(folder_name, file_2)\n",
    "    bevs_2 = [{\n",
    "        \"image\": output_image_path,\n",
    "        \"bev_map\": bev_2.tolist(),\n",
    "        \"way_point\": [wp_x, wp_y]\n",
    "    }]\n",
    "\n",
    "    # print(bevs_1[0]['image'], bevs_1[0]['way_point'])\n",
    "    # print(bevs_2[0]['image'], bevs_2[0]['way_point'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_the_wps_A2(images_dir, root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "getting dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BEVDataset(Dataset):\n",
    "    def __init__(self, root, bev_file, sequence_length = 5):\n",
    "        self.data = []\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "        with open(os.path.join(root, bev_file), \"r\") as wf:\n",
    "            images = []\n",
    "            bev_maps = []\n",
    "            waypoints = []\n",
    "            for data in wf:\n",
    "                data = json.loads(data)\n",
    "                # print(data.keys())\n",
    "\n",
    "                image = data['image']\n",
    "                bev_map = torch.tensor(data['bev_map'])\n",
    "                way_point = torch.tensor(data['way_point'])\n",
    "\n",
    "                images.append(image)\n",
    "                bev_maps.append(bev_map)\n",
    "                waypoints.append(way_point)\n",
    "\n",
    "                if len(images) >= self.sequence_length:\n",
    "                    sequence_images = images[-self.sequence_length:]\n",
    "                    sequence_bev_maps = bev_maps[-self.sequence_length:]\n",
    "                    sequence_waypoints = waypoints[-self.sequence_length:]\n",
    "                    self.data.append([sequence_images, sequence_bev_maps, sequence_waypoints])\n",
    "                    images = []\n",
    "                    bev_maps = []\n",
    "                    waypoints = []\n",
    "                    # print(self.data)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split indices for train and validation\n",
    "dataset = BEVDataset(root, bev_file, sequence_length = 5)\n",
    "remaining_indices, test_indices = train_test_split(range(len(dataset)), test_size=0.1, random_state=42)\n",
    "\n",
    "test_dataset = Subset(dataset, test_indices)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, drop_last=True)\n",
    "\n",
    "train_indices, val_indices = train_test_split(remaining_indices, test_size=0.5, random_state=42)\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BEV to image model (for faster operation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageToBEV(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageToBEV, self).__init__()\n",
    "        self.backbone = models.resnet18(pretrained=True)\n",
    "        self.backbone = nn.Sequential(*list(self.backbone.children())[:-2])\n",
    "        self.conv1x1 = nn.Conv2d(512, 256, kernel_size=1)\n",
    "        \n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 512, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(512, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 1, kernel_size=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.conv1x1(x)\n",
    "        x = self.upsample(x)\n",
    "        x = F.interpolate(x, size=(200, 200), mode='bilinear', align_corners=False)\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_to_bev_model = ImageToBEV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, bev_file):\n",
    "        self.data = []\n",
    "\n",
    "        with open(os.path.join(root, bev_file), \"r\") as wf:\n",
    "            for data in wf:\n",
    "                data = json.loads(data)\n",
    "                data = data[0]\n",
    "                image = np.array(Image.open(data['image']))\n",
    "                bev_map = torch.tensor(data['bev_map'])\n",
    "\n",
    "                self.data.append([image, bev_map])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_to_bev_model = torch.load('models/ImageToBEV.pth', map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageset = ImageDataset(bev_file)\n",
    "image_loader = DataLoader(imageset, batch_size = batch_size, shuffle = True, pin_memory=True, drop_last=True)\n",
    "\n",
    "dataloader = image_loader\n",
    "train_loader = image_loader\n",
    "val_loader = image_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bev_generation(image, model):\n",
    "    return model(torch.tensor(np.array(image, dtype = np.float32) / 255.0, dtype=torch.float32).unsqueeze(0).permute(0,3,1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, image_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in image_loader:\n",
    "        img = data[0]\n",
    "        bev = data[1]\n",
    "        optimizer.zero_grad()\n",
    "        pred = bev_generation(img[0], model)\n",
    "        loss = criterion(pred, bev[0].float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(image_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, image_loader, optimizer, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in image_loader:\n",
    "            img = data[0]\n",
    "            bev = data[1]\n",
    "            pred = bev_generation(img[0], model)\n",
    "            loss = criterion(pred, bev[0].float())\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(image_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, image_loader, optimizer, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in image_loader:\n",
    "            img = data[0]\n",
    "            bev = data[1]\n",
    "            pred = bev_generation(img[0], model)\n",
    "            print(pred)\n",
    "            print(waypoints)\n",
    "            loss = criterion(pred, bev[0].float())\n",
    "            total_loss += loss.item()\n",
    "            break\n",
    "    return total_loss / len(image_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_train = image_to_bev_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_save = best_model_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_to_save, 'models/ImageToBEV.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OUR TEMPORAL FUSION MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size=3):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.conv = nn.Conv2d(input_dim + hidden_dim, hidden_dim * 4, kernel_size, padding=padding)\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        h, c = hidden\n",
    "        combined = torch.cat([x, h], dim=1)\n",
    "        conv_out = self.conv(combined)\n",
    "        i, f, g, o = torch.chunk(conv_out, 4, dim=1)\n",
    "        i, f, g, o = torch.sigmoid(i), torch.sigmoid(f), torch.tanh(g), torch.sigmoid(o)\n",
    "        c_next = f * c + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "        return h_next, c_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim, sequence_length):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.output_dim = output_dim\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "        self.lstm_cells = nn.ModuleList([\n",
    "            ConvLSTMCell(input_dim if i == 0 else hidden_dim, hidden_dim)\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "        self.fc = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, c, h, w = x.size()\n",
    "        h_states = [torch.zeros(b, self.hidden_dim, h, w, device=x.device) for _ in range(self.num_layers)]\n",
    "        c_states = [torch.zeros(b, self.hidden_dim, h, w, device=x.device) for _ in range(self.num_layers)]\n",
    "        \n",
    "        for time_step in range(t):\n",
    "            inp = x[:, time_step]\n",
    "            for i, cell in enumerate(self.lstm_cells):\n",
    "                h_states[i], c_states[i] = cell(inp, (h_states[i], c_states[i]))\n",
    "                inp = h_states[i]\n",
    "        \n",
    "        out = torch.flatten(inp, start_dim=1)\n",
    "\n",
    "        if self.fc is None:\n",
    "            self.fc = nn.Linear(self.hidden_dim * h * w, self.output_dim * self.sequence_length).to(x.device)\n",
    "\n",
    "        return self.fc(out).contiguous().view(b, t, self.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim, hidden_dim, num_layers, output_dim, seq_length = 1, 64, 2, 2, 5\n",
    "model_to_train = ConvLSTM(input_dim, hidden_dim, num_layers, output_dim, seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*--training set ups--*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wp_generation(bev, model):\n",
    "    return model(bev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, train_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        bev_maps = torch.stack(data[1], dim=1)\n",
    "        waypoints = torch.stack(data[2], dim=1).to(torch.float32)\n",
    "        bev_maps = bev_maps.unsqueeze(2)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pred = wp_generation(bev_maps, model)\n",
    "        loss = criterion(pred, waypoints)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, val_loader, optimizer, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            bev_maps = torch.stack(data[1], dim=1)\n",
    "            waypoints = torch.stack(data[2], dim=1).to(torch.float32)\n",
    "            bev_maps = bev_maps.unsqueeze(2)\n",
    "            \n",
    "            pred = wp_generation(bev_maps, model)\n",
    "            loss = criterion(pred, waypoints)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataloader, optimizer, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            bev_maps = torch.stack(data[1], dim=1)\n",
    "            waypoints = torch.stack(data[2], dim=1).to(torch.float32)\n",
    "            bev_maps = bev_maps.unsqueeze(2)\n",
    "            \n",
    "            pred = wp_generation(bev_maps, model)\n",
    "            print(pred)\n",
    "            print(waypoints)\n",
    "            loss = criterion(pred, waypoints)\n",
    "            total_loss += loss.item()\n",
    "            break\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_save = best_model_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_to_save, 'models/ConvLSTM.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_model = model_to_save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ALGORITHM 3: Future Waypoint Prediction using BEV representations and GPS mechanism**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_processor = AutoImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "detr_model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "\n",
    "root = \"datasets\"\n",
    "images_dir = f\"{root}/images\"\n",
    "labels_dir = f\"{root}/labels\"\n",
    "bev_t1_t2_file = \"A3_dataset_m.json\"\n",
    "batch_size = 2\n",
    "\n",
    "skip = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Creation (IMAGE to t1, t2 BEV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "image and navigation targets > bird's eye view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_depth_map(image):\n",
    "    estimator = pipeline(task=\"depth-estimation\", model=\"Intel/dpt-large\")\n",
    "    result = estimator(images=image)\n",
    "    depth_map = result['predicted_depth']\n",
    "    return depth_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def object_detection(image):\n",
    "    inputs = image_processor(images=image, return_tensors=\"pt\")\n",
    "    outputs = detr_model(**inputs)\n",
    "    target_sizes = torch.tensor([image.size[::-1]])\n",
    "    objects = image_processor.post_process_object_detection(outputs, threshold=0.9, target_sizes=target_sizes)[0]\n",
    "\n",
    "    return objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def future_positions(objects, alpha = 0.1, diff = 5):\n",
    "    trajectories_t1 = []\n",
    "    trajectories_t2 = []\n",
    "    \n",
    "    for obj in objects:\n",
    "        smoothed_positions_t1 = [obj]\n",
    "        smoothed_positions_t2 = [obj]\n",
    "        for t in range(0, len(obj)):\n",
    "            smoothed_pos_t1 = alpha * obj[t] + (1 - alpha) * smoothed_positions_t1[-1]\n",
    "            smoothed_pos_t2 = (diff * alpha) * obj[t] + (1 - (diff * alpha)) * smoothed_positions_t2[-1]\n",
    "            smoothed_positions_t1.append(smoothed_pos_t1)\n",
    "            smoothed_positions_t2.append(smoothed_pos_t2)\n",
    "        future_positions_t1 = [smoothed_positions_t1[-1]]\n",
    "        future_positions_t2 = [smoothed_positions_t2[-1]]\n",
    "        trajectories_t1.append(future_positions_t1)\n",
    "        trajectories_t2.append(future_positions_t2)\n",
    "\n",
    "    new_t1 = []\n",
    "    new_t2 = []\n",
    "    for t1, t2 in zip(trajectories_t1, trajectories_t2):\n",
    "        new_t1.append(t1[0].detach().numpy())\n",
    "        new_t2.append(t2[0].detach().numpy())\n",
    "\n",
    "    trajectories_t1 = torch.tensor(new_t1, dtype = torch.float32)\n",
    "    trajectories_t2 = torch.tensor(new_t2, dtype = torch.float32)\n",
    "        \n",
    "    return trajectories_t1, trajectories_t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def depth_map_to_bev(depth_map, objects, trajectories_t1, trajectories_t2):\n",
    "    f_x = 1000\n",
    "    f_y = 1000\n",
    "    c_x = 400\n",
    "    c_y = 300\n",
    "    bev_size = 200\n",
    "    scale_factor = 15\n",
    "\n",
    "    height, width = depth_map.shape\n",
    "    bev_map_t1 = np.zeros((bev_size, bev_size), dtype=np.float32)\n",
    "    bev_map_t2 = np.zeros((bev_size, bev_size), dtype=np.float32)\n",
    "\n",
    "    x_coords, y_coords = np.meshgrid(np.arange(width), np.arange(height))\n",
    "\n",
    "    depth = depth_map[y_coords, x_coords].detach().numpy()\n",
    "    X = (x_coords - c_x) * depth / f_x\n",
    "    Y = (y_coords - c_y) * depth / f_y\n",
    "\n",
    "    bev_x = np.clip((X * scale_factor).astype(int), 0, bev_size - 1)\n",
    "    bev_y = np.clip((Y * scale_factor).astype(int), 0, bev_size - 1)\n",
    "\n",
    "    for i in range(height * width):\n",
    "        bev_map_t1[bev_y.flat[i], bev_x.flat[i]] = max(bev_map_t1[bev_y.flat[i], bev_x.flat[i]], depth.flat[i])\n",
    "        bev_map_t2[bev_y.flat[i], bev_x.flat[i]] = max(bev_map_t2[bev_y.flat[i], bev_x.flat[i]], depth.flat[i])\n",
    "\n",
    "    for obj, traj_t1, traj_t2 in zip(objects, trajectories_t1, trajectories_t2):\n",
    "        x_min, y_min, x_max, y_max = obj\n",
    "        x_min, y_min, x_max, y_max = x_min.item(), y_min.item(), x_max.item(), y_max.item()\n",
    "\n",
    "        traj_x_min_t1, traj_y_min_t1, traj_x_max_t1, traj_y_max_t1 = traj_t1\n",
    "        traj_x_min_t1, traj_y_min_t1, traj_x_max_t1, traj_y_max_t1 = traj_x_min_t1.item(), traj_y_min_t1.item(), traj_x_max_t1.item(), traj_y_max_t1.item()\n",
    "\n",
    "        traj_x_min_t2, traj_y_min_t2, traj_x_max_t2, traj_y_max_t2 = traj_t2\n",
    "        traj_x_min_t2, traj_y_min_t2, traj_x_max_t2, traj_y_max_t2 = traj_x_min_t2.item(), traj_y_min_t2.item(), traj_x_max_t2.item(), traj_y_max_t2.item()\n",
    "\n",
    "        obj_depths = depth_map[int(y_min):int(y_max), int(x_min):int(x_max)].detach().numpy()\n",
    "        if obj_depths.size > 0:\n",
    "            mean_depth = np.mean(obj_depths)\n",
    "        else:\n",
    "            mean_depth = 0\n",
    "        \n",
    "        X_min = (x_min - c_x) * mean_depth / f_x\n",
    "        X_max = (x_max - c_x) * mean_depth / f_x\n",
    "        Y_min = (y_min - c_y) * mean_depth / f_y\n",
    "        Y_max = (y_max - c_y) * mean_depth / f_y\n",
    "        Z = mean_depth\n",
    "        bev_x_min = int(100 + X_min * scale_factor)\n",
    "        bev_x_max = int(100 + X_max * scale_factor)\n",
    "        bev_y_min = int(100 + Y_min * scale_factor)\n",
    "        bev_y_max = int(100 + Y_max * scale_factor)\n",
    "\n",
    "        traj_X_min_t1 = (traj_x_min_t1 - c_x) * mean_depth / f_x\n",
    "        traj_X_max_t1 = (traj_x_max_t1 - c_x) * mean_depth / f_x\n",
    "        traj_Y_min_t1 = (traj_y_min_t1 - c_y) * mean_depth / f_y\n",
    "        traj_Y_max_t1 = (traj_y_max_t1 - c_y) * mean_depth / f_y\n",
    "        traj_Z_t1 = mean_depth\n",
    "        traj_bev_x_min_t1 = int(100 + traj_X_min_t1 * scale_factor)\n",
    "        traj_bev_x_max_t1 = int(100 + traj_X_max_t1 * scale_factor)\n",
    "        traj_bev_y_min_t1 = int(100 + traj_Y_min_t1 * scale_factor)\n",
    "        traj_bev_y_max_t1 = int(100 + traj_Y_max_t1 * scale_factor)\n",
    "\n",
    "        traj_X_min_t2 = (traj_x_min_t2 - c_x) * mean_depth / f_x\n",
    "        traj_X_max_t2 = (traj_x_max_t2 - c_x) * mean_depth / f_x\n",
    "        traj_Y_min_t2 = (traj_y_min_t2 - c_y) * mean_depth / f_y\n",
    "        traj_Y_max_t2 = (traj_y_max_t2 - c_y) * mean_depth / f_y\n",
    "        traj_Z_t2 = mean_depth\n",
    "        traj_bev_x_min_t2 = int(100 + traj_X_min_t2 * scale_factor)\n",
    "        traj_bev_x_max_t2 = int(100 + traj_X_max_t2 * scale_factor)\n",
    "        traj_bev_y_min_t2 = int(100 + traj_Y_min_t2 * scale_factor)\n",
    "        traj_bev_y_max_t2 = int(100 + traj_Y_max_t2 * scale_factor)\n",
    "\n",
    "        mean_depth = torch.tensor(mean_depth, dtype = torch.float32)\n",
    "        if mean_depth > 4:\n",
    "            # bev_map_t1[bev_y_min-10:bev_y_max+10, bev_x_min-10:bev_x_max+10] = 0 # bev_map_t1.max() - mean_depth\n",
    "            bev_map_t1[traj_bev_y_min_t1-10:traj_bev_y_max_t1+10, traj_bev_x_min_t1-10:traj_bev_x_max_t1+10] = 0 # bev_map_t1.max() - mean_depth\n",
    "            \n",
    "            # bev_map_t2[bev_y_min-10:bev_y_max+10, bev_x_min-10:bev_x_max+10] = 0 # bev_map_t2.max() - mean_depth\n",
    "            bev_map_t2[traj_bev_y_min_t2-10:traj_bev_y_max_t2+10, traj_bev_x_min_t2-10:traj_bev_x_max_t2+10] = 0 # bev_map_t2.max() - mean_depth\n",
    "\n",
    "    bev_map_t1 = np.clip(bev_map_t1, 0, 255)\n",
    "    bev_map_t1 = bev_map_t1.astype(np.uint8)\n",
    "\n",
    "    bev_map_t2 = np.clip(bev_map_t2, 0, 255)\n",
    "    bev_map_t2 = bev_map_t2.astype(np.uint8)\n",
    "\n",
    "    return bev_map_t1, bev_map_t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_waypoint(bev_map, x_mid):\n",
    "    bev_map = bev_map.max() - bev_map\n",
    "    bev_map = cv2.normalize(bev_map, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    bev_map = np.uint8(bev_map)\n",
    "    dist_from_x_mid = float('inf')\n",
    "\n",
    "    filtered_ones = []\n",
    "    for x in range(0,200,4):\n",
    "        for y in range(0,200,4):\n",
    "            b = bev_map[y,x]\n",
    "            if b < 80:\n",
    "                dist_from_x_mid = abs(x_mid - x)\n",
    "                x_for_filtering = x\n",
    "                y_for_filtering = y\n",
    "                # plt.scatter(x_for_filtering * 4, y_for_filtering * 3, color = 'black', marker='x')\n",
    "                filtered_ones.append([y_for_filtering, x_for_filtering])\n",
    "                # break\n",
    "\n",
    "    fof_cam = []\n",
    "    for i in filtered_ones:\n",
    "        fy = i[0]\n",
    "        fx = i[1]\n",
    "        fof_cam.append(bev_map[fy, fx])\n",
    "    fof_center = []\n",
    "    for i in range(0, len(filtered_ones)):\n",
    "        curr_x = filtered_ones[i][1]\n",
    "        fof_center.append(abs(x_mid - curr_x))\n",
    "\n",
    "    mini = min(fof_center)\n",
    "    mini_idxs = []\n",
    "    mini_val = float('inf')\n",
    "    for i in range(0, len(fof_center)):\n",
    "        if fof_center[i] <= mini_val:\n",
    "            mini_val = fof_center[i]\n",
    "    for i in range(0, len(fof_center)):\n",
    "        if(fof_center[i] == mini_val):\n",
    "            mini_idxs.append(i)\n",
    "    \n",
    "    max_x = -float('inf')\n",
    "    for i in mini_idxs:\n",
    "        cords = filtered_ones[i]\n",
    "        if cords[1] > max_x:\n",
    "            max_x = cords[1]\n",
    "            sel_y = cords[0]\n",
    "    cord_x = max_x\n",
    "    cord_y = sel_y\n",
    "\n",
    "    x_above = y_for_filtering\n",
    "    \n",
    "    # plt.scatter(cord_x * 4, cord_y * 3, color = 'black', marker='x')\n",
    "    # plt.imshow(depth_map.max() - depth_map, cmap = 'inferno')\n",
    "    # plt.grid(True)\n",
    "    # plt.show()\n",
    "\n",
    "    return cord_x * 4, cord_y * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_bev(bev_map):\n",
    "    bev_map = bev_map.max() - bev_map\n",
    "    bev_map = cv2.normalize(bev_map, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    bev_map = np.uint8(bev_map)\n",
    "    plt.figure(figsize = (3,3))\n",
    "    plt.imshow(bev_map, cmap=\"jet\", interpolation='nearest')\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_depth_map(depth_map, cord_x = 400, cord_y = 75, c = 'white'):\n",
    "    depth_map = depth_map.max() - depth_map\n",
    "    height, width = depth_map.shape\n",
    "    depth_at_cord = depth_map[cord_y-1, cord_x-1]\n",
    "    print(f\"Depth at co-ordinate ({cord_x}, {cord_y}) of the image: {depth_at_cord:.2f} meters\")\n",
    "    plt.scatter(cord_x, cord_y, color=c, marker='x')\n",
    "    plt.imshow(depth_map, cmap = 'inferno')\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Saving t1, t1 BEV details*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bev_for_A3(image):\n",
    "    # depth_map = image_to_depth_map(image)\n",
    "    # objects = object_detection(image)\n",
    "    # trajectories_t1, trajectories_t2 = future_positions(objects['boxes'], alpha = 0.3, diff = 0.3)\n",
    "    # bev_map_t1, bev_map_t2 = depth_map_to_bev(depth_map, objects['boxes'], trajectories_t1, trajectories_t2)\n",
    "    bev_map = bev_t1_t2_generation(image, image_to_bev_t1_t2_model).squeeze(0)\n",
    "    return bev_map[0], bev_map[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_waypoints(file, image_name):\n",
    "    with open(os.path.join('datasets','A1_dataset.json'), 'r') as rf:\n",
    "        for data in rf:\n",
    "            data = json.loads(data)\n",
    "            if data[0]['label'] == image_name:\n",
    "                x, y = data[0]['waypoints']\n",
    "                val = [x, y]\n",
    "                # print(f\"match found: {val}\")\n",
    "                return val\n",
    "\n",
    "    return [400, 600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_t1_t2_bev_dataset(images_dir, root, bev_t1_t2_file):\n",
    "\n",
    "    for i in range(0, 28):\n",
    "        k = \"0\" if i <= 9 else \"\"\n",
    "        curr_folder_images = f\"{images_dir}/Video_0{k}{i}\"\n",
    "        images_path = [file for file in os.listdir(curr_folder_images) if file.endswith(\".png\")]\n",
    "        images_path.sort()\n",
    "        images_path = images_path[::skip]\n",
    "        x_val = 20\n",
    "\n",
    "        for image_file in images_path:\n",
    "            if x_val >= 180:\n",
    "                x_val = 20\n",
    "            bevs = []\n",
    "            image_path = os.path.join(curr_folder_images, image_file)\n",
    "            image = Image.open(image_path)\n",
    "            image_np = np.array(image)\n",
    "            bev_map_t1, bev_map_t2 = generate_bev_for_A3(image)\n",
    "\n",
    "            wp_x_t1, wp_y_t1 = generate_waypoints(image_path, f\"datasets\\\\{image_file}\")\n",
    "            times = 1 + ((x_val - (wp_x_t1 / 4)) / 200)\n",
    "            wp_x_t2 = wp_x_t1 * times\n",
    "            wp_y_t2 = wp_y_t1 * times\n",
    "\n",
    "            output_image_path = os.path.join(curr_folder_images, image_file)\n",
    "            bevs.append({\n",
    "                \"target\": x_val,\n",
    "                \"image\": output_image_path,\n",
    "                \"bev_map_1\": bev_map_t1.tolist(),\n",
    "                \"bev_map_2\": bev_map_t2.tolist(),\n",
    "                \"way_points\": [wp_x_t1, wp_y_t1, wp_x_t2, wp_y_t2] # mapping that SOAB to both t1, t2 :)\n",
    "            })\n",
    "            # break\n",
    "\n",
    "            with open(os.path.join(root, bev_t1_t2_file), 'a') as wf:\n",
    "                json.dump(bevs, wf)\n",
    "                wf.write(\"\\n\")\n",
    "            break\n",
    "\n",
    "            x_val = x_val + 40\n",
    "\n",
    "        if(i<27):\n",
    "            print(f\"\\rProgress: {((i + 1) / 0.27):.2f} %\", end = \"\")\n",
    "        # break\n",
    "\n",
    "    print(f\"\\nDone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_t1_t2_bev_dataset(images_dir, root, bev_t1_t2_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_the_wps_A3(images_dir, output_dir):\n",
    "\n",
    "    folder_name = f\"{images_dir}/Video_000\"\n",
    "    file_1 = os.path.join(folder_name, \"v000_0000.png\")\n",
    "    bev_1, _ = generate_bev_for_A3(Image.open(file_1))\n",
    "    wp_x, wp_y = generate_waypoints(file_1, \"datasets\\\\v000_0000.png\")\n",
    "    wp_x = wp_x / 4\n",
    "    wp_y = wp_y / 3\n",
    "\n",
    "    plt.figure(figsize = (4,4))\n",
    "    plt.imshow(bev_1)\n",
    "    plt.scatter(wp_x, wp_y, c = 'red', marker = 'o')\n",
    "    plt.show()\n",
    "    \n",
    "    output_image_path = os.path.join(folder_name, \"v000_0000.png\")\n",
    "    bevs_1 = [{\n",
    "        \"image\": output_image_path,\n",
    "        \"bev_map\": bev_1.tolist(),\n",
    "        \"way_point\": [wp_x, wp_y]\n",
    "    }]\n",
    "\n",
    "    folder_name = f\"{images_dir}/Video_007\"\n",
    "    file_2 = os.path.join(folder_name, \"v007_0019.png\")\n",
    "    _, bev_2 = generate_bev_for_A3(Image.open(file_2))\n",
    "    wp_x, wp_y = generate_waypoints(file_2, \"datasets\\\\v007_0019.png\")\n",
    "    times = 1 + ((20 - wp_x) / 200)\n",
    "    wp_x = wp_x / 4\n",
    "    wp_y = wp_y / 3\n",
    "\n",
    "    plt.figure(figsize = (4,4))\n",
    "    plt.imshow(bev_2)\n",
    "    plt.scatter(wp_x, wp_y, c = 'red', marker = 'o')\n",
    "    plt.show()\n",
    "    \n",
    "    output_image_path = os.path.join(folder_name, file_2)\n",
    "    bevs_2 = [{\n",
    "        \"image\": output_image_path,\n",
    "        \"bev_map\": bev_2.tolist(),\n",
    "        \"way_point\": [wp_x, wp_y]\n",
    "    }]\n",
    "\n",
    "    # print(bevs_1[0]['image'], bevs_1[0]['way_point'])\n",
    "    # print(bevs_2[0]['image'], bevs_2[0]['way_point'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_the_wps_A3(images_dir, root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "getting dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BEV_t1_t2_Dataset(Dataset):\n",
    "    def __init__(self, root, bev_t1_t2_file):\n",
    "        self.data = []\n",
    "\n",
    "        with open(os.path.join(root, bev_t1_t2_file), \"r\") as wf:\n",
    "            for data in wf:\n",
    "                data = json.loads(data)\n",
    "                data = data[0]\n",
    "                # print(data.keys())\n",
    "\n",
    "                target = data['target']\n",
    "                image = data['image']\n",
    "                bev_map_1 = torch.tensor(data['bev_map_1'])\n",
    "                bev_map_2 = torch.tensor(data['bev_map_2']) # not used here\n",
    "                way_point = torch.tensor(data['way_points'])\n",
    "                \n",
    "                self.data.append([target, image, bev_map_1, way_point])\n",
    "                # break\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split indices for train and validation\n",
    "dataset = BEV_t1_t2_Dataset(root, bev_t1_t2_file)\n",
    "remaining_indices, test_indices = train_test_split(range(len(dataset)), test_size=0.1, random_state=42)\n",
    "\n",
    "test_dataset = Subset(dataset, test_indices)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, drop_last=True)\n",
    "\n",
    "train_indices, val_indices = train_test_split(remaining_indices, test_size=0.3, random_state=42)\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BEV_t1_t2 to image model (for faster operation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageToBEV_t1_t2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageToBEV_t1_t2, self).__init__()\n",
    "        self.backbone = models.resnet18(pretrained=True)\n",
    "        self.backbone = nn.Sequential(*list(self.backbone.children())[:-2])\n",
    "        self.conv1x1 = nn.Conv2d(512, 256, kernel_size=1)\n",
    "        \n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 512, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(512, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 2, kernel_size=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.conv1x1(x)\n",
    "        x = self.upsample(x)\n",
    "        x = F.interpolate(x, size=(200, 200), mode='bilinear', align_corners=False)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_to_bev_t1_t2_model = ImageToBEV_t1_t2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Imaget1t2Dataset(Dataset):\n",
    "    def __init__(self, bev_file):\n",
    "        self.data = []\n",
    "\n",
    "        with open(os.path.join(root, bev_file), \"r\") as wf:\n",
    "            for data in wf:\n",
    "                data = json.loads(data)\n",
    "                # print(data)\n",
    "                data = data[0]\n",
    "                image = np.array(Image.open(data['image']))\n",
    "                bev_map_1 = torch.tensor(data['bev_map_1'])\n",
    "                bev_map_2 = torch.tensor(data['bev_map_2'])\n",
    "\n",
    "                self.data.append([image, torch.stack((bev_map_1, bev_map_2), dim = 0)])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_to_bev_t1_t2_model = torch.load('models/ImageToBEV_t1_t2.pth', map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_train = image_to_bev_t1_t2_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imaget1t2set = Imaget1t2Dataset(bev_t1_t2_file)\n",
    "imaget1t2_loader = DataLoader(imaget1t2set, batch_size = batch_size, shuffle = True, pin_memory=True, drop_last=True)\n",
    "\n",
    "test_loader = imaget1t2_loader\n",
    "train_loader = imaget1t2_loader\n",
    "val_loader = imaget1t2_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bev_t1_t2_generation(image, model):\n",
    "    np_image = np.array(image, dtype = np.float32) / 255.0\n",
    "    tensor_image = torch.tensor(np_image, dtype=torch.float32)\n",
    "    # print(tensor_image.shape)\n",
    "    input_image = tensor_image.unsqueeze(0).permute(0,3,1,2)\n",
    "    return model(input_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, image_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in image_loader:\n",
    "        img = data[0]\n",
    "        bev = data[1]\n",
    "        optimizer.zero_grad()\n",
    "        pred = bev_t1_t2_generation(img, model)\n",
    "        loss = criterion(pred, bev.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(image_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, image_loader, optimizer, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in image_loader:\n",
    "            img = data[0]\n",
    "            bev = data[1]\n",
    "            pred = bev_t1_t2_generation(img, model)\n",
    "            loss = criterion(pred, bev.float())\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(image_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, image_loader, optimizer, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in image_loader:\n",
    "            img = data[0]\n",
    "            bev = data[1]\n",
    "            pred = bev_t1_t2_generation(img, model)\n",
    "            print(pred.shape)\n",
    "            print(bev.shape)\n",
    "            loss = criterion(pred, bev.float())\n",
    "            total_loss += loss.item()\n",
    "            break\n",
    "    return total_loss / len(image_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_save = best_model_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_to_save, 'models/ImageToBEV_t1_t2.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OUR CNN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_WPModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_WPModel, self).__init__()\n",
    "\n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "        self.resnet_features = nn.Sequential(*list(resnet.children())[:-1])\n",
    "\n",
    "        # self.conv_layers = nn.Sequential(\n",
    "        #     nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
    "        #     nn.BatchNorm2d(32),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.MaxPool2d(2, 2),\n",
    "        #     nn.Conv2d(32, 256, kernel_size=3, stride=1, padding=1),\n",
    "        #     nn.BatchNorm2d(256),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.MaxPool2d(2, 2),\n",
    "        #     nn.Conv2d(256, 1024, kernel_size=3, stride=1, padding=1),\n",
    "        #     nn.BatchNorm2d(1024),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.MaxPool2d(2, 2),\n",
    "        #     nn.Conv2d(1024, 256, kernel_size=3, stride=1, padding=1),\n",
    "        #     nn.BatchNorm2d(256),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.MaxPool2d(2, 2),\n",
    "        #     nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "        #     nn.BatchNorm2d(512),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.MaxPool2d(2, 2),\n",
    "        # )\n",
    "        \n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(512 + 1, 512),\n",
    "            # nn.Linear(512 * 6 * 6 + 1, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 4),\n",
    "            nn.Dropout(0.25),\n",
    "        )\n",
    "\n",
    "    def forward(self, pixel_values, target_loc):\n",
    "        x = self.resnet_features(pixel_values)\n",
    "        # x = self.conv_layers(pixel_values)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.cat((x, target_loc.unsqueeze(0).view(-1, 1)), dim=1)\n",
    "        output = self.fc_layers(x)\n",
    "        \n",
    "        return output.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_train = CNN_WPModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*--training set ups--*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wp_generation(values, locs, model):\n",
    "    return model(values, locs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, train_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        target_loc = data[0]\n",
    "        bev_maps = data[2].float()\n",
    "        waypoints = data[3].float()\n",
    "        bev_maps = bev_maps.unsqueeze(0).permute(1,0,2,3)\n",
    "        bev_maps = bev_maps.repeat(1, 3, 1, 1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pred = wp_generation(bev_maps, target_loc, model)\n",
    "        loss = criterion(pred, waypoints)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, val_loader, optimizer, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            target_loc = data[0]\n",
    "            bev_maps = data[2].float()\n",
    "            waypoints = data[3].float()\n",
    "            bev_maps = bev_maps.unsqueeze(0).permute(1,0,2,3)\n",
    "            bev_maps = bev_maps.repeat(1, 3, 1, 1)\n",
    "            \n",
    "            pred = wp_generation(bev_maps, target_loc, model)\n",
    "            loss = criterion(pred, waypoints)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataloader, optimizer, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            target_loc = data[0]\n",
    "            bev_maps = data[2].float()\n",
    "            waypoints = data[3].float()\n",
    "            bev_maps = bev_maps.unsqueeze(0).permute(1,0,2,3)\n",
    "            bev_maps = bev_maps.repeat(1, 3, 1, 1)\n",
    "            \n",
    "            pred = wp_generation(bev_maps, target_loc, model)\n",
    "            print(pred)\n",
    "            print(waypoints)\n",
    "            loss = criterion(pred, waypoints)\n",
    "            total_loss += loss.item()\n",
    "            break\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_save = best_model_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_to_save, 'models/CNN_WPModel.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_model = model_to_save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Management Section**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 8\n",
    "num_epochs = 50\n",
    "optimizer = optim.Adam(model_to_train.parameters(), lr=1e-2)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = test(model_to_train, test_loader, optimizer, criterion)\n",
    "print(f\"test loss is: {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = test(model_to_train, test_loader, optimizer, criterion)\n",
    "print(f\"test loss is: {test_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e2\n",
    "best_train_loss = float('inf')\n",
    "best_val_loss = float('inf')\n",
    "for i in range(0,iterations):\n",
    "  lr = lr / 10\n",
    "  optimizer = optim.Adam(model_to_train.parameters(), lr=lr)\n",
    "  patience = 10\n",
    "  print(f\"Setting Learning rate to: {lr}\")\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "    best_model_trained = model_to_train\n",
    "    if(patience == 0):\n",
    "      break\n",
    "    \n",
    "    train_loss = training(model_to_train, train_loader, optimizer, criterion)\n",
    "    val_loss = validation(model_to_train, val_loader, optimizer, criterion)\n",
    "    \n",
    "    if(train_loss > (best_train_loss * 0.95) or (val_loss > (best_val_loss * 0.95))):\n",
    "      model_to_train = best_model_trained\n",
    "      patience -= 1\n",
    "    else:\n",
    "      best_train_loss = train_loss\n",
    "      best_val_loss = val_loss\n",
    "      best_model_trained = model_to_train\n",
    "      patience = 10\n",
    "      # torch.save(model_to_train.state_dict(), '/content/drive/MyDrive/my_model.pth')\n",
    "    print(f\"\\rEpoch {epoch + 1}/{num_epochs}, train-Loss: {train_loss:.1f}, val-Loss: {val_loss:.1f}     \", end = \"\")#, patience: {patience}\", end = \"\")\n",
    "    # break\n",
    "  print(\"\\n\")\n",
    "  # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e0\n",
    "ctr = 0\n",
    "best_train_loss = float('inf')\n",
    "best_val_loss = float('inf')\n",
    "for i in range(0,iterations):\n",
    "  lr = lr / 10\n",
    "  optimizer = optim.Adam(model_to_train.parameters(), lr=lr)\n",
    "  patience = 10\n",
    "  ctr = ctr + 1\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "    best_model_trained = model_to_train\n",
    "    if(patience == 0):\n",
    "      break\n",
    "    \n",
    "    train_loss = training(model_to_train, train_loader, optimizer, criterion)\n",
    "    val_loss = validation(model_to_train, val_loader, optimizer, criterion)\n",
    "    \n",
    "    if(train_loss > (best_train_loss * 0.95) or (val_loss > (best_val_loss * 0.95))):\n",
    "      model_to_train = best_model_trained\n",
    "      patience =-1\n",
    "    else:\n",
    "      best_train_loss = train_loss\n",
    "      best_val_loss = val_loss\n",
    "      best_model_trained = model_to_train\n",
    "      patience = 10\n",
    "\n",
    "    print(f\"\\r{i + 1}> Epoch {epoch + 1}/{num_epochs}, train-Loss: {train_loss:.1f}, val-Loss: {val_loss:.1f}     \", end = \"\")\n",
    "  print(\"\")\n",
    "  torch.save(best_model_trained, f\"models/A1/update count - {ctr}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e1\n",
    "ctr = 0\n",
    "best_train_loss = float('inf')\n",
    "best_val_loss = float('inf')\n",
    "for i in range(0,iterations):\n",
    "  lr = lr / 10\n",
    "  optimizer = optim.Adam(model_to_train.parameters(), lr=lr)\n",
    "  patience = 5\n",
    "  ctr = ctr + 1\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "    best_model_trained = model_to_train\n",
    "    if(patience == 0):\n",
    "      break\n",
    "    \n",
    "    train_loss = training(model_to_train, train_loader, optimizer, criterion)\n",
    "    val_loss = validation(model_to_train, val_loader, optimizer, criterion)\n",
    "    \n",
    "    if(train_loss > (best_train_loss * 0.95) or (val_loss > (best_val_loss * 0.95))):\n",
    "      model_to_train = best_model_trained\n",
    "      patience -= 1\n",
    "    else:\n",
    "      best_train_loss = train_loss\n",
    "      best_val_loss = val_loss\n",
    "      best_model_trained = model_to_train\n",
    "      patience = 5\n",
    "\n",
    "    print(f\"\\r{i + 1}> Epoch {epoch + 1}/{num_epochs}, train-Loss: {train_loss:.1f}, val-Loss: {val_loss:.1f} (patience: {patience})     \", end = \"\")\n",
    "  print(\"\")\n",
    "  torch.save(best_model_trained, f\"models/A2/update count - {ctr}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e2\n",
    "ctr = 0\n",
    "best_train_loss = float('inf')\n",
    "best_val_loss = float('inf')\n",
    "for i in range(0,iterations):\n",
    "  lr = lr / 10\n",
    "  optimizer = optim.Adam(model_to_train.parameters(), lr=lr)\n",
    "  patience = 10\n",
    "  ctr = ctr + 1\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "    best_model_trained = model_to_train\n",
    "    if(patience == 0):\n",
    "      break\n",
    "    \n",
    "    train_loss = training(model_to_train, train_loader, optimizer, criterion)\n",
    "    val_loss = validation(model_to_train, val_loader, optimizer, criterion)\n",
    "    \n",
    "    if(train_loss > (best_train_loss * 0.95) or (val_loss > (best_val_loss * 0.95))):\n",
    "      model_to_train = best_model_trained\n",
    "      patience =-1\n",
    "    else:\n",
    "      best_train_loss = train_loss\n",
    "      best_val_loss = val_loss\n",
    "      best_model_trained = model_to_train\n",
    "      patience = 10\n",
    "\n",
    "    print(f\"\\r{i + 1}> Epoch {epoch + 1}/{num_epochs}, train-Loss: {train_loss:.1f}, val-Loss: {val_loss:.1f}     \", end = \"\")\n",
    "  print(\"\")\n",
    "  torch.save(best_model_trained, f\"models/A3/update count - {ctr}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------- REDUNENT CODE ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VitWPModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VitWPModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(512)\n",
    "        self.conv3 = nn.Conv2d(512, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "        self.conv5 = nn.Conv2d(512, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 512)\n",
    "        self.fc2 = nn.Linear(512, 2)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        b = pixel_values.shape[0]\n",
    "        x = torch.relu(self.conv1(pixel_values))\n",
    "        x = self.bn1(x)\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        \n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.bn2(x)\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = self.bn3(x)\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "\n",
    "        x = torch.relu(self.conv4(x))\n",
    "        x = self.bn4(x)\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "\n",
    "        x = torch.relu(self.conv5(x))\n",
    "        x = self.bn5(x)\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        output = self.fc2(x)\n",
    "        return output.view(b,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BEVToImage(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BEVToImage, self).__init__()\n",
    "        self.backbone = models.resnet18(pretrained=True)\n",
    "        self.backbone = nn.Sequential(*list(self.backbone.children())[:-2])\n",
    "        self.conv1x1 = nn.Conv2d(512, 256, kernel_size=1)\n",
    "        \n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 512, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(512, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 1, kernel_size=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.conv1x1(x)\n",
    "        x = self.upsample(x)\n",
    "        x = F.interpolate(x, size=(200, 200), mode='bilinear', align_corners=False)\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size=3):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.conv = nn.Conv2d(input_dim + hidden_dim, hidden_dim * 4, kernel_size, padding=padding)\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        h, c = hidden\n",
    "        combined = torch.cat([x, h], dim=1)\n",
    "        conv_out = self.conv(combined)\n",
    "        i, f, g, o = torch.chunk(conv_out, 4, dim=1)\n",
    "        i, f, g, o = torch.sigmoid(i), torch.sigmoid(f), torch.tanh(g), torch.sigmoid(o)\n",
    "        c_next = f * c + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "        return h_next, c_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim, sequence_length):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.output_dim = output_dim\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "        self.lstm_cells = nn.ModuleList([\n",
    "            ConvLSTMCell(input_dim if i == 0 else hidden_dim, hidden_dim)\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "        self.fc = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, c, h, w = x.size()\n",
    "        h_states = [torch.zeros(b, self.hidden_dim, h, w, device=x.device) for _ in range(self.num_layers)]\n",
    "        c_states = [torch.zeros(b, self.hidden_dim, h, w, device=x.device) for _ in range(self.num_layers)]\n",
    "        \n",
    "        for time_step in range(t):\n",
    "            inp = x[:, time_step]\n",
    "            for i, cell in enumerate(self.lstm_cells):\n",
    "                h_states[i], c_states[i] = cell(inp, (h_states[i], c_states[i]))\n",
    "                inp = h_states[i]\n",
    "        \n",
    "        out = torch.flatten(inp, start_dim=1)\n",
    "\n",
    "        if self.fc is None:\n",
    "            self.fc = nn.Linear(self.hidden_dim * h * w, self.output_dim * self.sequence_length).to(x.device)\n",
    "        output = self.fc(out).contiguous().view(b, -1, self.output_dim)\n",
    "        # print(output[0][0])\n",
    "        return output[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageToBEV_t1_t2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageToBEV_t1_t2, self).__init__()\n",
    "        self.backbone = models.resnet18(pretrained=True)\n",
    "        self.backbone = nn.Sequential(*list(self.backbone.children())[:-2])\n",
    "        self.conv1x1 = nn.Conv2d(512, 256, kernel_size=1)\n",
    "        \n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 512, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(512, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 1, kernel_size=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.conv1x1(x)\n",
    "        x = self.upsample(x)\n",
    "        x = F.interpolate(x, size=(200, 200), mode='bilinear', align_corners=False)\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_WPModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_WPModel, self).__init__()\n",
    "\n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "        self.resnet_features = nn.Sequential(*list(resnet.children())[:-1])\n",
    "\n",
    "        # self.conv_layers = nn.Sequential(\n",
    "        #     nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
    "        #     nn.BatchNorm2d(32),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.MaxPool2d(2, 2),\n",
    "        #     nn.Conv2d(32, 256, kernel_size=3, stride=1, padding=1),\n",
    "        #     nn.BatchNorm2d(256),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.MaxPool2d(2, 2),\n",
    "        #     nn.Conv2d(256, 1024, kernel_size=3, stride=1, padding=1),\n",
    "        #     nn.BatchNorm2d(1024),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.MaxPool2d(2, 2),\n",
    "        #     nn.Conv2d(1024, 256, kernel_size=3, stride=1, padding=1),\n",
    "        #     nn.BatchNorm2d(256),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.MaxPool2d(2, 2),\n",
    "        #     nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "        #     nn.BatchNorm2d(512),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.MaxPool2d(2, 2),\n",
    "        # )\n",
    "        \n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(512 + 1, 512),\n",
    "            # nn.Linear(512 * 6 * 6 + 1, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 4),\n",
    "            nn.Dropout(0.25),\n",
    "        )\n",
    "\n",
    "    def forward(self, pixel_values, target_loc):\n",
    "        x = self.resnet_features(pixel_values)\n",
    "        # x = self.conv_layers(pixel_values)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.cat((x, target_loc.unsqueeze(0).view(-1, 1)), dim=1)\n",
    "        output = self.fc_layers(x)\n",
    "        \n",
    "        return output.view(-1,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CarLA simulation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "semantic_label_classes = [\"Traffic Sign\", \"Building\", \"Fence\", \"Other\", \"Pedestrian\", \"Pole\", \"Road Line\", \"Road\", \"Sidewalk\", \"Vegetation\", \"Car\", \"Wall\", \"Unlabeled\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = carla.Client('localhost', 2000)\n",
    "world = client.get_world()\n",
    "blueprint_library = world.get_blueprint_library()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_all_actors():\n",
    "    actors = world.get_actors()\n",
    "    for actor in actors:\n",
    "        if 'vehicle' in actor.type_id:\n",
    "            actor.destroy()\n",
    "    world.tick()\n",
    "    print(f\"\\rprevious ones destroyed\", end = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initiating_carla(world, blueprint_library):\n",
    "    ego_blueprint = blueprint_library.filter('*model3')[0]\n",
    "    spawn_points = world.get_map().get_spawn_points()[0]\n",
    "    global ego\n",
    "    ego = world.spawn_actor(ego_blueprint, spawn_points)\n",
    "\n",
    "    camera_bp = blueprint_library.find('sensor.camera.rgb')\n",
    "    camera_bp.set_attribute('image_size_x', '800')\n",
    "    camera_bp.set_attribute('image_size_y', '600')\n",
    "    camera_bp.set_attribute('fov', '110')\n",
    "    camera_transform = carla.Transform(carla.Location(x = 1.5, z = 2.4))\n",
    "    global camera\n",
    "    camera = world.spawn_actor(camera_bp, camera_transform, attach_to=ego)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_all_actors()\n",
    "initiating_carla(world, blueprint_library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EGO_relative_view(EGO):\n",
    "    view = EGO.get_transform().location + carla.Location(x=0, y=0, z=10)\n",
    "    return view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_cam_to_EGO(EGO, angle = 65):\n",
    "    spectator = world.get_spectator()\n",
    "    vehicle_transform = EGO.get_transform()\n",
    "    spectator.set_transform(\n",
    "        carla.Transform(\n",
    "            EGO_relative_view(EGO),\n",
    "            carla.Rotation(pitch = -angle, yaw = vehicle_transform.rotation.yaw, roll = 0)\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def camera_to_image(image):\n",
    "    global image_captured\n",
    "    array = np.frombuffer(image.raw_data, dtype=np.uint8)\n",
    "    array = array.reshape((image.height, image.width, 4))\n",
    "    image_captured = Image.fromarray(array[:, :, :3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_cam_to_EGO(ego)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "functions for model output to control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def steering(x, intended_x, s_power):\n",
    "    towards = intended_x - x\n",
    "    steer = towards * 0.0002\n",
    "    steer = steer * (1 + s_power) / 1.5\n",
    "    steer = max(-1, min(steer, 1))\n",
    "    return steer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TandB(y_mid, intended_y_mid, curr_y = 25):\n",
    "    if(intended_y_mid <= curr_y):\n",
    "        return 0, 1\n",
    "    intended_y_mid = intended_y_mid if intended_y_mid < y_mid else y_mid\n",
    "    return max(0.3, min((y_mid - intended_y_mid) / (2 * y_mid), 1)), 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def control_ego(steer, throttle, brake):\n",
    "    control = carla.VehicleControl()\n",
    "    control.steer = steer\n",
    "    control.throttle = throttle\n",
    "    control.brake = brake\n",
    "\n",
    "    return control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculation of metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*--route completion--*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_RC(intended_x_mid, ego_x_position, intended_y_mid, ego_y_position, th, d):\n",
    "    global dist_covered\n",
    "    if intended_y_mid > ego_y_position:\n",
    "        dist = math.sqrt(((intended_x_mid - ego_x_position) ** 2) + ((intended_y_mid - ego_y_position) ** 2))\n",
    "        if dist <= th:\n",
    "            dist_covered = dist_covered + d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*--infraction score--*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollisionSensor(object):\n",
    "    def __init__(self, vehicle):\n",
    "        self.vehicle = vehicle\n",
    "        self.world = vehicle.get_world()\n",
    "        self.collision_history = []\n",
    "\n",
    "        blueprint = self.world.get_blueprint_library().find('sensor.other.collision')\n",
    "        self.sensor = self.world.spawn_actor(blueprint, carla.Transform(), attach_to=self.vehicle)\n",
    "        self.sensor.listen(self._on_collision)\n",
    "\n",
    "    def _on_collision(self, event):\n",
    "        self.collision_history.append(event)\n",
    "        \n",
    "    def get_collision_count(self):\n",
    "        return len(self.collision_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_red_light_violation():\n",
    "    global red_light_violations\n",
    "    ego_position = ego.get_location()\n",
    "    traffic_lights = world.get_traffic_lights()\n",
    "    red_light_violations = False\n",
    "    for traffic_light in traffic_lights:\n",
    "        light_position = traffic_light.get_location()\n",
    "        distance_to_light = ego_position.distance(light_position)\n",
    "        if distance_to_light < 20.0:\n",
    "            if traffic_light.get_state() == carla.TrafficLightState.Red:\n",
    "                red_light_violations = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_lane_violation():\n",
    "    global lane_violations\n",
    "    if ego.is_outside_lane():\n",
    "        lane_violations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_tailgating():\n",
    "    global tailgating_violations\n",
    "    front_vehicle = ego.get_closest_vehicle_ahead()\n",
    "    if front_vehicle:\n",
    "        distance = ego.get_location().distance(front_vehicle.get_location())\n",
    "        speed = ego.get_velocity().length()\n",
    "        safe_distance = max(5, speed * 0.5)  # Example: 0.5 sec rule\n",
    "        if distance < safe_distance:\n",
    "            tailgating_violations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_illegal_turns():\n",
    "    global illegal_turn_violations\n",
    "    if ego.made_illegal_turn():  # Assuming a function to detect illegal turns\n",
    "        illegal_turn_violations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_stop_sign_violation():\n",
    "    global stop_sign_violations\n",
    "    stop_signs = world.get_stop_signs()\n",
    "    ego_position = ego.get_location()\n",
    "    for stop_sign in stop_signs:\n",
    "        stop_position = stop_sign.get_location()\n",
    "        distance = ego_position.distance(stop_position)\n",
    "        if distance < 10.0 and ego.get_velocity().length() > 0.5:  # Didn't stop\n",
    "            stop_sign_violations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_IS(speed_limit, red_light_violations, collision_sensor, lane_violations, tailgating_violations, illegal_turn_violations, stop_sign_violations):\n",
    "    global isc\n",
    "\n",
    "    velocity = ego.get_velocity()\n",
    "    speed = (velocity.x**2 + velocity.y**2 + velocity.z**2) ** 0.5 * 3.6\n",
    "    \n",
    "    if speed > speed_limit:\n",
    "        isc += 1\n",
    "        \n",
    "    if red_light_violations:\n",
    "        isc += 1\n",
    "\n",
    "    if collision_sensor.get_collision_count() > 0:\n",
    "        isc += 10\n",
    "\n",
    "    if lane_violations:\n",
    "        isc += 2\n",
    "\n",
    "    if tailgating_violations:\n",
    "        isc += 2\n",
    "\n",
    "    if illegal_turn_violations:\n",
    "        isc += 2\n",
    "\n",
    "    if stop_sign_violations:\n",
    "        isc += 2\n",
    "\n",
    "    if isc > 100:\n",
    "        isc = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*--driving score--*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_DS(RC, IS):\n",
    "    return RC * (100 - IS) / 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "controlling based on models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*--vit_wp_model--*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_emb(image):\n",
    "    return processor(text=semantic_label_classes, images=image, return_tensors=\"pt\", padding=True)['pixel_values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_output_generation(image, vit_wp_model):\n",
    "    pixel_values = image_to_emb(image).squeeze(1)\n",
    "    outputs = vit_wp_model(pixel_values)\n",
    "    op = outputs.squeeze()\n",
    "    op[0] = op[0] * 1.05\n",
    "    op[1] = 600 - op[1]\n",
    "    return op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_wp(way_p_1, top_w,top_h):\n",
    "    way_p = way_p_1\n",
    "\n",
    "    mini_f = float('inf')\n",
    "    maxi_f = -float('inf')\n",
    "    for i in way_p[0]:\n",
    "        if(maxi_f < i[0]):\n",
    "            maxi_f = i[0]\n",
    "        if(mini_f > i[0]):\n",
    "            mini_f = i[0]\n",
    "        \n",
    "    mini_s = float('inf')\n",
    "    maxi_s = -float('inf')\n",
    "    for i in way_p[1]:\n",
    "        if(maxi_s < i[1]):\n",
    "            maxi_s = i[1]\n",
    "        if(mini_s > i[1]):\n",
    "            mini_s = i[1]\n",
    "        \n",
    "    for i in way_p:\n",
    "        i[0] = (i[0] - mini_f) / (maxi_f - mini_f)\n",
    "        i[1] = (i[1] - mini_s) / (maxi_s - mini_s)\n",
    "        i[0] *= top_h\n",
    "        i[1] *= top_w\n",
    "\n",
    "    return way_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def points():\n",
    "    return 400, 139"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th = 15\n",
    "driver_model = VitWPModel()\n",
    "driver_model = torch.load('models/VitWPModel.pth', map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*--conv_LSTM_model--*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_output_generation(image, driver_model):\n",
    "    bev = image_to_bev_model(torch.tensor(np.array(image, dtype = np.float32) / 255.0, dtype=torch.float32).unsqueeze(0).permute(0,3,1,2)).unsqueeze(0)\n",
    "    bev = bev.unsqueeze(0)\n",
    "    outputs = driver_model(bev.unsqueeze(0))\n",
    "    op = outputs.squeeze()\n",
    "    op[0] = 1.1 * op[0] / 50\n",
    "    op[1] = op[1] / 50\n",
    "    # op[1] = 600 - op[1]\n",
    "    return op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def points():\n",
    "    return 400, 324.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_to_bev_model = BEVToImage()\n",
    "image_to_bev_model = torch.load('models/ImageToBEV.pth', map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th = 750\n",
    "\n",
    "input_dim, hidden_dim, num_layers, output_dim, seq_length = 1, 64, 2, 2, 5\n",
    "driver_model = ConvLSTM(input_dim, hidden_dim, num_layers, output_dim, seq_length)\n",
    "driver_model = torch.load('models/ConvLSTM.pth', map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*--CNN_wp_model--*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_relative_to_ego(ego_transform, point_world):\n",
    "    ego_loc = ego_transform.location\n",
    "    ego_rot = ego_transform.rotation\n",
    "    \n",
    "    yaw_rad = np.radians(ego_rot.yaw)\n",
    "    \n",
    "    R = np.array([\n",
    "        [np.cos(-yaw_rad), -np.sin(-yaw_rad)],\n",
    "        [np.sin(-yaw_rad),  np.cos(-yaw_rad)]\n",
    "    ])\n",
    "    \n",
    "    translated_point = np.array([point_world.x - ego_loc.x, point_world.y - ego_loc.y])\n",
    "    \n",
    "    local_point = R @ translated_point\n",
    "    \n",
    "    return local_point[1], local_point[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ego_rotation(local_x, local_y):\n",
    "    rel = math.degrees(math.atan(local_x / local_y))\n",
    "    return rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_points_from_gps(target):\n",
    "    ego_transform = ego.get_transform()\n",
    "    point_world = carla.Location(*target)\n",
    "\n",
    "    local_x, local_y = target_relative_to_ego(ego_transform, point_world)\n",
    "    dist_from_ego = ((local_x ** 2) + (local_y ** 2)) ** (1/2)\n",
    "    rel = ego_rotation(local_x, local_y)\n",
    "\n",
    "    rel = max(min(rel, 50), -50)\n",
    "    local_y = max(min(local_y, 37.5), 0)\n",
    "\n",
    "    x_to_plot = 400 + (8 * rel)\n",
    "    y_to_plot = 600 - (25 * local_y)\n",
    "\n",
    "    return dist_from_ego, rel, x_to_plot, y_to_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bev_on_graph(local_x, local_y):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(0, 0, color='red', label='e', s=100)\n",
    "    plt.scatter(local_x, local_y, color='blue', label='t')\n",
    "    plt.xlim(-800, 800)\n",
    "    plt.ylim(-600, 600)\n",
    "    plt.xlabel(\"X (meters)\")\n",
    "    plt.ylabel(\"Y (meters)\")\n",
    "    plt.axhline(0, color='black',linewidth=1)\n",
    "    plt.axvline(0, color='black',linewidth=1)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_road_map(vehicle, world):\n",
    "    curr_loc = vehicle.get_location()\n",
    "    map = world.get_map()\n",
    "    wps = map.get_waypoint(curr_loc,project_to_road=True, lane_type=(carla.LaneType.Driving | carla.LaneType.Sidewalk))\n",
    "\n",
    "    my_geolocation = map.transform_to_geolocation(vehicle.get_transform().location)\n",
    "    all_map_waypoints = map.generate_waypoints(2.0)\n",
    "    waypoints_on_map = map.get_topology()\n",
    "\n",
    "    return waypoints_on_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(loc1, loc2):\n",
    "    return math.sqrt((loc1.x - loc2.x) ** 2 + (loc1.y - loc2.y) ** 2 + (loc1.z - loc2.z) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_graph(junction_waypoints):\n",
    "    graph = {}\n",
    "    ego_loc = (ego.get_transform().location.x, ego.get_transform().location.y, ego.get_transform().location.z)\n",
    "    if ego_loc not in graph:\n",
    "        graph[ego_loc] = []\n",
    "    to = (junction_waypoints[0][0].transform.location.x, junction_waypoints[0][0].transform.location.y, junction_waypoints[0][0].transform.location.z)\n",
    "    graph[ego_loc].append((to, 0.0))\n",
    "    \n",
    "    for start_wp, end_wp in junction_waypoints:\n",
    "        start_loc = (start_wp.transform.location.x, start_wp.transform.location.y, start_wp.transform.location.z)\n",
    "        end_loc = (end_wp.transform.location.x, end_wp.transform.location.y, end_wp.transform.location.z)\n",
    "        dist = distance(start_wp.transform.location, end_wp.transform.location)\n",
    "\n",
    "        if start_loc not in graph:\n",
    "            graph[start_loc] = []\n",
    "        graph[start_loc].append((end_loc, dist))\n",
    "\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dijkstra(graph, start):\n",
    "    queue = [(0, start)]\n",
    "    distances = {node: float('inf') for node in graph}\n",
    "    distances[start] = 0\n",
    "    \n",
    "    previous_nodes = {}\n",
    "\n",
    "    while queue:\n",
    "        current_distance, current_node = heapq.heappop(queue)\n",
    "        for neighbor, weight in graph.get(current_node, []):\n",
    "            new_distance = current_distance + weight\n",
    "            if new_distance < distances[neighbor]:\n",
    "                distances[neighbor] = new_distance\n",
    "                previous_nodes[neighbor] = current_node\n",
    "                heapq.heappush(queue, (new_distance, neighbor))\n",
    "\n",
    "    return distances, previous_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_path(previous_nodes, start, goal):\n",
    "    path = []\n",
    "    current = goal\n",
    "    while current != start:\n",
    "        path.append(current)\n",
    "        current = previous_nodes.get(current)\n",
    "        if current is None:\n",
    "            return None\n",
    "    path.append(start)\n",
    "    path.reverse()\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_ego_loc = ego.get_transform().location\n",
    "initiation = (curr_ego_loc.x, curr_ego_loc.y, curr_ego_loc.z)\n",
    "goal = (158.0238037109375, 46.202396392822266, 0.0)\n",
    "\n",
    "wps = make_graph(get_road_map(ego, world))\n",
    "distances, prev_nodes = dijkstra(wps, initiation)\n",
    "path = reconstruct_path(prev_nodes, initiation, goal)\n",
    "print(\"Shortest path:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_output_generation(image, model, locs = [400, 300]):\n",
    "    np_image = np.array(image, dtype = np.float32) / 255.0\n",
    "    tensor_image = torch.tensor(np_image, dtype=torch.float32)\n",
    "    tensor_image = tensor_image.permute(2,0,1).unsqueeze(0)\n",
    "    bev_op = image_to_bev_t1_t2_model(tensor_image)\n",
    "\n",
    "    bev_maps = bev_op.squeeze()\n",
    "    bev = bev_maps.unsqueeze(0).permute(1,0,2,3).repeat(1, 3, 1, 1)\n",
    "    locs = torch.tensor(locs, dtype = torch.float32)\n",
    "\n",
    "    # print(bev.shape, locs.shape)\n",
    "    outputs = driver_model(bev, locs)\n",
    "    # print(outputs)\n",
    "    op = torch.tensor([outputs[:,0].mean(),outputs[:,1].mean()])\n",
    "    op[0] = op[0] * 1.8\n",
    "    op[1] = op[1]\n",
    "\n",
    "    # print(op)\n",
    "    return op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def points():\n",
    "    return 400, 140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_to_bev_t1_t2_model = ImageToBEV_t1_t2()\n",
    "image_to_bev_t1_t2_model = torch.load('models/ImageToBEV_t1_t2.pth', map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th = 200\n",
    "\n",
    "driver_model = CNN_WPModel()\n",
    "driver_model = torch.load('models/CNN_WPModel.pth', map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RUNNING THE SIMULATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # print(\"started\")\n",
    "\n",
    "    prev_image_captured = None\n",
    "    image_captured = None\n",
    "    init_pos = ego.get_transform().location\n",
    "\n",
    "    RC = 0\n",
    "    IS = 0\n",
    "    DS = 0\n",
    "    dist_covered = 0\n",
    "    distance = 0\n",
    "    isc = 0\n",
    "    speed_limit = 50\n",
    "\n",
    "    count_dist = 0\n",
    "    dist_counter = 0\n",
    "\n",
    "    red_light_violations = False\n",
    "    collision_sensor = CollisionSensor(ego)\n",
    "    lane_violations = False\n",
    "    tailgating_violations = False\n",
    "    illegal_turn_violations = False\n",
    "    stop_sign_violations = False\n",
    "    set_cam_to_EGO(ego)\n",
    "    camera.listen(camera_to_image) #changes image_captured globally\n",
    "\n",
    "    while image_captured is None:\n",
    "        world.tick()\n",
    "        start_time = time.time()\n",
    "\n",
    "    while True:\n",
    "        world.wait_for_tick()\n",
    "        if not prev_image_captured == image_captured:\n",
    "            dist_counter += 1\n",
    "\n",
    "            scene_x, scene_y = 800, 600\n",
    "            ego_x_mid = scene_x/2\n",
    "            ego_y_mid = scene_y/2\n",
    "\n",
    "            outputs = model_output_generation(image_captured, driver_model)\n",
    "            waypoint = abs(outputs.detach().numpy())\n",
    "\n",
    "            ego_x_position, ego_y_position = points()\n",
    "            \n",
    "            intended_x = waypoint[0]\n",
    "            intended_y = waypoint[1]\n",
    "\n",
    "            if dist_counter % 2 == 0:\n",
    "                init_pos = ego.get_transform().location\n",
    "                d = 0\n",
    "            else:\n",
    "                curr_pos = ego.get_transform().location\n",
    "                dx = abs(curr_pos.x - init_pos.x)\n",
    "                dy = abs(curr_pos.y - init_pos.y)\n",
    "                dz = abs(curr_pos.z - init_pos.z)\n",
    "                d = math.sqrt((dx ** 2) + (dy ** 2) + (dz ** 2))\n",
    "                distance += d\n",
    "                evaluate_RC(intended_x, ego_x_position, intended_y, ego_y_position, th, d)\n",
    "                evaluate_IS(speed_limit, red_light_violations, collision_sensor, lane_violations, tailgating_violations, illegal_turn_violations, stop_sign_violations)\n",
    "\n",
    "                count_dist += math.sqrt(((intended_x - ego_x_position) ** 2) + ((intended_y - ego_y_position) ** 2))\n",
    "\n",
    "                # if distance > 0:\n",
    "                #     print(f\"\\r{dist_covered:.2f} / {distance:.2f}, {isc:.2f}        \", end = \"\")\n",
    "                    # break\n",
    "\n",
    "            throttle, brake = TandB(ego_y_mid, intended_y, ego_y_position)\n",
    "            velocity = ego.get_velocity()\n",
    "            speed = math.sqrt((velocity.x**2 + velocity.y**2 + velocity.z**2)) / 5\n",
    "            s_power = (1 - brake) * (1 - speed)\n",
    "            steer = steering(ego_x_position, intended_x, s_power)\n",
    "\n",
    "            print(f\"\\rgoing towards: ({intended_x:.1f}, {intended_y:.1f}), with b:{brake}, t:{throttle:.1f}, s:{steer:.1f}      \", end = \"\")\n",
    "\n",
    "            ego.apply_control(control_ego(steer, throttle, brake))\n",
    "            set_cam_to_EGO(ego)\n",
    "            \n",
    "            red_light_violations = False\n",
    "            collision_sensor = CollisionSensor(ego)\n",
    "            lane_violations = False\n",
    "            tailgating_violations = False\n",
    "            illegal_turn_violations = False\n",
    "            stop_sign_violations = False\n",
    "\n",
    "        if time.time() >= start_time + 60:\n",
    "            break\n",
    "        prev_image_captured = image_captured\n",
    "        \n",
    "    if(distance > 0):\n",
    "        RC = dist_covered * 100 / distance\n",
    "        IS = isc\n",
    "        DS = evaluate_DS(RC, IS)\n",
    "\n",
    "    print(f\"\\nRC is: {RC:.2f}, IS is: {IS:.2f}, DS is: {DS:.2f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n{e}\")\n",
    "finally:\n",
    "    camera.stop()\n",
    "\n",
    "    ego.apply_control(control_ego(0,0,1))\n",
    "\n",
    "    remove_all_actors()\n",
    "    initiating_carla(world, blueprint_library)\n",
    "    set_cam_to_EGO(ego, 65)\n",
    "\n",
    "    # print(\"stopped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RC is: 81.51, IS is: 10.00, DS is: 73.36; 5+\n",
    "# RC is: 89.83, IS is: 20.00, DS is: 71.86; 21+\n",
    "# RC is: 41.85, IS is: 60.00, DS is: 16.74; 75+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "metric evaluation (final model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_count = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averages_m1 = [0,0,0]\n",
    "for my_i in range (0, iter_count):\n",
    "    try:\n",
    "        # print(\"started\")\n",
    "\n",
    "        prev_image_captured = None\n",
    "        image_captured = None\n",
    "        init_pos = ego.get_transform().location\n",
    "\n",
    "        RC = 0\n",
    "        IS = 0\n",
    "        DS = 0\n",
    "        dist_covered = 0\n",
    "        distance = 0\n",
    "        isc = 0\n",
    "        speed_limit = 50\n",
    "\n",
    "        count_dist = 0\n",
    "        dist_counter = 0\n",
    "\n",
    "                \n",
    "        red_light_violations = False\n",
    "        collision_sensor = CollisionSensor(ego)\n",
    "        lane_violations = False\n",
    "        tailgating_violations = False\n",
    "        illegal_turn_violations = False\n",
    "        stop_sign_violations = False\n",
    "        set_cam_to_EGO(ego)\n",
    "        camera.listen(camera_to_image) #changes image_captured globally\n",
    "\n",
    "        while image_captured is None:\n",
    "            world.tick()\n",
    "            start_time = time.time()\n",
    "\n",
    "        while True:\n",
    "            world.wait_for_tick()\n",
    "            if not prev_image_captured == image_captured:\n",
    "                dist_counter += 1\n",
    "\n",
    "                scene_x, scene_y = 800, 600\n",
    "                ego_x_mid = scene_x/2\n",
    "                ego_y_mid = scene_y/2\n",
    "\n",
    "                outputs = model_output_generation(image_captured, driver_model)\n",
    "                waypoint = abs(outputs.detach().numpy())\n",
    "\n",
    "                ego_x_position, ego_y_position = points()\n",
    "                \n",
    "                intended_x = waypoint[0]\n",
    "                intended_y = waypoint[1]\n",
    "\n",
    "                if dist_counter % 2 == 0:\n",
    "                    init_pos = ego.get_transform().location\n",
    "                    d = 0\n",
    "                else:\n",
    "                    curr_pos = ego.get_transform().location\n",
    "                    dx = abs(curr_pos.x - init_pos.x)\n",
    "                    dy = abs(curr_pos.y - init_pos.y)\n",
    "                    dz = abs(curr_pos.z - init_pos.z)\n",
    "                    d = math.sqrt((dx ** 2) + (dy ** 2) + (dz ** 2))\n",
    "                    distance += d\n",
    "                    evaluate_RC(intended_x, ego_x_position, intended_y, ego_y_position, th, d)\n",
    "                    evaluate_IS(speed_limit, red_light_violations, collision_sensor, lane_violations, tailgating_violations, illegal_turn_violations, stop_sign_violations)\n",
    "\n",
    "                    count_dist += math.sqrt(((intended_x - ego_x_position) ** 2) + ((intended_y - ego_y_position) ** 2))\n",
    "\n",
    "                    # if distance > 0:\n",
    "                    #     print(f\"\\r{dist_covered:.2f} / {distance:.2f}, {isc:.2f}        \", end = \"\")\n",
    "                        # break\n",
    "\n",
    "                throttle, brake = TandB(ego_y_mid, intended_y, ego_y_position)\n",
    "                velocity = ego.get_velocity()\n",
    "                speed = math.sqrt((velocity.x**2 + velocity.y**2 + velocity.z**2)) / 5\n",
    "                s_power = (1 - brake) * (1 - speed)\n",
    "                steer = steering(ego_x_position, intended_x, s_power)\n",
    "\n",
    "                # print(f\"\\rgoing towards: ({intended_x:.1f}, {intended_y:.1f}), with b:{brake}, t:{throttle:.1f}, s:{steer:.1f}      \", end = \"\")\n",
    "\n",
    "                ego.apply_control(control_ego(steer, throttle, brake))\n",
    "                set_cam_to_EGO(ego)\n",
    "                \n",
    "                red_light_violations = False\n",
    "                collision_sensor = CollisionSensor(ego)\n",
    "                lane_violations = False\n",
    "                tailgating_violations = False\n",
    "                illegal_turn_violations = False\n",
    "                stop_sign_violations = False\n",
    "\n",
    "            if time.time() >= start_time + 90:\n",
    "                break\n",
    "            prev_image_captured = image_captured\n",
    "            \n",
    "        if(distance > 0):\n",
    "            RC = dist_covered * 100 / distance\n",
    "            IS = isc\n",
    "            DS = evaluate_DS(RC, IS)\n",
    "        averages_m1[0] += RC\n",
    "        averages_m1[1] += IS\n",
    "        averages_m1[2] += DS\n",
    "        # print(f\"\\nRC is: {RC:.2f}, IS is: {IS:.2f}, DS is: {DS:.2f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n{e}\")\n",
    "    finally:\n",
    "        camera.stop()\n",
    "\n",
    "        ego.apply_control(control_ego(0,0,1))\n",
    "\n",
    "        remove_all_actors()\n",
    "        initiating_carla(world, blueprint_library)\n",
    "        set_cam_to_EGO(ego, 65)\n",
    "\n",
    "        # print(\"stopped\")\n",
    "\n",
    "for my_i in range(len(averages_m1)):\n",
    "    averages_m1[my_i] = averages_m1[my_i] / iter_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averages_m2 = [0,0,0]\n",
    "for my_i in range (0, iter_count):\n",
    "    try:\n",
    "        # print(\"started\")\n",
    "\n",
    "        prev_image_captured = None\n",
    "        image_captured = None\n",
    "        init_pos = ego.get_transform().location\n",
    "\n",
    "        RC = 0\n",
    "        IS = 0\n",
    "        DS = 0\n",
    "        dist_covered = 0\n",
    "        distance = 0\n",
    "        isc = 0\n",
    "        speed_limit = 50\n",
    "\n",
    "        count_dist = 0\n",
    "        dist_counter = 0\n",
    "\n",
    "                \n",
    "        red_light_violations = False\n",
    "        collision_sensor = CollisionSensor(ego)\n",
    "        lane_violations = False\n",
    "        tailgating_violations = False\n",
    "        illegal_turn_violations = False\n",
    "        stop_sign_violations = False\n",
    "        set_cam_to_EGO(ego)\n",
    "        camera.listen(camera_to_image) #changes image_captured globally\n",
    "\n",
    "        while image_captured is None:\n",
    "            world.tick()\n",
    "            start_time = time.time()\n",
    "\n",
    "        while True:\n",
    "            world.wait_for_tick()\n",
    "            if not prev_image_captured == image_captured:\n",
    "                dist_counter += 1\n",
    "\n",
    "                scene_x, scene_y = 800, 600\n",
    "                ego_x_mid = scene_x/2\n",
    "                ego_y_mid = scene_y/2\n",
    "\n",
    "                outputs = model_output_generation(image_captured, driver_model)\n",
    "                waypoint = abs(outputs.detach().numpy())\n",
    "\n",
    "                ego_x_position, ego_y_position = points()\n",
    "                \n",
    "                intended_x = waypoint[0]\n",
    "                intended_y = waypoint[1]\n",
    "\n",
    "                if dist_counter % 2 == 0:\n",
    "                    init_pos = ego.get_transform().location\n",
    "                    d = 0\n",
    "                else:\n",
    "                    curr_pos = ego.get_transform().location\n",
    "                    dx = abs(curr_pos.x - init_pos.x)\n",
    "                    dy = abs(curr_pos.y - init_pos.y)\n",
    "                    dz = abs(curr_pos.z - init_pos.z)\n",
    "                    d = math.sqrt((dx ** 2) + (dy ** 2) + (dz ** 2))\n",
    "                    distance += d\n",
    "                    evaluate_RC(intended_x, ego_x_position, intended_y, ego_y_position, th, d)\n",
    "                    evaluate_IS(speed_limit, red_light_violations, collision_sensor, lane_violations, tailgating_violations, illegal_turn_violations, stop_sign_violations)\n",
    "\n",
    "                    count_dist += math.sqrt(((intended_x - ego_x_position) ** 2) + ((intended_y - ego_y_position) ** 2))\n",
    "\n",
    "                    # if distance > 0:\n",
    "                    #     print(f\"\\r{dist_covered:.2f} / {distance:.2f}, {isc:.2f}        \", end = \"\")\n",
    "                        # break\n",
    "\n",
    "                throttle, brake = TandB(ego_y_mid, intended_y, ego_y_position)\n",
    "                velocity = ego.get_velocity()\n",
    "                speed = math.sqrt((velocity.x**2 + velocity.y**2 + velocity.z**2)) / 5\n",
    "                s_power = (1 - brake) * (1 - speed)\n",
    "                steer = steering(ego_x_position, intended_x, s_power)\n",
    "\n",
    "                # print(f\"\\rgoing towards: ({intended_x:.1f}, {intended_y:.1f}), with b:{brake}, t:{throttle:.1f}, s:{steer:.1f}      \", end = \"\")\n",
    "\n",
    "                ego.apply_control(control_ego(steer, throttle, brake))\n",
    "                set_cam_to_EGO(ego)\n",
    "                \n",
    "                red_light_violations = False\n",
    "                collision_sensor = CollisionSensor(ego)\n",
    "                lane_violations = False\n",
    "                tailgating_violations = False\n",
    "                illegal_turn_violations = False\n",
    "                stop_sign_violations = False\n",
    "\n",
    "            if time.time() >= start_time + 90:\n",
    "                break\n",
    "            prev_image_captured = image_captured\n",
    "            \n",
    "        if(distance > 0):\n",
    "            RC = dist_covered * 100 / distance\n",
    "            IS = isc\n",
    "            DS = evaluate_DS(RC, IS)\n",
    "        averages_m2[0] += RC\n",
    "        averages_m2[1] += IS\n",
    "        averages_m2[2] += DS\n",
    "        # print(f\"\\nRC is: {RC:.2f}, IS is: {IS:.2f}, DS is: {DS:.2f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n{e}\")\n",
    "    finally:\n",
    "        camera.stop()\n",
    "\n",
    "        ego.apply_control(control_ego(0,0,1))\n",
    "\n",
    "        remove_all_actors()\n",
    "        initiating_carla(world, blueprint_library)\n",
    "        set_cam_to_EGO(ego, 65)\n",
    "\n",
    "        # print(\"stopped\")\n",
    "\n",
    "for my_i in range(len(averages_m2)):\n",
    "    averages_m2[my_i] = averages_m2[my_i] / iter_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averages_m3 = [0,0,0]\n",
    "for my_i in range (0, iter_count):\n",
    "    try:\n",
    "        # print(\"started\")\n",
    "\n",
    "        prev_image_captured = None\n",
    "        image_captured = None\n",
    "        init_pos = ego.get_transform().location\n",
    "\n",
    "        RC = 0\n",
    "        IS = 0\n",
    "        DS = 0\n",
    "        dist_covered = 0\n",
    "        distance = 0\n",
    "        isc = 0\n",
    "        speed_limit = 50\n",
    "\n",
    "        count_dist = 0\n",
    "        dist_counter = 0\n",
    "\n",
    "                \n",
    "        red_light_violations = False\n",
    "        collision_sensor = CollisionSensor(ego)\n",
    "        lane_violations = False\n",
    "        tailgating_violations = False\n",
    "        illegal_turn_violations = False\n",
    "        stop_sign_violations = False\n",
    "        set_cam_to_EGO(ego)\n",
    "        camera.listen(camera_to_image) #changes image_captured globally\n",
    "\n",
    "        while image_captured is None:\n",
    "            world.tick()\n",
    "            start_time = time.time()\n",
    "\n",
    "        while True:\n",
    "            world.wait_for_tick()\n",
    "            if not prev_image_captured == image_captured:\n",
    "                dist_counter += 1\n",
    "\n",
    "                scene_x, scene_y = 800, 600\n",
    "                ego_x_mid = scene_x/2\n",
    "                ego_y_mid = scene_y/2\n",
    "\n",
    "                outputs = model_output_generation(image_captured, driver_model)\n",
    "                waypoint = abs(outputs.detach().numpy())\n",
    "\n",
    "                ego_x_position, ego_y_position = points()\n",
    "                \n",
    "                intended_x = waypoint[0]\n",
    "                intended_y = waypoint[1]\n",
    "\n",
    "                if dist_counter % 2 == 0:\n",
    "                    init_pos = ego.get_transform().location\n",
    "                    d = 0\n",
    "                else:\n",
    "                    curr_pos = ego.get_transform().location\n",
    "                    dx = abs(curr_pos.x - init_pos.x)\n",
    "                    dy = abs(curr_pos.y - init_pos.y)\n",
    "                    dz = abs(curr_pos.z - init_pos.z)\n",
    "                    d = math.sqrt((dx ** 2) + (dy ** 2) + (dz ** 2))\n",
    "                    distance += d\n",
    "                    evaluate_RC(intended_x, ego_x_position, intended_y, ego_y_position, th, d)\n",
    "                    evaluate_IS(speed_limit, red_light_violations, collision_sensor, lane_violations, tailgating_violations, illegal_turn_violations, stop_sign_violations)\n",
    "\n",
    "                    count_dist += math.sqrt(((intended_x - ego_x_position) ** 2) + ((intended_y - ego_y_position) ** 2))\n",
    "\n",
    "                    # if distance > 0:\n",
    "                    #     print(f\"\\r{dist_covered:.2f} / {distance:.2f}, {isc:.2f}        \", end = \"\")\n",
    "                        # break\n",
    "\n",
    "                throttle, brake = TandB(ego_y_mid, intended_y, ego_y_position)\n",
    "                velocity = ego.get_velocity()\n",
    "                speed = math.sqrt((velocity.x**2 + velocity.y**2 + velocity.z**2)) / 5\n",
    "                s_power = (1 - brake) * (1 - speed)\n",
    "                steer = steering(ego_x_position, intended_x, s_power)\n",
    "\n",
    "                # print(f\"\\rgoing towards: ({intended_x:.1f}, {intended_y:.1f}), with b:{brake}, t:{throttle:.1f}, s:{steer:.1f}      \", end = \"\")\n",
    "\n",
    "                ego.apply_control(control_ego(steer, throttle, brake))\n",
    "                set_cam_to_EGO(ego)\n",
    "                \n",
    "                red_light_violations = False\n",
    "                collision_sensor = CollisionSensor(ego)\n",
    "                lane_violations = False\n",
    "                tailgating_violations = False\n",
    "                illegal_turn_violations = False\n",
    "                stop_sign_violations = False\n",
    "\n",
    "            if time.time() >= start_time + 90:\n",
    "                break\n",
    "            prev_image_captured = image_captured\n",
    "            \n",
    "        if(distance > 0):\n",
    "            RC = dist_covered * 100 / distance\n",
    "            IS = isc\n",
    "            DS = evaluate_DS(RC, IS)\n",
    "        averages_m3[0] += RC\n",
    "        averages_m3[1] += IS\n",
    "        averages_m3[2] += DS\n",
    "        # print(f\"\\nRC is: {RC:.2f}, IS is: {IS:.2f}, DS is: {DS:.2f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n{e}\")\n",
    "    finally:\n",
    "        camera.stop()\n",
    "\n",
    "        ego.apply_control(control_ego(0,0,1))\n",
    "\n",
    "        remove_all_actors()\n",
    "        initiating_carla(world, blueprint_library)\n",
    "        set_cam_to_EGO(ego, 65)\n",
    "\n",
    "        # print(\"stopped\")\n",
    "\n",
    "for my_i in range(len(averages_m3)):\n",
    "    averages_m3[my_i] = averages_m3[my_i] / iter_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(averages_m1, averages_m2, averages_m3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\"Route Completion\", \"Infraction Score\", \"Driving Score\"]\n",
    "\n",
    "x = np.arange(len(categories))\n",
    "width = 0.1\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(x - width, averages_m1, width=width, label=\"Algorithm 1\", color=\"b\")\n",
    "plt.bar(x, averages_m2, width=width, label=\"Algorithm 2\", color=\"g\")\n",
    "plt.bar(x + width, averages_m3, width=width, label=\"Algorithm 3\", color=\"r\")\n",
    "plt.xlabel(\"Metrics\")\n",
    "plt.ylabel(\"Values\")\n",
    "plt.ylim(0, 100)\n",
    "plt.xticks(x, categories)\n",
    "plt.title(\"Comparison of RC, IS, and DS for the three algorithms\")\n",
    "plt.legend()\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "metric evaluation (intermediate models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = []\n",
    "for my_i in range (1,iterations+1):\n",
    "    driver_model = torch.load(f\"models/A1/update count - {my_i}.pth\", map_location=torch.device('cpu'), weights_only=False)\n",
    "    try:\n",
    "        # print(\"started\")\n",
    "\n",
    "        prev_image_captured = None\n",
    "        image_captured = None\n",
    "        init_pos = ego.get_transform().location\n",
    "\n",
    "        RC = 0\n",
    "        IS = 0\n",
    "        DS = 0\n",
    "        dist_covered = 0\n",
    "        distance = 0\n",
    "        isc = 0\n",
    "        speed_limit = 50\n",
    "\n",
    "        count_dist = 0\n",
    "        dist_counter = 0\n",
    "\n",
    "        red_light_violations = False\n",
    "        collision_sensor = CollisionSensor(ego)\n",
    "        lane_violations = False\n",
    "        tailgating_violations = False\n",
    "        illegal_turn_violations = False\n",
    "        stop_sign_violations = False\n",
    "        set_cam_to_EGO(ego)\n",
    "        camera.listen(camera_to_image) #changes image_captured globally\n",
    "\n",
    "        while image_captured is None:\n",
    "            world.tick()\n",
    "            start_time = time.time()\n",
    "\n",
    "        while True:\n",
    "            world.wait_for_tick()\n",
    "            if not prev_image_captured == image_captured:\n",
    "                dist_counter += 1\n",
    "\n",
    "                scene_x, scene_y = 800, 600\n",
    "                ego_x_mid = scene_x/2\n",
    "                ego_y_mid = scene_y/2\n",
    "\n",
    "                outputs = model_output_generation(image_captured, driver_model)\n",
    "                waypoint = abs(outputs.detach().numpy())\n",
    "\n",
    "                ego_x_position, ego_y_position = points()\n",
    "                \n",
    "                intended_x = waypoint[0]\n",
    "                intended_y = waypoint[1]\n",
    "\n",
    "                if dist_counter % 2 == 0:\n",
    "                    init_pos = ego.get_transform().location\n",
    "                    d = 0\n",
    "                else:\n",
    "                    curr_pos = ego.get_transform().location\n",
    "                    dx = abs(curr_pos.x - init_pos.x)\n",
    "                    dy = abs(curr_pos.y - init_pos.y)\n",
    "                    dz = abs(curr_pos.z - init_pos.z)\n",
    "                    d = math.sqrt((dx ** 2) + (dy ** 2) + (dz ** 2))\n",
    "                    distance += d\n",
    "                    evaluate_RC(intended_x, ego_x_position, intended_y, ego_y_position, th, d)\n",
    "                    evaluate_IS(speed_limit, red_light_violations, collision_sensor, lane_violations, tailgating_violations, illegal_turn_violations, stop_sign_violations)\n",
    "\n",
    "                    count_dist += math.sqrt(((intended_x - ego_x_position) ** 2) + ((intended_y - ego_y_position) ** 2))\n",
    "\n",
    "                    # if distance > 0:\n",
    "                    #     print(f\"\\r{dist_covered:.2f} / {distance:.2f}, {isc:.2f}        \", end = \"\")\n",
    "                        # break\n",
    "\n",
    "                throttle, brake = TandB(ego_y_mid, intended_y, ego_y_position)\n",
    "                velocity = ego.get_velocity()\n",
    "                speed = math.sqrt((velocity.x**2 + velocity.y**2 + velocity.z**2)) / 5\n",
    "                s_power = (1 - brake) * (1 - speed)\n",
    "                steer = steering(ego_x_position, intended_x, s_power)\n",
    "\n",
    "                # print(f\"\\rgoing towards: ({intended_x:.1f}, {intended_y:.1f}), with b:{brake}, t:{throttle:.1f}, s:{steer:.1f}      \", end = \"\")\n",
    "\n",
    "                ego.apply_control(control_ego(steer, throttle, brake))\n",
    "                set_cam_to_EGO(ego)\n",
    "                \n",
    "                red_light_violations = False\n",
    "                collision_sensor = CollisionSensor(ego)\n",
    "                lane_violations = False\n",
    "                tailgating_violations = False\n",
    "                illegal_turn_violations = False\n",
    "                stop_sign_violations = False\n",
    "\n",
    "            if time.time() >= start_time + 60:\n",
    "                break\n",
    "            prev_image_captured = image_captured\n",
    "            \n",
    "        if(distance > 0):\n",
    "            RC = dist_covered * 100 / distance\n",
    "            IS = isc\n",
    "            DS = evaluate_DS(RC, IS)\n",
    "        m1.append([RC, IS, DS])\n",
    "        # print(f\"\\nRC is: {RC:.2f}, IS is: {IS:.2f}, DS is: {DS:.2f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n{e}\")\n",
    "    finally:\n",
    "        camera.stop()\n",
    "\n",
    "        ego.apply_control(control_ego(0,0,1))\n",
    "\n",
    "        remove_all_actors()\n",
    "        initiating_carla(world, blueprint_library)\n",
    "        set_cam_to_EGO(ego, 65)\n",
    "\n",
    "        # print(\"stopped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 = []\n",
    "for my_i in range (1,iterations+1):\n",
    "    driver_model = torch.load(f\"models/A2/update count - {my_i}.pth\", map_location=torch.device('cpu'), weights_only=False)\n",
    "    try:\n",
    "        # print(\"started\")\n",
    "\n",
    "        prev_image_captured = None\n",
    "        image_captured = None\n",
    "        init_pos = ego.get_transform().location\n",
    "\n",
    "        RC = 0\n",
    "        IS = 0\n",
    "        DS = 0\n",
    "        dist_covered = 0\n",
    "        distance = 0\n",
    "        isc = 0\n",
    "        speed_limit = 50\n",
    "\n",
    "        count_dist = 0\n",
    "        dist_counter = 0\n",
    "\n",
    "        red_light_violations = False\n",
    "        collision_sensor = CollisionSensor(ego)\n",
    "        lane_violations = False\n",
    "        tailgating_violations = False\n",
    "        illegal_turn_violations = False\n",
    "        stop_sign_violations = False\n",
    "        set_cam_to_EGO(ego)\n",
    "        camera.listen(camera_to_image) #changes image_captured globally\n",
    "\n",
    "        while image_captured is None:\n",
    "            world.tick()\n",
    "            start_time = time.time()\n",
    "\n",
    "        while True:\n",
    "            world.wait_for_tick()\n",
    "            if not prev_image_captured == image_captured:\n",
    "                dist_counter += 1\n",
    "\n",
    "                scene_x, scene_y = 800, 600\n",
    "                ego_x_mid = scene_x/2\n",
    "                ego_y_mid = scene_y/2\n",
    "\n",
    "                outputs = model_output_generation(image_captured, driver_model)\n",
    "                waypoint = abs(outputs.detach().numpy())\n",
    "\n",
    "                ego_x_position, ego_y_position = points()\n",
    "                \n",
    "                intended_x = waypoint[0]\n",
    "                intended_y = waypoint[1]\n",
    "\n",
    "                if dist_counter % 2 == 0:\n",
    "                    init_pos = ego.get_transform().location\n",
    "                    d = 0\n",
    "                else:\n",
    "                    curr_pos = ego.get_transform().location\n",
    "                    dx = abs(curr_pos.x - init_pos.x)\n",
    "                    dy = abs(curr_pos.y - init_pos.y)\n",
    "                    dz = abs(curr_pos.z - init_pos.z)\n",
    "                    d = math.sqrt((dx ** 2) + (dy ** 2) + (dz ** 2))\n",
    "                    distance += d\n",
    "                    evaluate_RC(intended_x, ego_x_position, intended_y, ego_y_position, th, d)\n",
    "                    evaluate_IS(speed_limit, red_light_violations, collision_sensor, lane_violations, tailgating_violations, illegal_turn_violations, stop_sign_violations)\n",
    "\n",
    "                    count_dist += math.sqrt(((intended_x - ego_x_position) ** 2) + ((intended_y - ego_y_position) ** 2))\n",
    "\n",
    "                    # if distance > 0:\n",
    "                    #     print(f\"\\r{dist_covered:.2f} / {distance:.2f}, {isc:.2f}        \", end = \"\")\n",
    "                        # break\n",
    "\n",
    "                throttle, brake = TandB(ego_y_mid, intended_y, ego_y_position)\n",
    "                velocity = ego.get_velocity()\n",
    "                speed = math.sqrt((velocity.x**2 + velocity.y**2 + velocity.z**2)) / 5\n",
    "                s_power = (1 - brake) * (1 - speed)\n",
    "                steer = steering(ego_x_position, intended_x, s_power)\n",
    "\n",
    "                # print(f\"\\rgoing towards: ({intended_x:.1f}, {intended_y:.1f}), with b:{brake}, t:{throttle:.1f}, s:{steer:.1f}      \", end = \"\")\n",
    "\n",
    "                ego.apply_control(control_ego(steer, throttle, brake))\n",
    "                set_cam_to_EGO(ego)\n",
    "                \n",
    "                red_light_violations = False\n",
    "                collision_sensor = CollisionSensor(ego)\n",
    "                lane_violations = False\n",
    "                tailgating_violations = False\n",
    "                illegal_turn_violations = False\n",
    "                stop_sign_violations = False\n",
    "\n",
    "            if time.time() >= start_time + 60:\n",
    "                break\n",
    "            prev_image_captured = image_captured\n",
    "            \n",
    "        if(distance > 0):\n",
    "            RC = dist_covered * 100 / distance\n",
    "            IS = isc\n",
    "            DS = evaluate_DS(RC, IS)    \n",
    "        m2.append([RC, IS, DS])\n",
    "        # print(f\"\\nRC is: {RC:.2f}, IS is: {IS:.2f}, DS is: {DS:.2f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n{e}\")\n",
    "    finally:\n",
    "        camera.stop()\n",
    "\n",
    "        ego.apply_control(control_ego(0,0,1))\n",
    "\n",
    "        remove_all_actors()\n",
    "        initiating_carla(world, blueprint_library)\n",
    "        set_cam_to_EGO(ego, 65)\n",
    "\n",
    "        # print(\"stopped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m3 = []\n",
    "for my_i in range (1,iterations+1):\n",
    "    try:\n",
    "        driver_model = torch.load(f\"models/A3/update count - {my_i}.pth\", map_location=torch.device('cpu'), weights_only=False)\n",
    "        # print(\"started\")\n",
    "\n",
    "        prev_image_captured = None\n",
    "        image_captured = None\n",
    "        init_pos = ego.get_transform().location\n",
    "\n",
    "        RC = 0\n",
    "        IS = 0\n",
    "        DS = 0\n",
    "        dist_covered = 0\n",
    "        distance = 0\n",
    "        isc = 0\n",
    "        speed_limit = 50\n",
    "\n",
    "        count_dist = 0\n",
    "        dist_counter = 0\n",
    "\n",
    "        red_light_violations = False\n",
    "        collision_sensor = CollisionSensor(ego)\n",
    "        lane_violations = False\n",
    "        tailgating_violations = False\n",
    "        illegal_turn_violations = False\n",
    "        stop_sign_violations = False\n",
    "        set_cam_to_EGO(ego)\n",
    "        camera.listen(camera_to_image) #changes image_captured globally\n",
    "\n",
    "        while image_captured is None:\n",
    "            world.tick()\n",
    "            start_time = time.time()\n",
    "\n",
    "        while True:\n",
    "            world.wait_for_tick()\n",
    "            if not prev_image_captured == image_captured:\n",
    "                dist_counter += 1\n",
    "\n",
    "                scene_x, scene_y = 800, 600\n",
    "                ego_x_mid = scene_x/2\n",
    "                ego_y_mid = scene_y/2\n",
    "\n",
    "                outputs = model_output_generation(image_captured, driver_model)\n",
    "                waypoint = abs(outputs.detach().numpy())\n",
    "\n",
    "                ego_x_position, ego_y_position = points()\n",
    "                \n",
    "                intended_x = waypoint[0]\n",
    "                intended_y = waypoint[1]\n",
    "\n",
    "                if dist_counter % 2 == 0:\n",
    "                    init_pos = ego.get_transform().location\n",
    "                    d = 0\n",
    "                else:\n",
    "                    curr_pos = ego.get_transform().location\n",
    "                    dx = abs(curr_pos.x - init_pos.x)\n",
    "                    dy = abs(curr_pos.y - init_pos.y)\n",
    "                    dz = abs(curr_pos.z - init_pos.z)\n",
    "                    d = math.sqrt((dx ** 2) + (dy ** 2) + (dz ** 2))\n",
    "                    distance += d\n",
    "                    evaluate_RC(intended_x, ego_x_position, intended_y, ego_y_position, th, d)\n",
    "                    evaluate_IS(speed_limit, red_light_violations, collision_sensor, lane_violations, tailgating_violations, illegal_turn_violations, stop_sign_violations)\n",
    "\n",
    "                    count_dist += math.sqrt(((intended_x - ego_x_position) ** 2) + ((intended_y - ego_y_position) ** 2))\n",
    "\n",
    "                    # if distance > 0:\n",
    "                    #     print(f\"\\r{dist_covered:.2f} / {distance:.2f}, {isc:.2f}        \", end = \"\")\n",
    "                        # break\n",
    "\n",
    "                throttle, brake = TandB(ego_y_mid, intended_y, ego_y_position)\n",
    "                velocity = ego.get_velocity()\n",
    "                speed = math.sqrt((velocity.x**2 + velocity.y**2 + velocity.z**2)) / 5\n",
    "                s_power = (1 - brake) * (1 - speed)\n",
    "                steer = steering(ego_x_position, intended_x, s_power)\n",
    "\n",
    "                # print(f\"\\rgoing towards: ({intended_x:.1f}, {intended_y:.1f}), with b:{brake}, t:{throttle:.1f}, s:{steer:.1f}      \", end = \"\")\n",
    "\n",
    "                ego.apply_control(control_ego(steer, throttle, brake))\n",
    "                set_cam_to_EGO(ego)\n",
    "                \n",
    "                red_light_violations = False\n",
    "                collision_sensor = CollisionSensor(ego)\n",
    "                lane_violations = False\n",
    "                tailgating_violations = False\n",
    "                illegal_turn_violations = False\n",
    "                stop_sign_violations = False\n",
    "\n",
    "            if time.time() >= start_time + 60:\n",
    "                break\n",
    "            prev_image_captured = image_captured\n",
    "            \n",
    "        if(distance > 0):\n",
    "            RC = dist_covered * 100 / distance\n",
    "            IS = isc\n",
    "            DS = evaluate_DS(RC, IS)\n",
    "        m3.append([RC, IS, DS])\n",
    "        # print(f\"\\nRC is: {RC:.2f}, IS is: {IS:.2f}, DS is: {DS:.2f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n{e}\")\n",
    "    finally:\n",
    "        camera.stop()\n",
    "\n",
    "        ego.apply_control(control_ego(0,0,1))\n",
    "\n",
    "        remove_all_actors()\n",
    "        initiating_carla(world, blueprint_library)\n",
    "        set_cam_to_EGO(ego, 65)\n",
    "\n",
    "        # print(\"stopped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M1,M2,M3 = m1,m2,m3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1,m2,m3 = M1,M2,M3\n",
    "a = 0\n",
    "for i,j,k in zip(m1,m2,m3):\n",
    "    a = a + 1\n",
    "    print(f\"epoch count {a}: [{i[0]:.2f}, {i[1]:.2f}, {i[2]:.2f}]\", end = \", \")\n",
    "    print(f\"[{j[0]:.2f}, {j[1]:.2f}, {j[2]:.2f}]\", end = \", \")\n",
    "    print(f\"[{k[0]:.2f}, {k[1]:.2f}, {k[2]:.2f}]\", end = \", \")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 40 * (np.arange(len(m1)) + 1)\n",
    "y_values = np.array(m1).T\n",
    "\n",
    "markers = ['o', 's', 'D']\n",
    "labels = ['RC', 'IS', 'DS']\n",
    "jitter_strength = 0.5\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "for i, (y, marker, label) in enumerate(zip(y_values, markers, labels)):\n",
    "    y_jittered = [val + random.uniform(-jitter_strength, jitter_strength) for val in y]\n",
    "    plt.plot(x, y_jittered, marker=marker, linestyle='-', label=label)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Metric values\")\n",
    "plt.ylim(-8, 109)\n",
    "plt.title(f\"{A1}\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 40 * (np.arange(len(m2)) + 1)\n",
    "y_values = np.array(m2).T\n",
    "\n",
    "markers = ['o', 's', 'D']\n",
    "labels = ['RC', 'IS', 'DS']\n",
    "jitter_strength = 0.5\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "for i, (y, marker, label) in enumerate(zip(y_values, markers, labels)):\n",
    "    y_jittered = [val + random.uniform(-jitter_strength, jitter_strength) for val in y]\n",
    "    plt.plot(x, y_jittered, marker=marker, linestyle='-', label=label)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Metric values\")\n",
    "plt.ylim(-8, 109)\n",
    "plt.title(f\"{A2}\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 40 * (np.arange(len(m3)) + 1)\n",
    "y_values = np.array(m3).T\n",
    "\n",
    "markers = ['o', 's', 'D']\n",
    "labels = ['RC', 'IS', 'DS']\n",
    "jitter_strength = 0.5\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "for i, (y, marker, label) in enumerate(zip(y_values, markers, labels)):\n",
    "    y_jittered = [val + random.uniform(-jitter_strength, jitter_strength) for val in y]\n",
    "    plt.plot(x, y_jittered, marker=marker, linestyle='-', label=label)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Metric values\")\n",
    "plt.ylim(-8, 109)\n",
    "plt.title(f\"{A3}\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
