{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import cv2\n",
    "import math\n",
    "import time\n",
    "import json\n",
    "import carla\n",
    "import heapq\n",
    "import torch\n",
    "# import pickle\n",
    "import random\n",
    "import requests\n",
    "import kagglehub\n",
    "import transformers\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision import models\n",
    "from collections import Counter\n",
    "from sklearn.decomposition import PCA\n",
    "from skimage.morphology import skeletonize\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from huggingface_hub import hf_hub_download\n",
    "from transformers.image_transforms import rgb_to_id\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from transformers import SegformerForSemanticSegmentation, SegformerConfig, CLIPProcessor, CLIPModel, AutoModelForSeq2SeqLM, AutoTokenizer, AutoImageProcessor, DetrForObjectDetection, DetrForSegmentation, TimeSeriesTransformerForPrediction, TimeSeriesTransformerConfig, pipeline\n",
    "\n",
    "from torchvision import transforms as A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ALGORITHM 1: VL pretrained-M fine tuned with semi-disentangled outputs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Creation (IMAGE to EMBEDDING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = kagglehub.dataset_download(\"albertozorzetto/carla-densely-annotated-driving-dataset\")\n",
    "# print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Download the CARLA dataset using the Kaggle API\n",
    "# dataset_name = \"albertozorzetto/carla-densely-annotated-driving-dataset\"\n",
    "# destination_path = \"/content/datasets\"\n",
    "\n",
    "# import os\n",
    "# # Install Kaggle API\n",
    "# !pip install kaggle --upgrade\n",
    "# # Upload kaggle.json for authentication\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "# # Move kaggle.json to the proper directory\n",
    "# !mkdir -p ~/.kaggle\n",
    "# !mv kaggle.json ~/.kaggle/\n",
    "# !chmod 600 ~/.kaggle/kaggle.json\n",
    "# # Verify Kaggle is set up correctly\n",
    "# !kaggle datasets list\n",
    "# !kaggle datasets download -d {dataset_name} -p {destination_path} --unzip\n",
    "# print(\"Path to dataset files:\", destination_path)\n",
    "\n",
    "# import tarfile\n",
    "# images_path = os.path.join(destination_path, \"images\")\n",
    "# labels_path = os.path.join(destination_path, \"labels\")\n",
    "# # Create directories for extracted files\n",
    "# os.makedirs(images_path, exist_ok=True)\n",
    "# os.makedirs(labels_path, exist_ok=True)\n",
    "# # Extract and manage .tar files\n",
    "# tar_files = [f for f in os.listdir(destination_path) if f.endswith('.tar')]\n",
    "# for tar_file in tar_files:\n",
    "#     tar_path = os.path.join(destination_path, tar_file)\n",
    "#     print(f\"Extracting {tar_path}...\")\n",
    "#     with tarfile.open(tar_path) as tar:\n",
    "#         # Determine the folder to extract based on the tar file name\n",
    "#         if \"images\" in tar_file.lower():\n",
    "#             tar.extractall(path=images_path)\n",
    "#         elif \"labels\" in tar_file.lower():\n",
    "#             tar.extractall(path=labels_path)\n",
    "#         else:\n",
    "#             print(f\"Skipping unknown .tar file: {tar_file}\")\n",
    "#     os.remove(tar_path)  # Optional: Remove the .tar file after extraction\n",
    "# print(\"All .tar files have been extracted.\")\n",
    "# print(f\"Images folder: {images_path}\")\n",
    "# print(f\"Labels folder: {labels_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "semantic_label_classes = [\"Traffic Sign\", \"Building\", \"Fence\", \"Other\", \"Pedestrian\", \"Pole\", \"Road Line\", \"Road\", \"Sidewalk\", \"Vegetation\", \"Car\", \"Wall\", \"Unlabeled\"]\n",
    "\n",
    "root = \"datasets\"\n",
    "images_dir = f\"{root}/images\"\n",
    "labels_dir = f\"{root}/labels\"\n",
    "waypoint_file = \"dataset_waypoints.json\"\n",
    "bev_file = \"dataset_bev.json\"\n",
    "batch_size = 2\n",
    "\n",
    "skip = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Saving the {way points} for passing to the LLM*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mid_waypoint(waypoints):\n",
    "    if len(waypoints) == 0:\n",
    "        return None\n",
    "    mid_index = len(waypoints) // 2\n",
    "    \n",
    "    mean_x = sum(wp[0] for wp in waypoints) / len(waypoints)\n",
    "    mean_y = sum(wp[1] for wp in waypoints) / len(waypoints)\n",
    "    mean_y = 600 - mean_y\n",
    "\n",
    "    mid_waypoint = [mean_x, mean_y]\n",
    "\n",
    "    return mid_waypoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_waypoints(label, num_waypoints):\n",
    "    if len(label.shape) == 3:\n",
    "        label = cv2.cvtColor(label, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    road_mask = (label == 90).astype(np.uint8)\n",
    "    \n",
    "    contours, _ = cv2.findContours(road_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if not contours:\n",
    "        return []\n",
    "    \n",
    "    largest_contour = max(contours, key=cv2.contourArea)\n",
    "    binary_image = (road_mask > 0).astype(np.uint8)\n",
    "    skeleton = skeletonize(binary_image).astype(np.uint8)\n",
    "    \n",
    "    y_coords, x_coords = np.where(skeleton > 0)\n",
    "    path_points = np.column_stack((x_coords, y_coords))\n",
    "    \n",
    "    # x_min, x_max = path_points[:, 0].min(), path_points[:, 0].max()\n",
    "    # y_min, y_max = path_points[:, 1].min(), path_points[:, 1].max()\n",
    "\n",
    "    # x_range = (x_max - x_min) * 0.25\n",
    "    # y_range = (y_max - y_min) * 0.25\n",
    "\n",
    "    # path_points = path_points[\n",
    "    #     (path_points[:, 0] > x_min + x_range) & (path_points[:, 0] < x_max - x_range) &\n",
    "    #     (path_points[:, 1] > y_min + y_range) & (path_points[:, 1] < y_max - y_range)\n",
    "    # ]\n",
    "\n",
    "    if len(path_points) == 0:\n",
    "        return []\n",
    "\n",
    "    sampled_indices = np.linspace(0, len(path_points) - 1, num_waypoints, dtype=int)\n",
    "    waypoints = path_points[sampled_indices]\n",
    "\n",
    "    num_selected_waypoints = min(25, len(path_points))\n",
    "    sampled_indices = np.linspace(0, len(path_points) - 1, num_selected_waypoints, dtype=int)\n",
    "    waypoints = path_points[sampled_indices]\n",
    "    \n",
    "    # print(\"Min/Max path points:\", path_points.min(), path_points.max())\n",
    "\n",
    "    # plt.imshow(skeleton, cmap=\"gray\")\n",
    "    # plt.scatter(x_coords, y_coords, c=\"red\", marker=\"o\")\n",
    "    # plt.show()\n",
    "\n",
    "    mid_waypoint = get_mid_waypoint(waypoints)\n",
    "    # print(\"Mid Waypoint:\", mid_waypoint)\n",
    "\n",
    "    waypoints = mid_waypoint\n",
    "\n",
    "    # print(mid_waypoint[0], 600 - mid_waypoint[1])\n",
    "\n",
    "    # if len(waypoints) > 0:\n",
    "    #     plt.figure(figsize=(10, 10))\n",
    "    #     plt.imshow(label, cmap=\"gray\")\n",
    "    #     x = waypoints[0]\n",
    "    #     y = 600 - waypoints[1]\n",
    "    #     plt.scatter(x, y, c='red', marker='o')\n",
    "    #     plt.text(x, y, f'({x:.1f}, {y:.1f})', fontsize=8, color='yellow', ha='right', va='bottom')\n",
    "    #     plt.title(\"Extracted Waypoints\")\n",
    "    #     plt.show()\n",
    "\n",
    "    return waypoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(labels_dir, output_dir, num_waypoints):\n",
    "    dataset_waypoints = []\n",
    "\n",
    "    for i in range(0, 28):\n",
    "        k = \"0\" if i <= 9 else \"\"\n",
    "        curr_folder_labels = f\"{labels_dir}/Video_0{k}{i}\"\n",
    "        labels_path = [file for file in os.listdir(curr_folder_labels) if file.endswith(\".png\")]\n",
    "        labels_path.sort()\n",
    "        labels_path = labels_path[::skip]\n",
    "\n",
    "        for label_file in labels_path:\n",
    "            label_path = os.path.join(curr_folder_labels, label_file)\n",
    "            label = Image.open(label_path)\n",
    "            label_np = np.array(label)\n",
    "            waypoints = generate_waypoints(label_np, num_waypoints=num_waypoints)\n",
    "            output_label_path = os.path.join(output_dir, label_file)\n",
    "            dataset_waypoints.append({\n",
    "                \"label\": output_label_path,\n",
    "                \"waypoints\": waypoints\n",
    "            })\n",
    "            # break\n",
    "\n",
    "        with open(os.path.join(output_dir, waypoint_file), 'a') as wf:\n",
    "            json.dump(dataset_waypoints, wf)\n",
    "            wf.write(\"\\n\")\n",
    "            # break\n",
    "\n",
    "        if(i<27):\n",
    "            print(f\"\\rProgress: {((i + 1) / 0.27):.2f} %\", end = \"\")\n",
    "        # break\n",
    "\n",
    "    print(f\"\\nDone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_dataset(labels_dir, root, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "getting dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, image_dir, labels_dir):\n",
    "        self.data = []\n",
    "        ei = []\n",
    "        cw = []\n",
    "\n",
    "        for i in range(0, 28):\n",
    "            k = \"0\" if i <= 9 else \"\"\n",
    "            curr_folder_images = f\"{images_dir}/Video_0{k}{i}\"\n",
    "            images_path = [file for file in os.listdir(curr_folder_images) if file.endswith(\".png\")]\n",
    "            images_path.sort()\n",
    "            images_path = images_path[::skip * 10]\n",
    "\n",
    "            for image_file in images_path:\n",
    "                image_path = os.path.join(curr_folder_images, image_file)\n",
    "                image = Image.open(image_path)\n",
    "                image_embeddings = processor(text=semantic_label_classes, images=image, return_tensors=\"pt\", padding=True)\n",
    "                ei.append(image_embeddings['pixel_values'])\n",
    "\n",
    "            with open(os.path.join(root, waypoint_file), \"r\") as wf:\n",
    "                content = wf.read()\n",
    "                data_list = content.splitlines()\n",
    "                for data in data_list:\n",
    "                    data = json.loads(data)\n",
    "                    for i in data:\n",
    "                        corresponding_waypoints = i['waypoints']\n",
    "                        cw.append(corresponding_waypoints)\n",
    "                        break\n",
    "                    # break\n",
    "            for e,c in zip(ei, cw):\n",
    "                self.data.append((e,c))\n",
    "                # break\n",
    "            # break\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        embedding_inputs, embedding_labels = self.data[idx]\n",
    "        return embedding_inputs, embedding_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split indices for train and validation\n",
    "dataset = SegmentationDataset(images_dir, labels_dir)\n",
    "dataloader = DataLoader(dataset, batch_size = batch_size, shuffle = True, pin_memory=True)\n",
    "\n",
    "train_indices, val_indices = train_test_split(range(len(dataset)), test_size=0.2, random_state=42)\n",
    "train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "\n",
    "val_dataset = torch.utils.data.Subset(dataset, val_indices)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(pixel_values):\n",
    "    mini = pixel_values.min()\n",
    "    maxi = pixel_values.max()\n",
    "    pixel_values = (pixel_values - mini) / (maxi - mini)\n",
    "    return pixel_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OUR VIT WP MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VitWPModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VitWPModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(512)\n",
    "        self.conv3 = nn.Conv2d(512, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "        self.conv5 = nn.Conv2d(512, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 512)\n",
    "        self.fc2 = nn.Linear(512, 2)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        x = torch.relu(self.conv1(pixel_values))\n",
    "        x = self.bn1(x)\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        \n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.bn2(x)\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = self.bn3(x)\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "\n",
    "        x = torch.relu(self.conv4(x))\n",
    "        x = self.bn4(x)\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "\n",
    "        x = torch.relu(self.conv5(x))\n",
    "        x = self.bn5(x)\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        output = self.fc2(x)\n",
    "        return output.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_train = VitWPModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*--training set ups--*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wp_generation(pixel_values, model):\n",
    "    pixel_values = pixel_values.squeeze(1)\n",
    "    outputs = model(pixel_values)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model_to_train, train_loader, optimizer, criterion):\n",
    "    model_to_train.train()\n",
    "    running_loss = 0.0\n",
    "    running_accuracy = 0.0\n",
    "\n",
    "    for data in train_loader:\n",
    "        p_val = data[0]\n",
    "        way_p = data[1]\n",
    "        target = way_p[0].float()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = wp_generation(p_val[0], model_to_train)\n",
    "        loss = criterion(outputs, target)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # break\n",
    "\n",
    "    return running_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model_to_validate, val_loader, optimizer, criterion):\n",
    "    model_to_validate.eval()\n",
    "    running_loss = 0.0\n",
    "    running_accuracy = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            p_val = data[0]\n",
    "            way_p = data[1]\n",
    "            target = way_p[0].float()\n",
    "\n",
    "            outputs = wp_generation(p_val[0], model_to_validate)\n",
    "            loss = criterion(outputs, target)\n",
    "            running_loss += loss.item()\n",
    "            # break\n",
    "\n",
    "    return running_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model_to_test, dataloader, optimizer, criterion):\n",
    "    model_to_test.eval()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            p_val = data[0]\n",
    "            way_p = data[1]\n",
    "            target = way_p[0].float()\n",
    "\n",
    "            outputs = wp_generation(p_val[0], model_to_test)\n",
    "            print(pred)\n",
    "            print(waypoints)\n",
    "            loss = criterion(outputs, target)\n",
    "            total_loss += loss.item()\n",
    "            break\n",
    "            break\n",
    "\n",
    "    return running_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_save = best_model_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model__to_save, 'models/VitWPModel.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_model = model_to_save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ALGORITHM 2: Future Trajectory Prediction By LSTM using Multi Modal Inputs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Createion (IMAGE to TEMPORAL DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_processor = AutoImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "detr_model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "\n",
    "root = \"datasets\"\n",
    "images_dir = f\"{root}/images\"\n",
    "labels_dir = f\"{root}/labels\"\n",
    "bev_file = \"dataset_bev.json\"\n",
    "batch_size = 8\n",
    "\n",
    "skip = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Saving BEV details*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_bev_dataset(images_dir, output_dir):\n",
    "    bevs = []\n",
    "\n",
    "    for i in range(0, 28):\n",
    "        k = \"0\" if i <= 9 else \"\"\n",
    "        curr_folder_images = f\"{images_dir}/Video_0{k}{i}\"\n",
    "        images_path = [file for file in os.listdir(curr_folder_images) if file.endswith(\".png\")]\n",
    "        images_path.sort()\n",
    "        images_path = images_path[::skip]\n",
    "\n",
    "        for image_file in images_path:\n",
    "            image_path = os.path.join(curr_folder_images, image_file)\n",
    "            image = Image.open(image_path)\n",
    "            image_np = np.array(image)\n",
    "\n",
    "            depth_map = image_to_depth_map(image)\n",
    "            objects = object_detection(image)\n",
    "            trajectories = future_positions(objects['boxes'])\n",
    "            bev_map = depth_map_to_bev(depth_map, objects['boxes'], trajectories)\n",
    "            wp_x, wp_y = get_waypoint(bev_map)\n",
    "\n",
    "            output_image_path = os.path.join(curr_folder_images, image_file)\n",
    "            bevs.append({\n",
    "                \"image\": output_image_path,\n",
    "                \"bev_map\": bev_map.tolist(),\n",
    "                \"way_point\": [wp_x, wp_y]\n",
    "            })\n",
    "            # break\n",
    "\n",
    "            with open(os.path.join(output_dir, bev_file), 'a') as wf:\n",
    "                json.dump(bevs, wf)\n",
    "                wf.write(\"\\n\")\n",
    "                # break\n",
    "\n",
    "        if(i<27):\n",
    "            print(f\"\\rProgress: {((i + 1) / 0.27):.2f} %\", end = \"\")\n",
    "        # break\n",
    "\n",
    "    print(f\"\\nDone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_bev_dataset(images_dir, root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "getting dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BEVDataset(Dataset):\n",
    "    def __init__(self, root, bev_file, sequence_length = 5):\n",
    "        self.data = []\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "        with open(os.path.join(root, bev_file), \"r\") as wf:\n",
    "            images = []\n",
    "            bev_maps = []\n",
    "            waypoints = []\n",
    "            for data in wf:\n",
    "                data = json.loads(data)\n",
    "                data = data[0]\n",
    "                # print(data.keys())\n",
    "\n",
    "                image = data['image']\n",
    "                bev_map = torch.tensor(data['bev_map'])\n",
    "                way_point = torch.tensor(data['way_point'])\n",
    "\n",
    "                images.append(image)\n",
    "                bev_maps.append(bev_map)\n",
    "                waypoints.append(way_point)\n",
    "\n",
    "                if len(images) >= self.sequence_length:\n",
    "                    sequence_images = images[-self.sequence_length:]\n",
    "                    sequence_bev_maps = bev_maps[-self.sequence_length:]\n",
    "                    sequence_waypoints = waypoints[-self.sequence_length:]\n",
    "                    self.data.append([sequence_images, sequence_bev_maps, sequence_waypoints])\n",
    "                # break\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split indices for train and validation\n",
    "dataset = BEVDataset(root, bev_file, sequence_length = 5)\n",
    "dataloader = DataLoader(dataset, batch_size = batch_size, shuffle = True, pin_memory=True, drop_last=True)\n",
    "\n",
    "train_indices, val_indices = train_test_split(range(len(dataset)), train_size = 0.5, test_size=0.5, random_state=42)\n",
    "train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, drop_last=True)\n",
    "\n",
    "val_dataset = torch.utils.data.Subset(dataset, val_indices)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BEV to image model (for faster operation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BEVToImage(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BEVToImage, self).__init__()\n",
    "        self.backbone = models.resnet18(pretrained=True)\n",
    "        self.backbone = nn.Sequential(*list(self.backbone.children())[:-2])\n",
    "        self.conv1x1 = nn.Conv2d(512, 256, kernel_size=1)\n",
    "        \n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 512, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(512, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 1, kernel_size=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.conv1x1(x)\n",
    "        x = self.upsample(x)\n",
    "        x = F.interpolate(x, size=(200, 200), mode='bilinear', align_corners=False)\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_dir):\n",
    "        self.data = []\n",
    "\n",
    "        with open(os.path.join(root, bev_file), \"r\") as wf:\n",
    "            for data in wf:\n",
    "                data = json.loads(data)\n",
    "                data = data[0]\n",
    "                image = np.array(Image.open(data['image']))\n",
    "                bev_map = torch.tensor(data['bev_map'])\n",
    "\n",
    "                self.data.append([image, bev_map])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bev_to_image_model = BEVToImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageset = ImageDataset(images_dir)\n",
    "image_loader = DataLoader(imageset, batch_size = batch_size, shuffle = True, pin_memory=True, drop_last=True)\n",
    "\n",
    "dataloader = image_loader\n",
    "train_loader = image_loader\n",
    "val_loader = image_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wp_generation(inputs, model):\n",
    "    return model(torch.tensor(np.array(image, dtype = np.float32) / 255.0, dtype=torch.float32).unsqueeze(0).permute(0,3,1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, image_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in image_loader:\n",
    "        img = data[0]\n",
    "        bev = data[1]\n",
    "        optimizer.zero_grad()\n",
    "        pred = wp_generation(img[0], model)\n",
    "        loss = criterion(pred, bev[0].float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(image_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, image_loader, optimizer, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in image_loader:\n",
    "            img = data[0]\n",
    "            bev = data[1]\n",
    "            pred = wp_generation(img[0], model)\n",
    "            loss = criterion(pred, bev[0].float())\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(image_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, image_loader, optimizer, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in image_loader:\n",
    "            img = data[0]\n",
    "            bev = data[1]\n",
    "            pred = wp_generation(img[0], model)\n",
    "            print(pred)\n",
    "            print(waypoints)\n",
    "            loss = criterion(pred, bev[0].float())\n",
    "            total_loss += loss.item()\n",
    "            break\n",
    "    return total_loss / len(image_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_train = bev_to_image_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_save = best_model_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_to_save, 'models/BEVToImage.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_to_bev_model = model_to_save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "image > depth map and object detection > bird's eye view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_depth_map(image):\n",
    "    estimator = pipeline(task=\"depth-estimation\", model=\"Intel/dpt-large\")\n",
    "    result = estimator(images=image)\n",
    "    depth_map = result['predicted_depth']\n",
    "    return depth_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_depth_map(depth_map, cord_x = 400, cord_y = 75, c = 'white'):\n",
    "    depth_map = depth_map.max() - depth_map\n",
    "    height, width = depth_map.shape\n",
    "    depth_at_cord = depth_map[cord_y-1, cord_x-1]\n",
    "    print(f\"Depth at co-ordinate ({cord_x}, {cord_y}) of the image: {depth_at_cord:.2f} meters\")\n",
    "    plt.scatter(cord_x, cord_y, color=c, marker='x')\n",
    "    plt.imshow(depth_map, cmap = 'inferno')\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def object_detection(image):\n",
    "    inputs = image_processor(images=image, return_tensors=\"pt\")\n",
    "    outputs = detr_model(**inputs)\n",
    "    target_sizes = torch.tensor([image.size[::-1]])\n",
    "    objects = image_processor.post_process_object_detection(outputs, threshold=0.9, target_sizes=target_sizes)[0]\n",
    "\n",
    "    return objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def future_positions(objects, alpha = 0.1):\n",
    "    trajectories = []\n",
    "    \n",
    "    for obj in objects:\n",
    "        smoothed_positions = [obj]\n",
    "        for t in range(0, len(obj)):\n",
    "            smoothed_pos = alpha * obj[t] + (1 - alpha) * smoothed_positions[-1]\n",
    "            smoothed_positions.append(smoothed_pos)\n",
    "        future_positions = [smoothed_positions[-1]]\n",
    "        trajectories.append(future_positions)\n",
    "\n",
    "    new = []\n",
    "    for i in trajectories:\n",
    "        new.append(i[0].detach().numpy())\n",
    "\n",
    "    trajectories = torch.tensor(new, dtype = torch.float32)\n",
    "        \n",
    "    return trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def depth_map_to_bev(depth_map, objects, trajectories):\n",
    "    f_x = 1000\n",
    "    f_y = 1000\n",
    "    c_x = 400\n",
    "    c_y = 300\n",
    "    bev_size = 200\n",
    "    scale_factor = 15\n",
    "\n",
    "    height, width = depth_map.shape\n",
    "    bev_map = np.zeros((bev_size, bev_size), dtype=np.float32)\n",
    "\n",
    "    x_coords, y_coords = np.meshgrid(np.arange(width), np.arange(height))\n",
    "\n",
    "    depth = depth_map[y_coords, x_coords].detach().numpy()\n",
    "    X = (x_coords - c_x) * depth / f_x\n",
    "    Y = (y_coords - c_y) * depth / f_y\n",
    "\n",
    "    bev_x = np.clip((X * scale_factor).astype(int), 0, bev_size - 1)\n",
    "    bev_y = np.clip((Y * scale_factor).astype(int), 0, bev_size - 1)\n",
    "\n",
    "    for i in range(height * width):\n",
    "        bev_map[bev_y.flat[i], bev_x.flat[i]] = max(bev_map[bev_y.flat[i], bev_x.flat[i]], depth.flat[i])\n",
    "\n",
    "    for obj, traj in zip(objects, trajectories):\n",
    "        x_min, y_min, x_max, y_max = obj\n",
    "        x_min, y_min, x_max, y_max = x_min.item(), y_min.item(), x_max.item(), y_max.item()\n",
    "\n",
    "        traj_x_min, traj_y_min, traj_x_max, traj_y_max = traj\n",
    "        traj_x_min, traj_y_min, traj_x_max, traj_y_max = traj_x_min.item(), traj_y_min.item(), traj_x_max.item(), traj_y_max.item()\n",
    "\n",
    "        obj_depths = depth_map[int(y_min):int(y_max), int(x_min):int(x_max)].detach().numpy()\n",
    "        if obj_depths.size > 0:\n",
    "            mean_depth = np.mean(obj_depths)\n",
    "        else:\n",
    "            mean_depth = 0\n",
    "        \n",
    "        X_min = (x_min - c_x) * mean_depth / f_x\n",
    "        X_max = (x_max - c_x) * mean_depth / f_x\n",
    "        Y_min = (y_min - c_y) * mean_depth / f_y\n",
    "        Y_max = (y_max - c_y) * mean_depth / f_y\n",
    "        Z = mean_depth\n",
    "        bev_x_min = int(100 + X_min * scale_factor)\n",
    "        bev_x_max = int(100 + X_max * scale_factor)\n",
    "        bev_y_min = int(100 + Y_min * scale_factor)\n",
    "        bev_y_max = int(100 + Y_max * scale_factor)\n",
    "\n",
    "        traj_X_min = (traj_x_min - c_x) * mean_depth / f_x\n",
    "        traj_X_max = (traj_x_max - c_x) * mean_depth / f_x\n",
    "        traj_Y_min = (traj_y_min - c_y) * mean_depth / f_y\n",
    "        traj_Y_max = (traj_y_max - c_y) * mean_depth / f_y\n",
    "        traj_Z = mean_depth\n",
    "        traj_bev_x_min = int(100 + traj_X_min * scale_factor)\n",
    "        traj_bev_x_max = int(100 + traj_X_max * scale_factor)\n",
    "        traj_bev_y_min = int(100 + traj_Y_min * scale_factor)\n",
    "        traj_bev_y_max = int(100 + traj_Y_max * scale_factor)\n",
    "\n",
    "        mean_depth = torch.tensor(mean_depth, dtype = torch.float32)\n",
    "        if mean_depth > 4:\n",
    "            bev_map[bev_y_min-10:bev_y_max+10, bev_x_min-10:bev_x_max+10] = 0 # bev_map.max() - mean_depth\n",
    "            bev_map[traj_bev_y_min-10:traj_bev_y_max+10, traj_bev_x_min-10:traj_bev_x_max+10] = 0 # bev_map.max() - mean_depth\n",
    "\n",
    "    bev_map = np.clip(bev_map, 0, 255)\n",
    "    bev_map = bev_map.astype(np.uint8)\n",
    "\n",
    "    return bev_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_bev(bev_map):\n",
    "    bev_map = bev_map.max() - bev_map\n",
    "    bev_map = cv2.normalize(bev_map, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    bev_map = np.uint8(bev_map)\n",
    "    plt.imshow(bev_map, cmap=\"jet\", interpolation='nearest')\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_waypoint(bev_map):\n",
    "    bev_map = bev_map.max() - bev_map\n",
    "    bev_map = cv2.normalize(bev_map, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    bev_map = np.uint8(bev_map)\n",
    "    x_mid = 100\n",
    "    dist_from_x_mid = float('inf')\n",
    "\n",
    "    filtered_ones = []\n",
    "    for x in range(0,200,4):\n",
    "        for y in range(0,200,4):\n",
    "            b = bev_map[y,x]\n",
    "            if b < 80:\n",
    "                dist_from_x_mid = abs(x_mid - x)\n",
    "                x_for_filtering = x\n",
    "                y_for_filtering = y\n",
    "                # plt.scatter(x_for_filtering * 4, y_for_filtering * 3, color = 'black', marker='x')\n",
    "                filtered_ones.append([y_for_filtering, x_for_filtering])\n",
    "                # break\n",
    "\n",
    "    fof_cam = []\n",
    "    for i in filtered_ones:\n",
    "        fy = i[0]\n",
    "        fx = i[1]\n",
    "        fof_cam.append(bev_map[fy, fx])\n",
    "    fof_center = []\n",
    "    for i in range(0, len(filtered_ones)):\n",
    "        curr_x = filtered_ones[i][1]\n",
    "        fof_center.append(abs(x_mid - curr_x))\n",
    "\n",
    "    mini = min(fof_center)\n",
    "    mini_idxs = []\n",
    "    mini_val = float('inf')\n",
    "    for i in range(0, len(fof_center)):\n",
    "        if fof_center[i] <= mini_val:\n",
    "            mini_val = fof_center[i]\n",
    "    for i in range(0, len(fof_center)):\n",
    "        if(fof_center[i] == mini_val):\n",
    "            mini_idxs.append(i)\n",
    "    \n",
    "    max_x = -float('inf')\n",
    "    for i in mini_idxs:\n",
    "        cords = filtered_ones[i]\n",
    "        if cords[1] > max_x:\n",
    "            max_x = cords[1]\n",
    "            sel_y = cords[0]\n",
    "    cord_x = max_x\n",
    "    cord_y = sel_y\n",
    "\n",
    "    x_above = y_for_filtering\n",
    "    \n",
    "    # plt.scatter(cord_x * 4, cord_y * 3, color = 'black', marker='x')\n",
    "    # plt.imshow(depth_map.max() - depth_map, cmap = 'inferno')\n",
    "    # plt.grid(True)\n",
    "    # plt.show()\n",
    "\n",
    "    return cord_x * 4, cord_y * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open('datasets/images/Video_000/v000_0000.png')\n",
    "depth_map = image_to_depth_map(image)\n",
    "plot_depth_map(depth_map, c = 'red')\n",
    "\n",
    "objects = object_detection(image)\n",
    "trajectories = future_positions(objects['boxes'])\n",
    "\n",
    "bev_map = depth_map_to_bev(depth_map, objects['boxes'], trajectories)\n",
    "visualize_bev(bev_map)\n",
    "\n",
    "wp_x, wp_y = get_waypoint(bev_map)\n",
    "plot_depth_map(depth_map, wp_x, wp_y, c = 'red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OUR TEMPORAL FUSION MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size=3):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.conv = nn.Conv2d(input_dim + hidden_dim, hidden_dim * 4, kernel_size, padding=padding)\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        h, c = hidden\n",
    "        combined = torch.cat([x, h], dim=1)\n",
    "        conv_out = self.conv(combined)\n",
    "        i, f, g, o = torch.chunk(conv_out, 4, dim=1)\n",
    "        i, f, g, o = torch.sigmoid(i), torch.sigmoid(f), torch.tanh(g), torch.sigmoid(o)\n",
    "        c_next = f * c + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "        return h_next, c_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim, sequence_length):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.output_dim = output_dim\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "        self.lstm_cells = nn.ModuleList([\n",
    "            ConvLSTMCell(input_dim if i == 0 else hidden_dim, hidden_dim)\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "        self.fc = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, c, h, w = x.size()\n",
    "        h_states = [torch.zeros(b, self.hidden_dim, h, w, device=x.device) for _ in range(self.num_layers)]\n",
    "        c_states = [torch.zeros(b, self.hidden_dim, h, w, device=x.device) for _ in range(self.num_layers)]\n",
    "        \n",
    "        for time_step in range(t):\n",
    "            inp = x[:, time_step]\n",
    "            for i, cell in enumerate(self.lstm_cells):\n",
    "                h_states[i], c_states[i] = cell(inp, (h_states[i], c_states[i]))\n",
    "                inp = h_states[i]\n",
    "        \n",
    "        out = torch.flatten(inp, start_dim=1)\n",
    "\n",
    "        if self.fc is None:\n",
    "            self.fc = nn.Linear(self.hidden_dim * h * w, self.output_dim * self.sequence_length).to(x.device)\n",
    "\n",
    "        return self.fc(out).contiguous().view(b, t, self.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim, hidden_dim, num_layers, output_dim, seq_length = 1, 64, 2, 2, 5\n",
    "model_to_train = ConvLSTM(input_dim, hidden_dim, num_layers, output_dim, seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*--training set ups--*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wp_generation(bev, model):\n",
    "    return model(bev_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, train_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        bev_maps = torch.stack(data[1], dim=1)\n",
    "        waypoints = torch.stack(data[2], dim=1).to(torch.float32)\n",
    "        bev_maps = bev_maps.unsqueeze(2)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pred = wp_generation(bev_maps, model)\n",
    "        loss = criterion(pred, waypoints)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, val_loader, optimizer, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            bev_maps = torch.stack(data[1], dim=1)\n",
    "            waypoints = torch.stack(data[2], dim=1).to(torch.float32)\n",
    "            bev_maps = bev_maps.unsqueeze(2)\n",
    "            \n",
    "            pred = wp_generation(bev_maps, model)\n",
    "            loss = criterion(pred, waypoints)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataloader, optimizer, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            bev_maps = torch.stack(data[1], dim=1)\n",
    "            waypoints = torch.stack(data[2], dim=1).to(torch.float32)\n",
    "            bev_maps = bev_maps.unsqueeze(2)\n",
    "            \n",
    "            pred = wp_generation(bev_maps, model)\n",
    "            print(pred)\n",
    "            print(waypoints)\n",
    "            loss = criterion(pred, waypoints)\n",
    "            total_loss += loss.item()\n",
    "            break\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_save = best_model_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_to_save, 'models/ConvLSTM.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_model = model_to_save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ALGORITHM 3: Future Waypoint Prediction using BEV representations and GPS mechanism**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Saving t1, t1 BEV details*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_processor = AutoImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "detr_model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "\n",
    "root = \"datasets\"\n",
    "images_dir = f\"{root}/images\"\n",
    "labels_dir = f\"{root}/labels\"\n",
    "bev_file = \"dataset_bev.json\"\n",
    "bev_t1_t2_file = \"dataset_t1_t2_bev.json\"\n",
    "batch_size = 8\n",
    "\n",
    "skip = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_t1_t2_bev_dataset(images_dir, output_dir):\n",
    "\n",
    "    for i in range(0, 28):\n",
    "        k = \"0\" if i <= 9 else \"\"\n",
    "        curr_folder_images = f\"{images_dir}/Video_0{k}{i}\"\n",
    "        images_path = [file for file in os.listdir(curr_folder_images) if file.endswith(\".png\")]\n",
    "        images_path.sort()\n",
    "        images_path = images_path[::skip]\n",
    "        x_val = 20\n",
    "\n",
    "        for image_file in images_path:\n",
    "            if x_val == 180:\n",
    "                x_val = 20\n",
    "            bevs = []\n",
    "            image_path = os.path.join(curr_folder_images, image_file)\n",
    "            image = Image.open(image_path)\n",
    "            image_np = np.array(image)\n",
    "\n",
    "            depth_map = image_to_depth_map(image)\n",
    "            objects = object_detection(image)\n",
    "            trajectories_t1, trajectories_t2 = future_positions(objects['boxes'], alpha = 0.3, diff = 0.3)\n",
    "            bev_map_t1, bev_map_t2 = depth_map_to_bev(depth_map, objects['boxes'], trajectories_t1, trajectories_t2)\n",
    "            wp_x_t1, wp_y_t1 = get_waypoint(bev_map_t1, x_val)\n",
    "            wp_x_t2, wp_y_t2 = get_waypoint(bev_map_t2, x_val)\n",
    "\n",
    "            output_image_path = os.path.join(curr_folder_images, image_file)\n",
    "            bevs.append({\n",
    "                \"target\": x_val,\n",
    "                \"image\": output_image_path,\n",
    "                \"bev_map\": bev_map_t1.tolist(), # yup, doing 1 bev only (skipping that of t2)\n",
    "                \"way_point\": [wp_x_t1, wp_y_t1, wp_x_t2, wp_y_t2]\n",
    "            })\n",
    "            # break\n",
    "\n",
    "            with open(os.path.join(output_dir, bev_t1_t2_file), 'a') as wf:\n",
    "                json.dump(bevs, wf)\n",
    "                wf.write(\"\\n\")\n",
    "                # break\n",
    "\n",
    "            x_val = x_val + 40\n",
    "\n",
    "        if(i<27):\n",
    "            print(f\"\\rProgress: {((i + 1) / 0.27):.2f} %\", end = \"\")\n",
    "        # break\n",
    "\n",
    "    print(f\"\\nDone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_t1_t2_bev_dataset(images_dir, root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "getitng dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BEV_t1_t2_Dataset(Dataset):\n",
    "    def __init__(self, root, bev_t1_t2_file):\n",
    "        self.data = []\n",
    "\n",
    "        with open(os.path.join(root, bev_t1_t2_file), \"r\") as wf:\n",
    "            for data in wf:\n",
    "                data = json.loads(data)\n",
    "                data = data[0]\n",
    "                # print(data.keys())\n",
    "\n",
    "                target = data['target']\n",
    "                image = data['image']\n",
    "                bev_map = torch.tensor(data['bev_map'])\n",
    "                way_point = torch.tensor(data['way_point'])\n",
    "                \n",
    "                self.data.append([target, image, bev_map, way_point])\n",
    "                # break\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split indices for train and validation\n",
    "dataset = BEV_t1_t2_Dataset(root, bev_t1_t2_file)\n",
    "dataloader = DataLoader(dataset, batch_size = batch_size, shuffle = True, pin_memory=True, drop_last=True)\n",
    "\n",
    "train_indices, val_indices = train_test_split(range(len(dataset)), train_size = 0.5, test_size=0.5, random_state=42)\n",
    "train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, drop_last=True)\n",
    "\n",
    "val_dataset = torch.utils.data.Subset(dataset, val_indices)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "image and navigation targets > bird's eye view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_depth_map(image):\n",
    "    estimator = pipeline(task=\"depth-estimation\", model=\"Intel/dpt-large\")\n",
    "    result = estimator(images=image)\n",
    "    depth_map = result['predicted_depth']\n",
    "    return depth_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def object_detection(image):\n",
    "    inputs = image_processor(images=image, return_tensors=\"pt\")\n",
    "    outputs = detr_model(**inputs)\n",
    "    target_sizes = torch.tensor([image.size[::-1]])\n",
    "    objects = image_processor.post_process_object_detection(outputs, threshold=0.9, target_sizes=target_sizes)[0]\n",
    "\n",
    "    return objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def future_positions(objects, alpha = 0.1, diff = 5):\n",
    "    trajectories_t1 = []\n",
    "    trajectories_t2 = []\n",
    "    \n",
    "    for obj in objects:\n",
    "        smoothed_positions_t1 = [obj]\n",
    "        smoothed_positions_t2 = [obj]\n",
    "        for t in range(0, len(obj)):\n",
    "            smoothed_pos_t1 = alpha * obj[t] + (1 - alpha) * smoothed_positions_t1[-1]\n",
    "            smoothed_pos_t2 = (diff * alpha) * obj[t] + (1 - (diff * alpha)) * smoothed_positions_t2[-1]\n",
    "            smoothed_positions_t1.append(smoothed_pos_t1)\n",
    "            smoothed_positions_t2.append(smoothed_pos_t2)\n",
    "        future_positions_t1 = [smoothed_positions_t1[-1]]\n",
    "        future_positions_t2 = [smoothed_positions_t2[-1]]\n",
    "        trajectories_t1.append(future_positions_t1)\n",
    "        trajectories_t2.append(future_positions_t2)\n",
    "\n",
    "    new_t1 = []\n",
    "    new_t2 = []\n",
    "    for t1, t2 in zip(trajectories_t1, trajectories_t2):\n",
    "        new_t1.append(t1[0].detach().numpy())\n",
    "        new_t2.append(t2[0].detach().numpy())\n",
    "\n",
    "    trajectories_t1 = torch.tensor(new_t1, dtype = torch.float32)\n",
    "    trajectories_t2 = torch.tensor(new_t2, dtype = torch.float32)\n",
    "        \n",
    "    return trajectories_t1, trajectories_t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def depth_map_to_bev(depth_map, objects, trajectories_t1, trajectories_t2):\n",
    "    f_x = 1000\n",
    "    f_y = 1000\n",
    "    c_x = 400\n",
    "    c_y = 300\n",
    "    bev_size = 200\n",
    "    scale_factor = 15\n",
    "\n",
    "    height, width = depth_map.shape\n",
    "    bev_map_t1 = np.zeros((bev_size, bev_size), dtype=np.float32)\n",
    "    bev_map_t2 = np.zeros((bev_size, bev_size), dtype=np.float32)\n",
    "\n",
    "    x_coords, y_coords = np.meshgrid(np.arange(width), np.arange(height))\n",
    "\n",
    "    depth = depth_map[y_coords, x_coords].detach().numpy()\n",
    "    X = (x_coords - c_x) * depth / f_x\n",
    "    Y = (y_coords - c_y) * depth / f_y\n",
    "\n",
    "    bev_x = np.clip((X * scale_factor).astype(int), 0, bev_size - 1)\n",
    "    bev_y = np.clip((Y * scale_factor).astype(int), 0, bev_size - 1)\n",
    "\n",
    "    for i in range(height * width):\n",
    "        bev_map_t1[bev_y.flat[i], bev_x.flat[i]] = max(bev_map_t1[bev_y.flat[i], bev_x.flat[i]], depth.flat[i])\n",
    "        bev_map_t2[bev_y.flat[i], bev_x.flat[i]] = max(bev_map_t2[bev_y.flat[i], bev_x.flat[i]], depth.flat[i])\n",
    "\n",
    "    for obj, traj_t1, traj_t2 in zip(objects, trajectories_t1, trajectories_t2):\n",
    "        x_min, y_min, x_max, y_max = obj\n",
    "        x_min, y_min, x_max, y_max = x_min.item(), y_min.item(), x_max.item(), y_max.item()\n",
    "\n",
    "        traj_x_min_t1, traj_y_min_t1, traj_x_max_t1, traj_y_max_t1 = traj_t1\n",
    "        traj_x_min_t1, traj_y_min_t1, traj_x_max_t1, traj_y_max_t1 = traj_x_min_t1.item(), traj_y_min_t1.item(), traj_x_max_t1.item(), traj_y_max_t1.item()\n",
    "\n",
    "        traj_x_min_t2, traj_y_min_t2, traj_x_max_t2, traj_y_max_t2 = traj_t2\n",
    "        traj_x_min_t2, traj_y_min_t2, traj_x_max_t2, traj_y_max_t2 = traj_x_min_t2.item(), traj_y_min_t2.item(), traj_x_max_t2.item(), traj_y_max_t2.item()\n",
    "\n",
    "        obj_depths = depth_map[int(y_min):int(y_max), int(x_min):int(x_max)].detach().numpy()\n",
    "        if obj_depths.size > 0:\n",
    "            mean_depth = np.mean(obj_depths)\n",
    "        else:\n",
    "            mean_depth = 0\n",
    "        \n",
    "        X_min = (x_min - c_x) * mean_depth / f_x\n",
    "        X_max = (x_max - c_x) * mean_depth / f_x\n",
    "        Y_min = (y_min - c_y) * mean_depth / f_y\n",
    "        Y_max = (y_max - c_y) * mean_depth / f_y\n",
    "        Z = mean_depth\n",
    "        bev_x_min = int(100 + X_min * scale_factor)\n",
    "        bev_x_max = int(100 + X_max * scale_factor)\n",
    "        bev_y_min = int(100 + Y_min * scale_factor)\n",
    "        bev_y_max = int(100 + Y_max * scale_factor)\n",
    "\n",
    "        traj_X_min_t1 = (traj_x_min_t1 - c_x) * mean_depth / f_x\n",
    "        traj_X_max_t1 = (traj_x_max_t1 - c_x) * mean_depth / f_x\n",
    "        traj_Y_min_t1 = (traj_y_min_t1 - c_y) * mean_depth / f_y\n",
    "        traj_Y_max_t1 = (traj_y_max_t1 - c_y) * mean_depth / f_y\n",
    "        traj_Z_t1 = mean_depth\n",
    "        traj_bev_x_min_t1 = int(100 + traj_X_min_t1 * scale_factor)\n",
    "        traj_bev_x_max_t1 = int(100 + traj_X_max_t1 * scale_factor)\n",
    "        traj_bev_y_min_t1 = int(100 + traj_Y_min_t1 * scale_factor)\n",
    "        traj_bev_y_max_t1 = int(100 + traj_Y_max_t1 * scale_factor)\n",
    "\n",
    "        traj_X_min_t2 = (traj_x_min_t2 - c_x) * mean_depth / f_x\n",
    "        traj_X_max_t2 = (traj_x_max_t2 - c_x) * mean_depth / f_x\n",
    "        traj_Y_min_t2 = (traj_y_min_t2 - c_y) * mean_depth / f_y\n",
    "        traj_Y_max_t2 = (traj_y_max_t2 - c_y) * mean_depth / f_y\n",
    "        traj_Z_t2 = mean_depth\n",
    "        traj_bev_x_min_t2 = int(100 + traj_X_min_t2 * scale_factor)\n",
    "        traj_bev_x_max_t2 = int(100 + traj_X_max_t2 * scale_factor)\n",
    "        traj_bev_y_min_t2 = int(100 + traj_Y_min_t2 * scale_factor)\n",
    "        traj_bev_y_max_t2 = int(100 + traj_Y_max_t2 * scale_factor)\n",
    "\n",
    "        mean_depth = torch.tensor(mean_depth, dtype = torch.float32)\n",
    "        if mean_depth > 4:\n",
    "            # bev_map_t1[bev_y_min-10:bev_y_max+10, bev_x_min-10:bev_x_max+10] = 0 # bev_map_t1.max() - mean_depth\n",
    "            bev_map_t1[traj_bev_y_min_t1-10:traj_bev_y_max_t1+10, traj_bev_x_min_t1-10:traj_bev_x_max_t1+10] = 0 # bev_map_t1.max() - mean_depth\n",
    "            \n",
    "            # bev_map_t2[bev_y_min-10:bev_y_max+10, bev_x_min-10:bev_x_max+10] = 0 # bev_map_t2.max() - mean_depth\n",
    "            bev_map_t2[traj_bev_y_min_t2-10:traj_bev_y_max_t2+10, traj_bev_x_min_t2-10:traj_bev_x_max_t2+10] = 0 # bev_map_t2.max() - mean_depth\n",
    "\n",
    "    bev_map_t1 = np.clip(bev_map_t1, 0, 255)\n",
    "    bev_map_t1 = bev_map_t1.astype(np.uint8)\n",
    "\n",
    "    bev_map_t2 = np.clip(bev_map_t2, 0, 255)\n",
    "    bev_map_t2 = bev_map_t2.astype(np.uint8)\n",
    "\n",
    "    return bev_map_t1, bev_map_t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_waypoint(bev_map, x_mid):\n",
    "    bev_map = bev_map.max() - bev_map\n",
    "    bev_map = cv2.normalize(bev_map, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    bev_map = np.uint8(bev_map)\n",
    "    dist_from_x_mid = float('inf')\n",
    "\n",
    "    filtered_ones = []\n",
    "    for x in range(0,200,4):\n",
    "        for y in range(0,200,4):\n",
    "            b = bev_map[y,x]\n",
    "            if b < 80:\n",
    "                dist_from_x_mid = abs(x_mid - x)\n",
    "                x_for_filtering = x\n",
    "                y_for_filtering = y\n",
    "                # plt.scatter(x_for_filtering * 4, y_for_filtering * 3, color = 'black', marker='x')\n",
    "                filtered_ones.append([y_for_filtering, x_for_filtering])\n",
    "                # break\n",
    "\n",
    "    fof_cam = []\n",
    "    for i in filtered_ones:\n",
    "        fy = i[0]\n",
    "        fx = i[1]\n",
    "        fof_cam.append(bev_map[fy, fx])\n",
    "    fof_center = []\n",
    "    for i in range(0, len(filtered_ones)):\n",
    "        curr_x = filtered_ones[i][1]\n",
    "        fof_center.append(abs(x_mid - curr_x))\n",
    "\n",
    "    mini = min(fof_center)\n",
    "    mini_idxs = []\n",
    "    mini_val = float('inf')\n",
    "    for i in range(0, len(fof_center)):\n",
    "        if fof_center[i] <= mini_val:\n",
    "            mini_val = fof_center[i]\n",
    "    for i in range(0, len(fof_center)):\n",
    "        if(fof_center[i] == mini_val):\n",
    "            mini_idxs.append(i)\n",
    "    \n",
    "    max_x = -float('inf')\n",
    "    for i in mini_idxs:\n",
    "        cords = filtered_ones[i]\n",
    "        if cords[1] > max_x:\n",
    "            max_x = cords[1]\n",
    "            sel_y = cords[0]\n",
    "    cord_x = max_x\n",
    "    cord_y = sel_y\n",
    "\n",
    "    x_above = y_for_filtering\n",
    "    \n",
    "    # plt.scatter(cord_x * 4, cord_y * 3, color = 'black', marker='x')\n",
    "    # plt.imshow(depth_map.max() - depth_map, cmap = 'inferno')\n",
    "    # plt.grid(True)\n",
    "    # plt.show()\n",
    "\n",
    "    return cord_x * 4, cord_y * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_relative_to_ego(ego_transform, point_world):\n",
    "    ego_loc = ego_transform.location\n",
    "    ego_rot = ego_transform.rotation\n",
    "    \n",
    "    yaw_rad = np.radians(ego_rot.yaw)\n",
    "    \n",
    "    R = np.array([\n",
    "        [np.cos(-yaw_rad), -np.sin(-yaw_rad)],\n",
    "        [np.sin(-yaw_rad),  np.cos(-yaw_rad)]\n",
    "    ])\n",
    "    \n",
    "    translated_point = np.array([point_world.x - ego_loc.x, point_world.y - ego_loc.y])\n",
    "    \n",
    "    local_point = R @ translated_point\n",
    "    \n",
    "    return local_point[1], local_point[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ego_rotation(local_x, local_y):\n",
    "    rel = math.degrees(math.atan(local_x / local_y))\n",
    "    return rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_points_from_gps(target):\n",
    "    ego_transform = ego.get_transform()\n",
    "    point_world = carla.Location(*target)\n",
    "\n",
    "    local_x, local_y = target_relative_to_ego(ego_transform, point_world)\n",
    "    dist_from_ego = ((local_x ** 2) + (local_y ** 2)) ** (1/2)\n",
    "    rel = ego_rotation(local_x, local_y)\n",
    "\n",
    "    rel = max(min(rel, 50), -50)\n",
    "    local_y = max(min(local_y, 37.5), 0)\n",
    "\n",
    "    x_to_plot = 400 + (8 * rel)\n",
    "    y_to_plot = 600 - (25 * local_y)\n",
    "\n",
    "    return dist_from_ego, rel, x_to_plot, y_to_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bev_on_graph(local_x, local_y):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(0, 0, color='red', label='e', s=100)\n",
    "    plt.scatter(local_x, local_y, color='blue', label='t')\n",
    "    plt.xlim(-800, 800)\n",
    "    plt.ylim(-600, 600)\n",
    "    plt.xlabel(\"X (meters)\")\n",
    "    plt.ylabel(\"Y (meters)\")\n",
    "    plt.axhline(0, color='black',linewidth=1)\n",
    "    plt.axvline(0, color='black',linewidth=1)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_bev(bev_map):\n",
    "    bev_map = bev_map.max() - bev_map\n",
    "    bev_map = cv2.normalize(bev_map, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    bev_map = np.uint8(bev_map)\n",
    "    plt.figure(figsize = (3,3))\n",
    "    plt.imshow(bev_map, cmap=\"jet\", interpolation='nearest')\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_depth_map(depth_map, cord_x = 400, cord_y = 75, c = 'white'):\n",
    "    depth_map = depth_map.max() - depth_map\n",
    "    height, width = depth_map.shape\n",
    "    depth_at_cord = depth_map[cord_y-1, cord_x-1]\n",
    "    print(f\"Depth at co-ordinate ({cord_x}, {cord_y}) of the image: {depth_at_cord:.2f} meters\")\n",
    "    plt.scatter(cord_x, cord_y, color=c, marker='x')\n",
    "    plt.imshow(depth_map, cmap = 'inferno')\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open('datasets/images/Video_000/v000_0000.png')\n",
    "depth_map = image_to_depth_map(image)\n",
    "objects = object_detection(image)\n",
    "trajectories_t1, trajectories_t2 = future_positions(objects['boxes'], alpha = 0.3, diff = 0.3)\n",
    "bev_map_t1, bev_map_t2 = depth_map_to_bev(depth_map, objects['boxes'], trajectories_t1, trajectories_t2)\n",
    "\n",
    "target = (158.0238037109375, 46.202396392822266, 0.0)\n",
    "wp_x_t1, wp_y_t1 = get_waypoint(bev_map_t1, target[0])\n",
    "wp_x_t2, wp_y_t2 = get_waypoint(bev_map_t2, target[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_bev(bev_map_t1)\n",
    "plot_depth_map(depth_map, wp_x_t1, wp_y_t1, c = 'red')\n",
    "visualize_bev(bev_map_t2)\n",
    "plot_depth_map(depth_map, wp_x_t2, wp_y_t2, c = 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = (158.0238037109375, 46.202396392822266, 0.0)\n",
    "dist, rel, x_target, y_target = target_points_from_gps(target)\n",
    "print(dist, rel, x_target, y_target)\n",
    "bev_on_graph(x_target, y_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OUR CNN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_WPModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_WPModel, self).__init__()\n",
    "\n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "        self.resnet_features = nn.Sequential(*list(resnet.children())[:-1])\n",
    "\n",
    "        # self.conv_layers = nn.Sequential(\n",
    "        #     nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
    "        #     nn.BatchNorm2d(32),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.MaxPool2d(2, 2),\n",
    "        #     nn.Conv2d(32, 256, kernel_size=3, stride=1, padding=1),\n",
    "        #     nn.BatchNorm2d(256),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.MaxPool2d(2, 2),\n",
    "        #     nn.Conv2d(256, 1024, kernel_size=3, stride=1, padding=1),\n",
    "        #     nn.BatchNorm2d(1024),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.MaxPool2d(2, 2),\n",
    "        #     nn.Conv2d(1024, 256, kernel_size=3, stride=1, padding=1),\n",
    "        #     nn.BatchNorm2d(256),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.MaxPool2d(2, 2),\n",
    "        #     nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "        #     nn.BatchNorm2d(512),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.MaxPool2d(2, 2),\n",
    "        # )\n",
    "        \n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(512 + 1, 512),\n",
    "            # nn.Linear(512 * 6 * 6 + 1, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 4),\n",
    "            nn.Dropout(0.25),\n",
    "        )\n",
    "\n",
    "    def forward(self, pixel_values, target_loc):\n",
    "        x = self.resnet_features(pixel_values)\n",
    "        # x = self.conv_layers(pixel_values)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.cat((x, target_loc.unsqueeze(0).view(1, 1)), dim=1)\n",
    "        output = self.fc_layers(x)\n",
    "        \n",
    "        return output.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_train = CNN_WPModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*--training set ups--*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wp_generation(values, locs, model):\n",
    "    return model(values, locs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, train_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        target_loc = torch.tensor(data[0], dtype = torch.float)\n",
    "        bev_maps = data[2].float()\n",
    "        waypoints = data[3].float()\n",
    "        bev_maps = bev_maps.unsqueeze(0).permute(0, 2, 1)\n",
    "        bev_maps = bev_maps.repeat(1, 3, 1, 1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pred = wp_generation(bev_maps, target_loc, model)\n",
    "        loss = criterion(pred, waypoints)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, val_loader, optimizer, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            target_loc = torch.tensor(data[0], dtype = torch.float)\n",
    "            bev_maps = data[2].float()\n",
    "            waypoints = data[3].float()\n",
    "            bev_maps = bev_maps.unsqueeze(0).permute(0, 2, 1)\n",
    "            bev_maps = bev_maps.repeat(1, 3, 1, 1)\n",
    "            \n",
    "            pred = wp_generation(bev_maps, target_loc, model)\n",
    "            loss = criterion(pred, waypoints)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataloader, optimizer, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            target_loc = torch.tensor(data[0], dtype = torch.float)\n",
    "            bev_maps = data[2].float()\n",
    "            waypoints = data[3].float()\n",
    "            bev_maps = bev_maps.unsqueeze(0).permute(0, 2, 1)\n",
    "            bev_maps = bev_maps.repeat(1, 3, 1, 1)\n",
    "            \n",
    "            pred = wp_generation(bev_maps, target_loc, model)\n",
    "            print(pred)\n",
    "            print(waypoints)\n",
    "            loss = criterion(pred, waypoints)\n",
    "            total_loss += loss.item()\n",
    "            break\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_save = best_model_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_to_save, 'models/CNN_WPModel.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_model = model_to_save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Management Section**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model_to_train.parameters(), lr=1e-2)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = test(model_to_train, dataset, optimizer, criterion)\n",
    "print(f\"test loss is: {test_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 10\n",
    "num_epochs = 20\n",
    "lr = 1e0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,iterations):\n",
    "  lr = lr / 10\n",
    "  optimizer = optim.Adam(model_to_train.parameters(), lr=lr)\n",
    "  best_train_loss = float('inf')\n",
    "  best_val_loss = float('inf')\n",
    "  patience = 5\n",
    "  print(f\"Setting Learning rate to: {lr}\")\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "    best_model_trained = model_to_train\n",
    "    if(patience == 0):\n",
    "      break\n",
    "    \n",
    "    train_loss = training(model_to_train, train_dataset, optimizer, criterion)\n",
    "    val_loss = validation(model_to_train, val_dataset, optimizer, criterion)\n",
    "    \n",
    "    if(train_loss > (best_train_loss * 0.95) or (val_loss > (best_val_loss * 0.95))):\n",
    "      model_to_train = best_model_trained\n",
    "      patience -= 1\n",
    "    else:\n",
    "      best_train_loss = train_loss\n",
    "      best_val_loss = val_loss\n",
    "      best_model_trained = model_to_train\n",
    "      patience = 5\n",
    "      # torch.save(model_to_train.state_dict(), '/content/drive/MyDrive/my_model.pth')\n",
    "    print(f\"\\rEpoch {epoch + 1}/{num_epochs}, train-Loss: {train_loss:.1f}, val-Loss: {val_loss:.1f}     \", end = \"\")#, patience: {patience}\", end = \"\")\n",
    "    # break\n",
    "  print(\"\\n\")\n",
    "  # break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------- REDUNENT CODE ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VitWPModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VitWPModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(512)\n",
    "        self.conv3 = nn.Conv2d(512, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "        self.conv5 = nn.Conv2d(512, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 512)\n",
    "        self.fc2 = nn.Linear(512, 2)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        x = torch.relu(self.conv1(pixel_values))\n",
    "        x = self.bn1(x)\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        \n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.bn2(x)\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = self.bn3(x)\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "\n",
    "        x = torch.relu(self.conv4(x))\n",
    "        x = self.bn4(x)\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "\n",
    "        x = torch.relu(self.conv5(x))\n",
    "        x = self.bn5(x)\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        output = self.fc2(x)\n",
    "        return output.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BEVToImage(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BEVToImage, self).__init__()\n",
    "        self.backbone = models.resnet18(pretrained=True)\n",
    "        self.backbone = nn.Sequential(*list(self.backbone.children())[:-2])\n",
    "        self.conv1x1 = nn.Conv2d(512, 256, kernel_size=1)\n",
    "        \n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 512, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(512, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 1, kernel_size=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.conv1x1(x)\n",
    "        x = self.upsample(x)\n",
    "        x = F.interpolate(x, size=(200, 200), mode='bilinear', align_corners=False)\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size=3):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.conv = nn.Conv2d(input_dim + hidden_dim, hidden_dim * 4, kernel_size, padding=padding)\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        h, c = hidden\n",
    "        combined = torch.cat([x, h], dim=1)\n",
    "        conv_out = self.conv(combined)\n",
    "        i, f, g, o = torch.chunk(conv_out, 4, dim=1)\n",
    "        i, f, g, o = torch.sigmoid(i), torch.sigmoid(f), torch.tanh(g), torch.sigmoid(o)\n",
    "        c_next = f * c + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "        return h_next, c_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim, sequence_length):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.output_dim = output_dim\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "        self.lstm_cells = nn.ModuleList([\n",
    "            ConvLSTMCell(input_dim if i == 0 else hidden_dim, hidden_dim)\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "        self.fc = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, c, h, w = x.size()\n",
    "        h_states = [torch.zeros(b, self.hidden_dim, h, w, device=x.device) for _ in range(self.num_layers)]\n",
    "        c_states = [torch.zeros(b, self.hidden_dim, h, w, device=x.device) for _ in range(self.num_layers)]\n",
    "        \n",
    "        for time_step in range(t):\n",
    "            inp = x[:, time_step]\n",
    "            for i, cell in enumerate(self.lstm_cells):\n",
    "                h_states[i], c_states[i] = cell(inp, (h_states[i], c_states[i]))\n",
    "                inp = h_states[i]\n",
    "        \n",
    "        out = torch.flatten(inp, start_dim=1)\n",
    "\n",
    "        if self.fc is None:\n",
    "            self.fc = nn.Linear(self.hidden_dim * h * w, self.output_dim * self.sequence_length).to(x.device)\n",
    "\n",
    "        return self.fc(out).view(b, 5, 2).mean(dim=1).view(b, t, self.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_to_bev_model = BEVToImage()\n",
    "image_to_bev_model = torch.load('models/BEVToImage.pth', map_location=torch.device('cpu'))\n",
    "\n",
    "input_dim, hidden_dim, num_layers, output_dim, seq_length = 1, 64, 2, 2, 5\n",
    "driver_model = ConvLSTM(input_dim, hidden_dim, num_layers, output_dim, seq_length)\n",
    "driver_model = torch.load('models/ConvLSTM.pth', map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_WPModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_WPModel, self).__init__()\n",
    "\n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "        self.resnet_features = nn.Sequential(*list(resnet.children())[:-1])\n",
    "\n",
    "        # self.conv_layers = nn.Sequential(\n",
    "        #     nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
    "        #     nn.BatchNorm2d(32),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.MaxPool2d(2, 2),\n",
    "        #     nn.Conv2d(32, 256, kernel_size=3, stride=1, padding=1),\n",
    "        #     nn.BatchNorm2d(256),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.MaxPool2d(2, 2),\n",
    "        #     nn.Conv2d(256, 1024, kernel_size=3, stride=1, padding=1),\n",
    "        #     nn.BatchNorm2d(1024),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.MaxPool2d(2, 2),\n",
    "        #     nn.Conv2d(1024, 256, kernel_size=3, stride=1, padding=1),\n",
    "        #     nn.BatchNorm2d(256),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.MaxPool2d(2, 2),\n",
    "        #     nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "        #     nn.BatchNorm2d(512),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.MaxPool2d(2, 2),\n",
    "        # )\n",
    "        \n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(512 + 1, 512),\n",
    "            # nn.Linear(512 * 6 * 6 + 1, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 4),\n",
    "            nn.Dropout(0.25),\n",
    "        )\n",
    "\n",
    "    def forward(self, pixel_values, target_loc):\n",
    "        x = self.resnet_features(pixel_values)\n",
    "        # x = self.conv_layers(pixel_values)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.cat((x, target_loc.unsqueeze(0).view(1, 1)), dim=1)\n",
    "        output = self.fc_layers(x)\n",
    "        \n",
    "        return output.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CarLA simulation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "semantic_label_classes = [\"Traffic Sign\", \"Building\", \"Fence\", \"Other\", \"Pedestrian\", \"Pole\", \"Road Line\", \"Road\", \"Sidewalk\", \"Vegetation\", \"Car\", \"Wall\", \"Unlabeled\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = carla.Client('localhost', 2000)\n",
    "world = client.get_world()\n",
    "blueprint_library = world.get_blueprint_library()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_all_actors():\n",
    "    actors = world.get_actors()\n",
    "    for actor in actors:\n",
    "        if 'vehicle' in actor.type_id:\n",
    "            actor.destroy()\n",
    "    world.tick()\n",
    "    print(\"previous ones destroyed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initiating_carla(world, blueprint_library):\n",
    "    ego_blueprint = blueprint_library.filter('*model3')[0]\n",
    "    spawn_points = world.get_map().get_spawn_points()[0]\n",
    "    global ego\n",
    "    ego = world.spawn_actor(ego_blueprint, spawn_points)\n",
    "\n",
    "    camera_bp = blueprint_library.find('sensor.camera.rgb')\n",
    "    camera_bp.set_attribute('image_size_x', '800')\n",
    "    camera_bp.set_attribute('image_size_y', '600')\n",
    "    camera_bp.set_attribute('fov', '110')\n",
    "    camera_transform = carla.Transform(carla.Location(x = 1.5, z = 2.4))\n",
    "    global camera\n",
    "    camera = world.spawn_actor(camera_bp, camera_transform, attach_to=ego)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_all_actors()\n",
    "initiating_carla(world, blueprint_library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EGO_relative_view(EGO):\n",
    "    view = EGO.get_transform().location + carla.Location(x=0, y=0, z=10)\n",
    "    return view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_cam_to_EGO(EGO, angle = 65):\n",
    "    spectator = world.get_spectator()\n",
    "    vehicle_transform = EGO.get_transform()\n",
    "    spectator.set_transform(\n",
    "        carla.Transform(\n",
    "            EGO_relative_view(EGO),\n",
    "            carla.Rotation(pitch = -angle, yaw = vehicle_transform.rotation.yaw, roll = 0)\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def camera_to_image(image):\n",
    "    global image_captured\n",
    "    array = np.frombuffer(image.raw_data, dtype=np.uint8)\n",
    "    array = array.reshape((image.height, image.width, 4))\n",
    "    image_captured = Image.fromarray(array[:, :, :3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_cam_to_EGO(ego)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*functions for model output to control*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def steering(x, intended_x, s_power):\n",
    "    towards = intended_x - x\n",
    "    steer = towards * 0.0002\n",
    "    steer = steer * (1 + s_power) / 1.5\n",
    "    steer = max(-1, min(steer, 1))\n",
    "    return steer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TandB(y_mid, intended_y_mid, curr_y = 25):\n",
    "    if(intended_y_mid <= curr_y):\n",
    "        return 0, 1\n",
    "    intended_y_mid = intended_y_mid if intended_y_mid < y_mid else y_mid\n",
    "    return max(0.3, min((y_mid - intended_y_mid) / (2 * y_mid), 1)), 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def control_ego(steer, throttle, brake):\n",
    "    control = carla.VehicleControl()\n",
    "    control.steer = steer\n",
    "    control.throttle = throttle\n",
    "    control.brake = brake\n",
    "\n",
    "    return control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculation of metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*--route completion--*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_RC(intended_x_mid, ego_x_position, intended_y_mid, ego_y_position):\n",
    "    global rcc\n",
    "    if intended_y_mid > ego_y_position:\n",
    "        dist = math.sqrt(((intended_x_mid - ego_x_position) ** 2) + ((intended_y_mid - ego_y_position) ** 2))\n",
    "        if dist <= 900:\n",
    "            rcc = rcc + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*--infraction score--*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollisionSensor(object):\n",
    "    def __init__(self, vehicle):\n",
    "        self.vehicle = vehicle\n",
    "        self.world = vehicle.get_world()\n",
    "        self.collision_history = []\n",
    "\n",
    "        blueprint = self.world.get_blueprint_library().find('sensor.other.collision')\n",
    "        self.sensor = self.world.spawn_actor(blueprint, carla.Transform(), attach_to=self.vehicle)\n",
    "        self.sensor.listen(self._on_collision)\n",
    "\n",
    "    def _on_collision(self, event):\n",
    "        self.collision_history.append(event)\n",
    "        \n",
    "    def get_collision_count(self):\n",
    "        return len(self.collision_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_red_light_violation():\n",
    "    global red_light_violations\n",
    "    ego_position = ego.get_location()\n",
    "    traffic_lights = world.get_traffic_lights()\n",
    "    red_light_violation = False\n",
    "    for traffic_light in traffic_lights:\n",
    "        light_position = traffic_light.get_location()\n",
    "        distance_to_light = ego_position.distance(light_position)\n",
    "        if distance_to_light < 20.0:\n",
    "            if traffic_light.get_state() == carla.TrafficLightState.Red:\n",
    "                red_light_violation = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_IS(speed_limit, red_light_violations, collision_sensor):\n",
    "    global isc\n",
    "\n",
    "    velocity = ego.get_velocity()\n",
    "    speed = (velocity.x**2 + velocity.y**2 + velocity.z**2) ** 0.5 * 3.6\n",
    "    \n",
    "    if speed > speed_limit:\n",
    "        isc -= 1\n",
    "        \n",
    "    if red_light_violations:\n",
    "        isc -= 1\n",
    "\n",
    "    if collision_sensor.get_collision_count() > 0:\n",
    "        isc -= 10\n",
    "\n",
    "    if isc < 0:\n",
    "        isc = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*--driving score--*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_DS():\n",
    "    global RC\n",
    "    global IS\n",
    "    global dsc\n",
    "    dsc = (RC + IS) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "controlling based on vit_wp_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_model = VitWPModel()\n",
    "driver_model.load_state_dict(torch.load('models/VitWPModel.pth', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_emb(image):\n",
    "    return processor(text=semantic_label_classes, images=image, return_tensors=\"pt\", padding=True)['pixel_values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_output_generation(image, vit_wp_model):\n",
    "    pixel_values = image_to_emb(image).squeeze(1)\n",
    "    outputs = vit_wp_model(pixel_values)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_wp(way_p_1, top_w,top_h):\n",
    "    way_p = way_p_1\n",
    "\n",
    "    mini_f = float('inf')\n",
    "    maxi_f = -float('inf')\n",
    "    for i in way_p[0]:\n",
    "        if(maxi_f < i[0]):\n",
    "            maxi_f = i[0]\n",
    "        if(mini_f > i[0]):\n",
    "            mini_f = i[0]\n",
    "        \n",
    "    mini_s = float('inf')\n",
    "    maxi_s = -float('inf')\n",
    "    for i in way_p[1]:\n",
    "        if(maxi_s < i[1]):\n",
    "            maxi_s = i[1]\n",
    "        if(mini_s > i[1]):\n",
    "            mini_s = i[1]\n",
    "        \n",
    "    for i in way_p:\n",
    "        i[0] = (i[0] - mini_f) / (maxi_f - mini_f)\n",
    "        i[1] = (i[1] - mini_s) / (maxi_s - mini_s)\n",
    "        i[0] *= top_h\n",
    "        i[1] *= top_w\n",
    "\n",
    "    return way_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def points():\n",
    "    return 400, 120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "controlling based on conv_LSTM_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_to_bev_model = BEVToImage()\n",
    "image_to_bev_model = torch.load('models/BEVToImage.pth', map_location=torch.device('cpu'))\n",
    "\n",
    "input_dim, hidden_dim, num_layers, output_dim, seq_length = 1, 64, 2, 2, 5\n",
    "driver_model = ConvLSTM(input_dim, hidden_dim, num_layers, output_dim, seq_length)\n",
    "driver_model = torch.load('models/ConvLSTM.pth', map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_output_generation(image, driver_model):\n",
    "    bev = image_to_bev_model(torch.tensor(np.array(image, dtype = np.float32) / 255.0, dtype=torch.float32).unsqueeze(0).permute(0,3,1,2)).unsqueeze(0)\n",
    "    bev = bev.unsqueeze(0)\n",
    "    outputs = driver_model(bev.unsqueeze(0))\n",
    "    return outputs.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def points():\n",
    "    return 400, 120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "controlling based on CNN_wp_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_to_bev_model = BEVToImage()\n",
    "image_to_bev_model = torch.load('models/BEVToImage.pth', map_location=torch.device('cpu'))\n",
    "\n",
    "driver_model = CNN_WPModel()\n",
    "driver_model = torch.load('models/CNN_WPModel.pth', map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_road_map(vehicle, world):\n",
    "    curr_loc = vehicle.get_location()\n",
    "    map = world.get_map()\n",
    "    wps = map.get_waypoint(curr_loc,project_to_road=True, lane_type=(carla.LaneType.Driving | carla.LaneType.Sidewalk))\n",
    "\n",
    "    my_geolocation = map.transform_to_geolocation(vehicle.get_transform().location)\n",
    "    all_map_waypoints = map.generate_waypoints(2.0)\n",
    "    waypoints_on_map = map.get_topology()\n",
    "\n",
    "    return waypoints_on_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(loc1, loc2):\n",
    "    return math.sqrt((loc1.x - loc2.x) ** 2 + (loc1.y - loc2.y) ** 2 + (loc1.z - loc2.z) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_graph(junction_waypoints):\n",
    "    graph = {}\n",
    "    ego_loc = (ego.get_transform().location.x, ego.get_transform().location.y, ego.get_transform().location.z)\n",
    "    if ego_loc not in graph:\n",
    "        graph[ego_loc] = []\n",
    "    to = (junction_waypoints[0][0].transform.location.x, junction_waypoints[0][0].transform.location.y, junction_waypoints[0][0].transform.location.z)\n",
    "    graph[ego_loc].append((to, 0.0))\n",
    "    \n",
    "    for start_wp, end_wp in junction_waypoints:\n",
    "        start_loc = (start_wp.transform.location.x, start_wp.transform.location.y, start_wp.transform.location.z)\n",
    "        end_loc = (end_wp.transform.location.x, end_wp.transform.location.y, end_wp.transform.location.z)\n",
    "        dist = distance(start_wp.transform.location, end_wp.transform.location)\n",
    "\n",
    "        if start_loc not in graph:\n",
    "            graph[start_loc] = []\n",
    "        graph[start_loc].append((end_loc, dist))\n",
    "\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dijkstra(graph, start):\n",
    "    queue = [(0, start)]\n",
    "    distances = {node: float('inf') for node in graph}\n",
    "    distances[start] = 0\n",
    "    \n",
    "    previous_nodes = {}\n",
    "\n",
    "    while queue:\n",
    "        current_distance, current_node = heapq.heappop(queue)\n",
    "        for neighbor, weight in graph.get(current_node, []):\n",
    "            new_distance = current_distance + weight\n",
    "            if new_distance < distances[neighbor]:\n",
    "                distances[neighbor] = new_distance\n",
    "                previous_nodes[neighbor] = current_node\n",
    "                heapq.heappush(queue, (new_distance, neighbor))\n",
    "\n",
    "    return distances, previous_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_path(previous_nodes, start, goal):\n",
    "    path = []\n",
    "    current = goal\n",
    "    while current != start:\n",
    "        path.append(current)\n",
    "        current = previous_nodes.get(current)\n",
    "        if current is None:\n",
    "            return None\n",
    "    path.append(start)\n",
    "    path.reverse()\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_ego_loc = ego.get_transform().location\n",
    "initiation = (curr_ego_loc.x, curr_ego_loc.y, curr_ego_loc.z)\n",
    "goal = (158.0238037109375, 46.202396392822266, 0.0)\n",
    "\n",
    "wps = make_graph(get_road_map(ego, world))\n",
    "distances, prev_nodes = dijkstra(wps, initiation)\n",
    "path = reconstruct_path(prev_nodes, initiation, goal)\n",
    "\n",
    "print(\"Shortest path:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_output_generation(image, model, locs = 400):\n",
    "    np_image = np.array(image, dtype = np.float32) / 255.0\n",
    "    tensor_image = torch.tensor(np_image, dtype=torch.float32)\n",
    "    tensor_image = tensor_image.permute(2,0,1).unsqueeze(0)\n",
    "    bev_op = image_to_bev_model(tensor_image)\n",
    "\n",
    "    bev_maps = bev_op.unsqueeze(0).permute(0, 2, 1)\n",
    "    bev = bev_maps.repeat(1, 3, 1, 1)\n",
    "\n",
    "    outputs = driver_model(bev, torch.tensor(locs, dtype = torch.float32))\n",
    "    return outputs.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def points():\n",
    "    return 400, 120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RUNNING THE SIMULATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"started\")\n",
    "\n",
    "    prev_image_captured = None\n",
    "    image_captured = None\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    RC = 0\n",
    "    IS = 0\n",
    "    DS = 0\n",
    "    rcc = 0\n",
    "    isc = 100\n",
    "\n",
    "    distance = 0.0\n",
    "    speed_limit = 50\n",
    "    red_light_violations = False\n",
    "    collision_sensor = CollisionSensor(ego)\n",
    "    init_pos = ego.get_transform().location\n",
    "    fin_pos = init_pos\n",
    "\n",
    "    set_cam_to_EGO(ego)\n",
    "    camera.listen(camera_to_image) #changes image_captured globally\n",
    "\n",
    "    while image_captured is None:\n",
    "        world.tick()\n",
    "\n",
    "    while True:\n",
    "        world.wait_for_tick()\n",
    "        if not prev_image_captured == image_captured:\n",
    "\n",
    "            scene_x, scene_y = 800, 600\n",
    "            ego_x_mid = scene_x/2\n",
    "            ego_y_mid = scene_y/2\n",
    "\n",
    "            outputs = model_output_generation(image_captured, driver_model)\n",
    "            waypoint = abs(outputs.detach().numpy())\n",
    "\n",
    "            ego_x_position, ego_y_position = points()\n",
    "\n",
    "            intended_x = waypoint[0]\n",
    "            intended_y = waypoint[1]\n",
    "            \n",
    "            curr_pos = ego.get_transform().location\n",
    "            dx = curr_pos.x - fin_pos.x\n",
    "            dy = curr_pos.y - fin_pos.y\n",
    "            dz = curr_pos.z - fin_pos.z\n",
    "            d = math.sqrt((dx ** 2) + (dy ** 2) + (dz ** 2))\n",
    "            distance = distance + d\n",
    "\n",
    "            evaluate_RC(intended_x, ego_x_position, intended_y, ego_y_position)\n",
    "            evaluate_IS(speed_limit, red_light_violations, collision_sensor)\n",
    "\n",
    "            throttle, brake = TandB(ego_y_mid, intended_y, ego_y_position)\n",
    "            velocity = ego.get_velocity()\n",
    "            speed = math.sqrt((velocity.x**2 + velocity.y**2 + velocity.z**2)) / 5\n",
    "            s_power = (1 - brake) * (1 - speed)\n",
    "            steer = steering(ego_x_position, intended_x, s_power)\n",
    "\n",
    "            print(f\"\\rpredicted: {waypoint}, going towards: ({intended_x:.1f}, {intended_y:.1f}), with b:{brake}, t:{throttle:.1f}, s:{steer:.1f}      \", end = \"\")\n",
    "\n",
    "            ego.apply_control(control_ego(steer, throttle, brake))\n",
    "            set_cam_to_EGO(ego)\n",
    "                \n",
    "        if(time.time() >= start_time + 60):\n",
    "            break\n",
    "        prev_image_captured = image_captured\n",
    "        \n",
    "    if(distance > 0):\n",
    "        RC = rcc * 100 / distance\n",
    "        IS = isc * 100 / distance\n",
    "    evaluate_DS()\n",
    "\n",
    "    print(f\"\\nRC is: {RC:.2f}, DS is: {DS:.2f}, IS is: {IS:.2f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n{e}\")\n",
    "finally:\n",
    "    camera.stop()\n",
    "    \n",
    "    ego.apply_control(control_ego(0,0,1))\n",
    "\n",
    "    remove_all_actors()\n",
    "    initiating_carla(world, blueprint_library)\n",
    "    set_cam_to_EGO(ego, 65)\n",
    "\n",
    "    print(\"stopped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ego.apply_control(control_ego(1,1,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ego.apply_control(control_ego(0,0,1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
