{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CarLA simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import carla\n",
    "import random\n",
    "import time\n",
    "import kagglehub\n",
    "import os\n",
    "import torch\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import SegformerForSemanticSegmentation, SegformerConfig\n",
    "from transformers import AutoImageProcessor, SegformerForSemanticSegmentation\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = carla.Client('localhost', 2000)\n",
    "world = client.get_world()\n",
    "\n",
    "spectator = world.get_spectator()\n",
    "transform = spectator.get_transform()\n",
    "location = transform.location\n",
    "rotation = transform.rotation\n",
    "spectator.set_transform(carla.Transform())\n",
    "\n",
    "# details of NPC, here = vehicles\n",
    "vehicle_blueprints = world.get_blueprint_library().filter('*vehicle*')\n",
    "spawn_points = world.get_map().get_spawn_points()\n",
    "\n",
    "blueprint_library = world.get_blueprint_library()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trying_spawn_location(blue_print):\n",
    "    try:\n",
    "        spawn_loc = world.spawn_actor(blue_print, random.choice(spawn_points))\n",
    "    except Exception as e:\n",
    "        spawn_loc = trying_spawn_location(blue_print)\n",
    "    return spawn_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EGO_relative_view():\n",
    "    view = EGO.get_transform().location + carla.Location(x=0, y=0, z=10)\n",
    "    return view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the spectator view TO our ego\n",
    "def set_cam():\n",
    "    vehicle_transform = EGO.get_transform()\n",
    "    spectator.set_transform(\n",
    "        carla.Transform(\n",
    "            EGO_relative_view(),\n",
    "            carla.Rotation(pitch = -30, yaw = vehicle_transform.rotation.yaw, roll = 0)\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def control_steer(x):\n",
    "    if x > 0:\n",
    "        steer = -0.3\n",
    "    if x <= 0:\n",
    "        steer = 0.3\n",
    "    return steer\n",
    "\n",
    "def get_unstuck(vehicle, rev, steer):\n",
    "    control = carla.VehicleControl()\n",
    "    control.reverse = rev\n",
    "    control.steer = steer\n",
    "    control.throttle = 0.75\n",
    "    return control\n",
    "\n",
    "def on_collision(vehicle):\n",
    "    print(vehicle.id)\n",
    "    direction = carla.VehicleControl().reverse\n",
    "    vehicle.set_autopilot(False)\n",
    "    \n",
    "    steer = control_steer(carla.VehicleControl().steer) * (-1)\n",
    "    vehicle.apply_control(get_unstuck(vehicle, direction, steer))\n",
    "    time.sleep(0.5)\n",
    "    \n",
    "    steer = steer * (-1)\n",
    "    vehicle.apply_control(get_unstuck(vehicle, not direction, steer))\n",
    "    time.sleep(0.75)\n",
    "    vehicle.set_autopilot(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply control to move the vehicle\n",
    "def random_custom_movements(i):\n",
    "    control = carla.VehicleControl()\n",
    "\n",
    "    value = random.uniform(-1.0, 1.0) # -1.0 to 1.0\n",
    "    if(i%500 == 0):\n",
    "        value = random.uniform(-1.0, 1.0)\n",
    "        # set_cam()\n",
    "\n",
    "    control.throttle = 0.75 # 0.0 to 1.0\n",
    "    control.steer = value\n",
    "    control.brake = 0.0 # 0.0 to 1.0\n",
    "    control.hand_brake = False\n",
    "    return control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_lights = world.get_actors().filter('traffic.traffic_light')\n",
    "for traffic_light in traffic_lights:\n",
    "    traffic_light.set_state(carla.TrafficLightState.Green)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all actors in the simulation\n",
    "actors = world.get_actors()\n",
    "\n",
    "# Iterate through all actors and destroy vehicles\n",
    "for actor in actors:\n",
    "    if 'vehicle' in actor.type_id:  # Check if the actor is a vehicle\n",
    "        actor.destroy()\n",
    "world.tick()\n",
    "print(\"previous ones destroyed\")\n",
    "\n",
    "\n",
    "# putting the actors (PCs and NPCs) to motion\n",
    "# for i in range(0,50):\n",
    "#     world.try_spawn_actor(random.choice(vehicle_blueprints), random.choice(spawn_points))\n",
    "    \n",
    "# for vehicle in world.get_actors().filter('*vehicle*'):\n",
    "#     vehicle.set_autopilot(True)\n",
    "#     i = vehicle.get_transform().location\n",
    "\n",
    "# OUR Autonomous Vehicle\n",
    "EGO_bp = vehicle_blueprints.find('vehicle.tesla.model3')\n",
    "EGO_bp.set_attribute('role_name', 'hero')\n",
    "\n",
    "EGO = world.spawn_actor(EGO_bp, random.choice(spawn_points))\n",
    "EGO.set_autopilot(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_cam()\n",
    "on_collision(EGO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles = [actor for actor in world.get_actors() if 'vehicle' in actor.type_id]\n",
    "\n",
    "for vehicle in vehicles:\n",
    "    collision_sensor = world.spawn_actor(\n",
    "        blueprint_library.find('sensor.other.collision'),\n",
    "        carla.Transform(),\n",
    "        attach_to=vehicle\n",
    "    )\n",
    "    collision_sensor.listen(lambda event, vehicle=vehicle: on_collision(vehicle, event))\n",
    "    # collision_sensor.listen(lambda event: on_collision(event.actor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EGO.set_autopilot(True)\n",
    "\n",
    "# while True:\n",
    "#     set_cam()\n",
    "#     time.sleep(1/240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle_transform = EGO.get_transform()\n",
    "\n",
    "spectator.set_transform(\n",
    "    carla.Transform(\n",
    "        EGO_relative_view(),\n",
    "        carla.Rotation(pitch = -30, yaw = vehicle_transform.rotation.yaw, roll = 0)\n",
    "    )\n",
    ")\n",
    "\n",
    "time.sleep(0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the recording function\n",
    "def save_details(image):\n",
    "    image.save_to_disk('dataset/out/%06d.png' % image.frame)\n",
    "\n",
    "    metadata = {\n",
    "        'frame': image.frame,\n",
    "        'timestamp': image.timestamp,\n",
    "        'width': image.width,\n",
    "        'height': image.height,\n",
    "        'camera_transform': camera.get_transform(),\n",
    "        'camera_intrinsics': camera_bp.get_attribute('fov').as_dict()\n",
    "    }\n",
    "\n",
    "    with open(f'dataset/out/{image.frame:06d}_metadata.json', 'w') as f:\n",
    "        json.dump(metadata, f)\n",
    "        \n",
    "    depth_camera_bp = world.get_blueprint_library().find('sensor.camera.depth')\n",
    "    depth_camera = world.spawn_actor(depth_camera_bp, camera_init_trans, attach_to=EGO)\n",
    "\n",
    "    def save_depth_image(depth_image):\n",
    "        depth_image.save_to_disk(f'dataset/out/{depth_image.frame:06d}_depth.png')\n",
    "\n",
    "    depth_camera.listen(save_depth_image)\n",
    "\n",
    "camera_init_trans = carla.Transform(carla.Location(z=1.5))\n",
    "camera_bp = world.get_blueprint_library().find('sensor.camera.rgb')\n",
    "camera = world.spawn_actor(camera_bp, camera_init_trans, attach_to=EGO)\n",
    "\n",
    "camera.listen(save_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blueprint = properties for some configuration\n",
    "collision_sensor_bp = blueprint_library.find('sensor.other.collision')\n",
    "vehicle_bp = random.choice(blueprint_library.filter('vehicle.*.*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the blueprint of the sensor.\n",
    "blueprint = world.get_blueprint_library().find('sensor.camera.rgb')\n",
    "# Modify the attributes of the blueprint to set image resolution and field of view.\n",
    "blueprint.set_attribute('image_size_x', '1920')\n",
    "blueprint.set_attribute('image_size_y', '1080')\n",
    "blueprint.set_attribute('fov', '110')\n",
    "# Set the time in seconds between sensor captures\n",
    "blueprint.set_attribute('sensor_tick', '1.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = carla.Transform(relative_view)\n",
    "sensor = world.spawn_actor(blueprint, transform, attach_to=EGO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sensor.listen(lambda data: do_something(data))\n",
    "\n",
    "# This collision sensor would print everytime a collision is detected. \n",
    "def callback(event):\n",
    "    for actor_id in event:\n",
    "        vehicle = world_ref().get_actor(actor_id)\n",
    "        print('Vehicle too close: %s' % vehicle.type_id)\n",
    "\n",
    "sensor.listen(callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import kagglehub\n",
    "\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import transforms as A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.6)\n",
      "Path to dataset files: C:\\Users\\devas\\.cache\\kagglehub\\datasets\\albertozorzetto\\carla-densely-annotated-driving-dataset\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "path = kagglehub.dataset_download(\"albertozorzetto/carla-densely-annotated-driving-dataset\")\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, image_dir, labels_dir, transform = None):\n",
    "        self.images_dir = images_dir\n",
    "        self.labels_dir = labels_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.label_paths = []\n",
    "\n",
    "        for i in range(0, 28):\n",
    "            k = \"0\" if i<=9 else \"\"\n",
    "            curr_folder_images = os.path.join(images_dir, f\"Video_0{k}{i}\")\n",
    "            curr_folder_labels = os.path.join(labels_dir, f\"Video_0{k}{i}\")\n",
    "\n",
    "            images_path = [file for file in os.listdir(curr_folder_images) if file.endswith(\".png\")]\n",
    "            labels_path = [file for file in os.listdir(curr_folder_labels) if file.endswith(\".png\")]\n",
    "\n",
    "            images_path.sort()\n",
    "            labels_path.sort()\n",
    "\n",
    "            for image_file, label_file in zip(images_path, labels_path):\n",
    "                self.image_paths.append(os.path.join(curr_folder_images, image_file))\n",
    "                self.label_paths.append(os.path.join(curr_folder_labels, label_file))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "        label = Image.open(self.label_paths[idx])\n",
    "\n",
    "        label = np.array(label)\n",
    "        label = torch.tensor(label, dtype = torch.long)\n",
    "\n",
    "        # label = label.permute(1,2,0) if label.ndim == 3 else label.unsqueeze(0)  # Add channel dim if grayscale\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minor preprocessing\n",
    "transform = A.Compose([\n",
    "    A.Resize((224, 224)),\n",
    "    A.ToTensor(),\n",
    "    # A.Normalize(mean = [0.5, 0.5, 0.5], std = [0.5, 0.5, 0.5]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"datasets\"\n",
    "images_dir = f\"{root}/images\"\n",
    "labels_dir = f\"{root}/labels\"\n",
    "batch_size = 4\n",
    "\n",
    "dataset = SegmentationDataset(images_dir, labels_dir, transform = transform)\n",
    "dataloader = DataLoader(dataset, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch of images shape: torch.Size([4, 3, 224, 224])\n",
      "Batch of labels shape: torch.Size([4, 600, 800, 3])\n"
     ]
    }
   ],
   "source": [
    "for images, labels in dataloader:\n",
    "    print(\"Batch of images shape:\", images.shape)\n",
    "    print(\"Batch of labels shape:\", labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now the ViT *using pytorch*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import timm\n",
    "import umap\n",
    "\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision import transforms\n",
    "from transformers import ViTModel, ViTFeatureExtractor, CLIPTextModel, CLIPTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CLIPTextModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 512)\n",
       "      (position_embedding): Embedding(77, 512)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPSdpaAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Load Pretrained Models (ViT for images, CLIP Text Encoder for labels)\n",
    "model = ViTModel.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "CLIP = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "model.eval()\n",
    "CLIP.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "CLIP.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2,3: Data loading and preprocessing\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size = batch_size, shuffle = True)\n",
    "print(len(dataloader))\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "for image_batch, label_batch in dataloader:\n",
    "    for image, label in zip(image_batch, label_batch):\n",
    "        images.append(image)\n",
    "        labels.append(label)\n",
    "    break\n",
    "\n",
    "print(len(images))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,l in zip(images, labels):\n",
    "    i = i.permute(1,2,0)\n",
    "    print(i.shape)\n",
    "    print(l.shape)\n",
    "    plt.figure(figsize = (4,4))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(i)\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(l)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Extract Features for Images\n",
    "image_features = []\n",
    "for image in images:\n",
    "    image = image.unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        features = model(image).last_hidden_state.mean(dim=1)\n",
    "        image_features.append(features)\n",
    "image_features = torch.cat(image_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Extract Features for Semantic Label Classes\n",
    "def preprocess_text(labels):\n",
    "    return tokenizer(labels, padding=True, truncation=True, return_tensors=\"pt\") # one label at a time\n",
    "\n",
    "semantic_label_classes = [\"Traffic Sign\", \"Building\", \"Fence\", \"Other\", \"Pedestrian\", \"Pole\", \"Road Line\", \"Road\", \"Sidewalk\", \"Vegetation\", \"Car\", \"Wall\", \"Unlabeled\"]\n",
    "text_inputs = preprocess_text(semantic_label_classes).to(device)\n",
    "with torch.no_grad():\n",
    "    text_features = CLIP(**text_inputs).last_hidden_state.mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image_features.shape)\n",
    "# <batch_size (here 8)> images processed in ViT and their outputs saved as extracted features\n",
    "\n",
    "print(text_features.shape)\n",
    "# 13 classes, converted to text_features via tokenization THEN powers of NLP and VLM pretrained clip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Align Features in Shared Space\n",
    "projection_layer = nn.Linear(512, 768)\n",
    "text_features_projected = projection_layer(text_features)\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    a = nn.functional.normalize(a, dim=-1)\n",
    "    b = nn.functional.normalize(b, dim=-1)\n",
    "    return torch.matmul(a, b.T)\n",
    "\n",
    "similarity_matrix = cosine_similarity(image_features, text_features_projected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Training*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-5, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_to_train = DataLoader(dataset, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2692"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTModel(\n",
       "  (embeddings): ViTEmbeddings(\n",
       "    (patch_embeddings): ViTPatchEmbeddings(\n",
       "      (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (encoder): ViTEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x ViTLayer(\n",
       "        (attention): ViTSdpaAttention(\n",
       "          (attention): ViTSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): ViTSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): ViTIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): ViTOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (pooler): ViTPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 56%"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "    \n",
    "    last_percent_done = -1\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(dataloader_to_train, 0):\n",
    "        inputs, processings = data\n",
    "\n",
    "        processings = processings.float()\n",
    "        processings = F.interpolate(processings, size=(batch_size, 768))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs).last_hidden_state\n",
    "        processings = processings.mean(dim = (1,2)).long()\n",
    "        loss = criterion(outputs, processings)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        percent_done = int((i / 2692) * 100)\n",
    "        if percent_done > last_percent_done:\n",
    "            print(f\"\\rProgress: {percent_done}%\", end=\"\")\n",
    "            last_percent_done = percent_done\n",
    "\n",
    "    print(f\"Epoch {epoch+1} completed. Running loss: {running_loss:.3f}\")\n",
    "    break\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './cifar_net.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "    \n",
    "    last_percent_done = -1\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(dataloader_to_train, 0):\n",
    "        inputs, processings = data\n",
    "\n",
    "        processings = processings.float()\n",
    "        processings = F.interpolate(processings, size=(batch_size, 768))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs).last_hidden_state\n",
    "        processings = processings.mean(dim = (1,2)).long()\n",
    "        loss = criterion(outputs, processings)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        percent_done = int((i / 2692) * 100)\n",
    "        if percent_done > last_percent_done:\n",
    "            print(f\"\\rProgress: {percent_done}%\", end=\"\")\n",
    "            last_percent_done = percent_done\n",
    "\n",
    "    print(f\"Epoch {epoch+1} completed. Running loss: {running_loss:.3f}\")\n",
    "    \n",
    "    PATH = './cifar_net.pth'\n",
    "    torch.save(net.state_dict(), PATH)\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Prediction*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Map Similarities Back to Segmentation Maps\n",
    "label_segmentation_maps = labels\n",
    "predicted_segmentations = []\n",
    "for idx, sim_scores in enumerate(similarity_matrix):\n",
    "    label_segmentation_map = label_segmentation_maps[idx]\n",
    "    label_segmentation_map = label_segmentation_map.argmax(dim=-1)\n",
    "    predicted_map = torch.zeros_like(label_segmentation_map, dtype=torch.float32)\n",
    "    for class_idx, label in enumerate(semantic_label_classes):\n",
    "        predicted_map[label_segmentation_map == class_idx] = sim_scores[class_idx]\n",
    "        # print(torch.unique(predicted_map))\n",
    "    # break\n",
    "    predicted_segmentations.append(predicted_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predicted_segmentations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Evaluate or Visualize Results\n",
    "print(\"Predicted Segmentation Maps:\", predicted_segmentations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_to_range(tensor, a, b):\n",
    "    tensor_min = float(tensor.min())\n",
    "    tensor_max = float(tensor.max())\n",
    "    \n",
    "    normalized_tensor = (tensor - tensor_min) / (tensor_max - tensor_min)    \n",
    "    scaled_tensor = normalized_tensor * (b - a) + a\n",
    "    \n",
    "    return scaled_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES = images\n",
    "LABEL_SEGMENTATION_MAPS = label_segmentation_maps\n",
    "PREDICTED_SEGMENTATIONS = predicted_segmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (image, label_segmentation_map, predicted_map) in enumerate(zip(IMAGES, LABEL_SEGMENTATION_MAPS, PREDICTED_SEGMENTATIONS)):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(image.permute(1, 2, 0).numpy().clip(0, 1), cmap = \"jet\")\n",
    "    plt.title(\"Original Image\")\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(label_segmentation_map.numpy(), cmap = \"jet\")\n",
    "    plt.title(\"Ground Truth Segmentation\")\n",
    "    print(f\"SegMp: {torch.unique(label_segmentation_map)}\")\n",
    "\n",
    "    # predicted_map[predicted_map < 0] = 0\n",
    "    print(f\"PM (before): {torch.unique(predicted_map)}\")\n",
    "    print(predicted_map.shape)\n",
    "    predicted_map = normalize_to_range(predicted_map, 0, 255).byte()\n",
    "    print(f\"PM (after): {torch.unique(predicted_map)}\")\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(predicted_map.detach().numpy(), cmap = \"jet\")\n",
    "    plt.title(\"Predicted Segmentation\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
