{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ALGORITHM 1: VL pretrained-M fine tuned with semi-disentangled outputs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import torch\n",
    "import os\n",
    "import kagglehub\n",
    "import pickle\n",
    "import math\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "from sklearn.decomposition import PCA\n",
    "from skimage.morphology import skeletonize\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import CLIPProcessor, CLIPModel, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from torchvision import transforms as A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = kagglehub.dataset_download(\"albertozorzetto/carla-densely-annotated-driving-dataset\")\n",
    "# print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Creation (IMAGE form to EMBEDDING form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Download the CARLA dataset using the Kaggle API\n",
    "# dataset_name = \"albertozorzetto/carla-densely-annotated-driving-dataset\"\n",
    "# destination_path = \"/content/datasets\"\n",
    "\n",
    "# import os\n",
    "# # Install Kaggle API\n",
    "# !pip install kaggle --upgrade\n",
    "# # Upload kaggle.json for authentication\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "# # Move kaggle.json to the proper directory\n",
    "# !mkdir -p ~/.kaggle\n",
    "# !mv kaggle.json ~/.kaggle/\n",
    "# !chmod 600 ~/.kaggle/kaggle.json\n",
    "# # Verify Kaggle is set up correctly\n",
    "# !kaggle datasets list\n",
    "# !kaggle datasets download -d {dataset_name} -p {destination_path} --unzip\n",
    "# print(\"Path to dataset files:\", destination_path)\n",
    "\n",
    "# import tarfile\n",
    "# images_path = os.path.join(destination_path, \"images\")\n",
    "# labels_path = os.path.join(destination_path, \"labels\")\n",
    "# # Create directories for extracted files\n",
    "# os.makedirs(images_path, exist_ok=True)\n",
    "# os.makedirs(labels_path, exist_ok=True)\n",
    "# # Extract and manage .tar files\n",
    "# tar_files = [f for f in os.listdir(destination_path) if f.endswith('.tar')]\n",
    "# for tar_file in tar_files:\n",
    "#     tar_path = os.path.join(destination_path, tar_file)\n",
    "#     print(f\"Extracting {tar_path}...\")\n",
    "#     with tarfile.open(tar_path) as tar:\n",
    "#         # Determine the folder to extract based on the tar file name\n",
    "#         if \"images\" in tar_file.lower():\n",
    "#             tar.extractall(path=images_path)\n",
    "#         elif \"labels\" in tar_file.lower():\n",
    "#             tar.extractall(path=labels_path)\n",
    "#         else:\n",
    "#             print(f\"Skipping unknown .tar file: {tar_file}\")\n",
    "#     os.remove(tar_path)  # Optional: Remove the .tar file after extraction\n",
    "# print(\"All .tar files have been extracted.\")\n",
    "# print(f\"Images folder: {images_path}\")\n",
    "# print(f\"Labels folder: {labels_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "semantic_label_classes = [\"Traffic Sign\", \"Building\", \"Fence\", \"Other\", \"Pedestrian\", \"Pole\", \"Road Line\", \"Road\", \"Sidewalk\", \"Vegetation\", \"Car\", \"Wall\", \"Unlabeled\"]\n",
    "\n",
    "root = \"datasets\"\n",
    "images_dir = f\"{root}/images\"\n",
    "labels_dir = f\"{root}/labels\"\n",
    "waypoint_file = \"dataset_waypoints.json\"\n",
    "batch_size = 2\n",
    "\n",
    "skip = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Saving the {way points} for passing to the LLM*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_waypoints(label, num_waypoints):\n",
    "    if len(label.shape) == 3:\n",
    "        label = cv2.cvtColor(label, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    road_mask = (label == 90).astype(np.uint8)\n",
    "    \n",
    "    contours, _ = cv2.findContours(road_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if not contours:\n",
    "        return []\n",
    "    \n",
    "    largest_contour = max(contours, key=cv2.contourArea)\n",
    "    binary_image = (road_mask > 0).astype(np.uint8)\n",
    "    skeleton = skeletonize(binary_image).astype(np.uint8)\n",
    "    \n",
    "    y_coords, x_coords = np.where(skeleton > 0)\n",
    "    path_points = np.column_stack((x_coords, y_coords))\n",
    "    \n",
    "    x_min, x_max = path_points[:, 0].min(), path_points[:, 0].max()\n",
    "    y_min, y_max = path_points[:, 1].min(), path_points[:, 1].max()\n",
    "\n",
    "    x_range = (x_max - x_min) * 0.25\n",
    "    y_range = (y_max - y_min) * 0.25\n",
    "\n",
    "    path_points = path_points[\n",
    "        (path_points[:, 0] > x_min + x_range) & (path_points[:, 0] < x_max - x_range) &\n",
    "        (path_points[:, 1] > y_min + y_range) & (path_points[:, 1] < y_max - y_range)\n",
    "    ]\n",
    "    \n",
    "    if len(path_points) == 0:\n",
    "        return []\n",
    "\n",
    "\n",
    "    sampled_indices = np.linspace(0, len(path_points) - 1, num_waypoints, dtype=int)\n",
    "    waypoints = path_points[sampled_indices]\n",
    "    \n",
    "    # print(\"Min/Max path points:\", path_points.min(), path_points.max())\n",
    "\n",
    "    # plt.imshow(skeleton, cmap=\"gray\")\n",
    "    # plt.scatter(x_coords, y_coords, c=\"red\", marker=\"o\")\n",
    "    # plt.show()\n",
    "\n",
    "    # print(waypoints)\n",
    "    # print(\"x\", waypoints[:,0])\n",
    "    # print(\"y\", waypoints[:,1])\n",
    "\n",
    "    # num_selected_waypoints = min(25, len(path_points))\n",
    "    # sampled_indices = np.linspace(0, len(path_points) - 1, num_selected_waypoints, dtype=int)\n",
    "    # waypoints = path_points[sampled_indices]\n",
    "\n",
    "    # if len(waypoints) > 0:\n",
    "    #     plt.figure(figsize=(10, 10))\n",
    "    #     plt.imshow(label, cmap=\"gray\")\n",
    "    #     plt.scatter(waypoints[:, 0], waypoints[:, 1], c='red', marker='o')\n",
    "    #     for i, (x, y) in enumerate(waypoints):\n",
    "    #         plt.text(x, y, f'({x:.1f}, {y:.1f})', fontsize=8, color='yellow', ha='right', va='bottom')\n",
    "    #     plt.title(\"Extracted Waypoints\")\n",
    "    #     plt.show()\n",
    "\n",
    "    return waypoints.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(labels_dir, output_dir, num_waypoints):\n",
    "    # os.makedirs(output_dir, exist_ok=True)\n",
    "    dataset_waypoints = []\n",
    "\n",
    "    for i in range(0, 28):\n",
    "        k = \"0\" if i <= 9 else \"\"\n",
    "        curr_folder_labels = f\"{labels_dir}/Video_0{k}{i}\"\n",
    "        labels_path = [file for file in os.listdir(curr_folder_labels) if file.endswith(\".png\")]\n",
    "        labels_path.sort()\n",
    "        labels_path = labels_path[::skip]\n",
    "\n",
    "        for label_file in labels_path:\n",
    "            label_path = os.path.join(curr_folder_labels, label_file)\n",
    "            label = Image.open(label_path)\n",
    "            label_np = np.array(label)\n",
    "            waypoints = generate_waypoints(label_np, num_waypoints=num_waypoints)\n",
    "            output_label_path = os.path.join(output_dir, label_file)\n",
    "            dataset_waypoints.append({\n",
    "                \"label\": output_label_path,\n",
    "                \"waypoints\": waypoints\n",
    "            })\n",
    "            # break\n",
    "\n",
    "        with open(os.path.join(output_dir, waypoint_file), 'a') as wf:\n",
    "            json.dump(dataset_waypoints, wf)\n",
    "            wf.write(\"\\n\")\n",
    "            # break\n",
    "\n",
    "        if(i<27):\n",
    "            print(f\"\\rProgress: {((i + 1) / 0.27):.2f} %\", end = \"\")\n",
    "        # break\n",
    "\n",
    "    print(f\"\\nDone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_dataset(labels_dir, root, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, image_dir, labels_dir):\n",
    "        self.data = []\n",
    "        ei = []\n",
    "        cw = []\n",
    "\n",
    "        for i in range(0, 28):\n",
    "            k = \"0\" if i <= 9 else \"\"\n",
    "            curr_folder_images = f\"{images_dir}/Video_0{k}{i}\"\n",
    "            images_path = [file for file in os.listdir(curr_folder_images) if file.endswith(\".png\")]\n",
    "            images_path.sort()\n",
    "            images_path = images_path[::skip]\n",
    "\n",
    "            for image_file in images_path:\n",
    "                image_path = os.path.join(curr_folder_images, image_file)\n",
    "                image = Image.open(image_path)\n",
    "                image_embeddings = processor(text=semantic_label_classes, images=image, return_tensors=\"pt\", padding=True)\n",
    "                ei.append(image_embeddings['pixel_values'])\n",
    "\n",
    "            with open(os.path.join(root, waypoint_file), \"r\") as wf:\n",
    "                content = wf.read()\n",
    "                data_list = content.splitlines()\n",
    "                for data in data_list:\n",
    "                    data = json.loads(data)\n",
    "                    for i in data:\n",
    "                        corresponding_waypoints = i['waypoints']\n",
    "                        cw.append(corresponding_waypoints)\n",
    "                        # print(corresponding_waypoints)\n",
    "                        # break\n",
    "                    # break\n",
    "            for e,c in zip(ei, cw):\n",
    "                self.data.append((e,c))\n",
    "                # break\n",
    "            # break\n",
    "        # print(self.data[0][1])\n",
    "        # print(self.data[0][0])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        embedding_inputs, embedding_labels = self.data[idx]\n",
    "        return embedding_inputs, embedding_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split indices for train and validation\n",
    "dataset = SegmentationDataset(images_dir, labels_dir)\n",
    "dataloader = DataLoader(dataset, batch_size = batch_size, shuffle = True, pin_memory=True)\n",
    "\n",
    "train_indices, val_indices = train_test_split(range(len(dataset)), test_size=0.2, random_state=42)\n",
    "train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "\n",
    "val_dataset = torch.utils.data.Subset(dataset, val_indices)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(pixel_values):\n",
    "    mini = pixel_values.min()\n",
    "    maxi = pixel_values.max()\n",
    "    pixel_values = (pixel_values - mini) / (maxi - mini)\n",
    "    return pixel_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OUR MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaypointModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WaypointModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(512)\n",
    "        self.conv3 = nn.Conv2d(512, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "        self.conv5 = nn.Conv2d(512, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 512)\n",
    "        self.fc2 = nn.Linear(512, 50)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        x = torch.relu(self.conv1(pixel_values))\n",
    "        x = self.bn1(x)\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        \n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.bn2(x)\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = self.bn3(x)\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "\n",
    "        x = torch.relu(self.conv4(x))\n",
    "        x = self.bn4(x)\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "\n",
    "        x = torch.relu(self.conv5(x))\n",
    "        x = self.bn5(x)\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        output = self.fc2(x)\n",
    "        return output.view(-1, 25, 2).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_float(val):\n",
    "    return val.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_output_generation(pixel_values, vit_wp_model):\n",
    "    pixel_values = pixel_values.squeeze(1)\n",
    "    outputs = vit_wp_model(pixel_values)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*--training--*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(f\"Using device: {device}\")\n",
    "# vit_wp_model = vit_wp_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_wp_model = WaypointModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(vit_wp_model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(outputs, targets):\n",
    "  threshold = targets.mean() * 0.01\n",
    "  # print(\"outputs:\", outputs)\n",
    "  # print(\"targets:\", targets)\n",
    "  MSE = torch.sqrt(torch.sum((outputs - targets) ** 2, dim=-1))\n",
    "  accurate_predictions = (MSE <= threshold).float()\n",
    "  accuracy = accurate_predictions.mean().item()\n",
    "  # print(\"MSE:\", MSE)\n",
    "  # print(\"acc preds:\", accurate_predictions)\n",
    "  # print(\"acc:\", accuracy)\n",
    "  return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wayp_to_targets(way_p):\n",
    "    # print(way_p)\n",
    "    return to_float(torch.stack([torch.stack(pair, dim = 0) for pair in way_p]).permute(2,0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(vit_wp_model, dataloader, optimizer, criterion):\n",
    "    vit_wp_model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_accuracy = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (p_val, way_p) in enumerate(dataloader):\n",
    "            print(\"way_p len\", len(way_p))\n",
    "            print(\"wayp:\", way_p)\n",
    "            target = wayp_to_targets(way_p)[0]\n",
    "            target = target.view(25, 2)\n",
    "            optimizer.zero_grad()\n",
    "            print(\"target shape\", target.shape)\n",
    "            print(\"target:\", target)\n",
    "\n",
    "            \n",
    "\n",
    "            outputs = model_output_generation(p_val[0], vit_wp_model)\n",
    "            print(\"output shape\", outputs.shape)\n",
    "            loss = criterion(outputs, target)\n",
    "            running_loss += loss.item()\n",
    "            print(way_p)\n",
    "            print(outputs)\n",
    "            # break\n",
    "            \n",
    "    return running_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "way_p len 10\n",
      "wayp: [[tensor([384, 403]), tensor([304, 298])], [tensor([357, 262]), tensor([322, 320])], [tensor([277, 242]), tensor([333, 332])], [tensor([328, 220]), tensor([342, 344])], [tensor([577,  67]), tensor([353, 354])], [tensor([257, 127]), tensor([368, 365])], [tensor([94, 85]), tensor([398, 381])], [tensor([269,  18]), tensor([444, 417])], [tensor([583, 118]), tensor([502, 469])], [tensor([163, 136]), tensor([599, 598])]]\n",
      "target shape torch.Size([10, 2])\n",
      "target: tensor([[384., 304.],\n",
      "        [357., 322.],\n",
      "        [277., 333.],\n",
      "        [328., 342.],\n",
      "        [577., 353.],\n",
      "        [257., 368.],\n",
      "        [ 94., 398.],\n",
      "        [269., 444.],\n",
      "        [583., 502.],\n",
      "        [163., 599.]])\n",
      "output shape torch.Size([10, 2])\n",
      "[[tensor([384, 403]), tensor([304, 298])], [tensor([357, 262]), tensor([322, 320])], [tensor([277, 242]), tensor([333, 332])], [tensor([328, 220]), tensor([342, 344])], [tensor([577,  67]), tensor([353, 354])], [tensor([257, 127]), tensor([368, 365])], [tensor([94, 85]), tensor([398, 381])], [tensor([269,  18]), tensor([444, 417])], [tensor([583, 118]), tensor([502, 469])], [tensor([163, 136]), tensor([599, 598])]]\n",
      "tensor([[307.8578, 161.2459],\n",
      "        [277.8594, 234.5086],\n",
      "        [242.8574, 234.4698],\n",
      "        [ 63.6790, 250.0610],\n",
      "        [253.9949, 265.7086],\n",
      "        [388.2668, 261.0059],\n",
      "        [317.5659, 290.6924],\n",
      "        [410.9444, 321.2579],\n",
      "        [365.6212, 353.5509],\n",
      "        [212.2683, 312.6209]])\n",
      "test loss is: 30.75406785608747\n"
     ]
    }
   ],
   "source": [
    "test_loss = test(vit_wp_model, dataloader, optimizer, criterion)\n",
    "print(f\"test loss is: {test_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "way_p len 10\n",
    "wayp: [[tensor([451, 451]), tensor([316, 316])], [tensor([261, 261]), tensor([339, 339])], [tensor([208, 208]), tensor([352, 352])], [tensor([173, 173]), tensor([366, 366])], [tensor([107, 107]), tensor([382, 382])], [tensor([38, 38]), tensor([401, 401])], [tensor([169, 169]), tensor([459, 459])], [tensor([47, 47]), tensor([508, 508])], [tensor([648, 648]), tensor([586, 586])], [tensor([31, 31]), tensor([592, 592])]]\n",
    "target shape torch.Size([10, 2])\n",
    "target: tensor([[451., 316.],\n",
    "        [261., 339.],\n",
    "        [208., 352.],\n",
    "        [173., 366.],\n",
    "        [107., 382.],\n",
    "        [ 38., 401.],\n",
    "        [169., 459.],\n",
    "        [ 47., 508.],\n",
    "        [648., 586.],\n",
    "        [ 31., 592.]])\n",
    "output shape torch.Size([10, 2])\n",
    "[[tensor([451, 451]), tensor([316, 316])], [tensor([261, 261]), tensor([339, 339])], [tensor([208, 208]), tensor([352, 352])], [tensor([173, 173]), tensor([366, 366])], [tensor([107, 107]), tensor([382, 382])], [tensor([38, 38]), tensor([401, 401])], [tensor([169, 169]), tensor([459, 459])], [tensor([47, 47]), tensor([508, 508])], [tensor([648, 648]), tensor([586, 586])], [tensor([31, 31]), tensor([592, 592])]]\n",
    "tensor([[-0.0217,  0.0139],\n",
    "        [ 0.0400,  0.0036],\n",
    "        [ 0.0327,  0.0203],\n",
    "        [ 0.0192, -0.0134],\n",
    "        [ 0.0304,  0.0422],\n",
    "        [ 0.0328,  0.0433],\n",
    "        [ 0.0071,  0.0051],\n",
    "        [-0.0380, -0.0139],\n",
    "        [-0.0003, -0.0257],\n",
    "        [-0.0057, -0.0179]])\n",
    "test loss is: 162.59809027777777"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(vit_wp_model, dataloader, optimizer, criterion):\n",
    "    vit_wp_model.train()\n",
    "    running_loss = 0.0\n",
    "    running_accuracy = 0.0\n",
    "\n",
    "    for i, (p_val, way_p) in enumerate(dataloader):\n",
    "        target = wayp_to_targets(way_p)[0]\n",
    "        target = target.view(25, 2)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model_output_generation(p_val[0], vit_wp_model)\n",
    "        loss = criterion(outputs, target)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        accuracy = compute_accuracy(outputs, target)\n",
    "        running_accuracy += accuracy\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # break\n",
    "\n",
    "    return running_loss / len(dataloader), running_accuracy / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(vit_wp_model, dataloader, optimizer, criterion):\n",
    "    vit_wp_model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_accuracy = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (p_val, way_p) in enumerate(dataloader):\n",
    "            target = wayp_to_targets(way_p)[0]\n",
    "            target = target.view(25, 2)\n",
    "\n",
    "            outputs = model_output_generation(p_val[0], vit_wp_model)\n",
    "            loss = criterion(outputs, target)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            accuracy = compute_accuracy(outputs, target)\n",
    "            running_accuracy += accuracy\n",
    "            # break\n",
    "\n",
    "    return running_loss / len(dataloader), running_accuracy / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_wp_model = WaypointModel()\n",
    "criterion = nn.MSELoss()\n",
    "num_epochs = 5\n",
    "lr = 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhancing Learning rate at: 0.1\n",
      "Epoch 5/5, train-Loss: 71731.1, val-Loss: 317790.9     4398.5     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_vit_wp_model = None\n",
    "for i in range(0,1):\n",
    "  optimizer = optim.Adam(vit_wp_model.parameters(), lr=lr)\n",
    "  best_train_loss = float('inf')\n",
    "  best_val_loss = float('inf')\n",
    "  # patience = 5\n",
    "  print(f\"Setting Learning rate to: {lr}\")\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "    lr = lr / 10\n",
    "    best_vit_wp_model = vit_wp_model\n",
    "    # if(patience == 0):\n",
    "    #   break\n",
    "    \n",
    "    train_loss, train_accuracy = training(vit_wp_model, train_loader, optimizer, criterion)\n",
    "    val_loss, val_accuracy = validation(vit_wp_model, val_loader, optimizer, criterion)\n",
    "    \n",
    "    if(train_loss > (best_train_loss * 0.95) or (val_loss > (best_val_loss * 0.95))):\n",
    "      vit_wp_model = best_vit_wp_model\n",
    "      # patience -= 1\n",
    "    else:\n",
    "      best_train_loss = train_loss\n",
    "      best_val_loss = val_loss\n",
    "      best_vit_wp_model = vit_wp_model\n",
    "      # patience = 5\n",
    "      # torch.save(vit_wp_model.state_dict(), '/content/drive/MyDrive/my_model.pth')\n",
    "    print(f\"\\rEpoch {epoch + 1}/{num_epochs}, train-Loss: {train_loss:.1f}, val-Loss: {val_loss:.1f}     \", end = \"\")#, patience: {patience}\", end = \"\")\n",
    "    # break\n",
    "  print(\"\\n\")\n",
    "  # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_model = best_vit_wp_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_vit_wp_model.state_dict(), 'best_vit_wp_model_updated.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CarLA simulation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "redundent code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import torch\n",
    "import os\n",
    "import kagglehub\n",
    "import pickle\n",
    "import math\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "from sklearn.decomposition import PCA\n",
    "from skimage.morphology import skeletonize\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import CLIPProcessor, CLIPModel, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torchvision import transforms as A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prev one\n",
    "class WaypointModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WaypointModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(512)\n",
    "        self.conv3 = nn.Conv2d(512, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "        self.conv5 = nn.Conv2d(512, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 512)\n",
    "        self.fc2 = nn.Linear(512, 40)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        x = torch.relu(self.conv1(pixel_values))\n",
    "        x = self.bn1(x)\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        \n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.bn2(x)\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = self.bn3(x)\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "\n",
    "        x = torch.relu(self.conv4(x))\n",
    "        x = self.bn4(x)\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "\n",
    "        x = torch.relu(self.conv5(x))\n",
    "        x = self.bn5(x)\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        output = self.fc2(x)\n",
    "        return output.view(-1, 10, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_float(val):\n",
    "    return val.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_output_generation(pixel_values, vit_wp_model):\n",
    "    pixel_values = pixel_values.squeeze(1)\n",
    "    outputs = vit_wp_model(pixel_values)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import carla\n",
    "import random\n",
    "import time\n",
    "import kagglehub\n",
    "import os\n",
    "import torch\n",
    "import requests\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import SegformerForSemanticSegmentation, SegformerConfig\n",
    "from transformers import AutoImageProcessor, SegformerForSemanticSegmentation\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\devas\\AppData\\Local\\Temp\\ipykernel_2060\\334242544.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  driver_model.load_state_dict(torch.load('best_model.pth', map_location=torch.device('cpu')))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver_model = WaypointModel()\n",
    "driver_model.load_state_dict(torch.load('best_vit_wp_model.pth', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "semantic_label_classes = [\"Traffic Sign\", \"Building\", \"Fence\", \"Other\", \"Pedestrian\", \"Pole\", \"Road Line\", \"Road\", \"Sidewalk\", \"Vegetation\", \"Car\", \"Wall\", \"Unlabeled\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = carla.Client('localhost', 2000)\n",
    "world = client.get_world()\n",
    "blueprint_library = world.get_blueprint_library()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "netstat -an | findstr 2000\n",
    "ping localhost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_all_actors():\n",
    "    actors = world.get_actors()\n",
    "    for actor in actors:\n",
    "        if 'vehicle' in actor.type_id:\n",
    "            actor.destroy()\n",
    "    world.tick()\n",
    "    print(\"previous ones destroyed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "previous ones destroyed\n"
     ]
    }
   ],
   "source": [
    "remove_all_actors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "ego_blueprint = blueprint_library.filter('*model3')[0]\n",
    "spawn_points = world.get_map().get_spawn_points()[0]\n",
    "ego = world.spawn_actor(ego_blueprint, spawn_points)\n",
    "\n",
    "camera_bp = blueprint_library.find('sensor.camera.rgb')\n",
    "camera_bp.set_attribute('image_size_x', '1920')\n",
    "camera_bp.set_attribute('image_size_y', '1080')\n",
    "camera_bp.set_attribute('fov', '110')\n",
    "camera_transform = carla.Transform(carla.Location(x = 1.5, z = 2.4))\n",
    "camera = world.spawn_actor(camera_bp, camera_transform, attach_to=ego)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EGO_relative_view(EGO):\n",
    "    view = EGO.get_transform().location + carla.Location(x=0, y=0, z=10)\n",
    "    return view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_cam_to_EGO(EGO, angle = 65):\n",
    "    spectator = world.get_spectator()\n",
    "    vehicle_transform = EGO.get_transform()\n",
    "    spectator.set_transform(\n",
    "        carla.Transform(\n",
    "            EGO_relative_view(EGO),\n",
    "            carla.Rotation(pitch = -angle, yaw = vehicle_transform.rotation.yaw, roll = 0)\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*functions to control*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def steering(x, intended_x, s_power):\n",
    "    towards = intended_x - x\n",
    "    steer = towards * 0.0002\n",
    "    steer = steer * (1 + s_power) / 1.5\n",
    "    steer = max(-1, min(steer, 1))\n",
    "    return steer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TandB(y_mid, intended_y_mid, curr_y = 25):\n",
    "    if(intended_y_mid <= curr_y):\n",
    "        return 0, 1\n",
    "    intended_y_mid = intended_y_mid if intended_y_mid < y_mid else y_mid\n",
    "    return max(0.3, min(intended_y_mid / (2 * y_mid), 1)), 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(image):\n",
    "    array = np.frombuffer(image.raw_data, dtype=np.uint8)\n",
    "    array = array.reshape((image.height, image.width, 4))\n",
    "    return Image.fromarray(array[:, :, :3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_emb(image):\n",
    "    global emb, image_1\n",
    "    image_1 = image\n",
    "    image = process_image(image)\n",
    "    emb = processor(text=semantic_label_classes, images=image, return_tensors=\"pt\", padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_wp(way_p_1, top_w,top_h):\n",
    "    way_p = way_p_1\n",
    "\n",
    "    mini_f = float('inf')\n",
    "    maxi_f = -float('inf')\n",
    "    for i in way_p[0]:\n",
    "        if(maxi_f < i[0]):\n",
    "            maxi_f = i[0]\n",
    "        if(mini_f > i[0]):\n",
    "            mini_f = i[0]\n",
    "        \n",
    "    mini_s = float('inf')\n",
    "    maxi_s = -float('inf')\n",
    "    for i in way_p[1]:\n",
    "        if(maxi_s < i[1]):\n",
    "            maxi_s = i[1]\n",
    "        if(mini_s > i[1]):\n",
    "            mini_s = i[1]\n",
    "        \n",
    "    for i in way_p:\n",
    "        i[0] = (i[0] - mini_f) / (maxi_f - mini_f)\n",
    "        i[1] = (i[1] - mini_s) / (maxi_s - mini_s)\n",
    "        i[0] *= top_h\n",
    "        i[1] *= top_w\n",
    "\n",
    "    return way_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_cam_to_EGO(ego)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculation of metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "RC = 0\n",
    "IS = 0\n",
    "DS = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*--route completion--*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_RC(intended_x_mid, ego_x_position, intended_y_mid, ego_y_position):\n",
    "    global rcc\n",
    "    if intended_y_mid > ego_y_position:\n",
    "        dist = math.sqrt(((intended_x_mid - ego_x_position) ** 2) + ((intended_y_mid - ego_y_position) ** 2))\n",
    "        if dist <= 900:\n",
    "            rcc = rcc + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*--infraction score--*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollisionSensor(object):\n",
    "    def __init__(self, vehicle):\n",
    "        self.vehicle = vehicle\n",
    "        self.world = vehicle.get_world()\n",
    "        self.collision_history = []\n",
    "\n",
    "        blueprint = self.world.get_blueprint_library().find('sensor.other.collision')\n",
    "        self.sensor = self.world.spawn_actor(blueprint, carla.Transform(), attach_to=self.vehicle)\n",
    "        self.sensor.listen(self._on_collision)\n",
    "\n",
    "    def _on_collision(self, event):\n",
    "        self.collision_history.append(event)\n",
    "        \n",
    "    def get_collision_count(self):\n",
    "        return len(self.collision_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_red_light_violation():\n",
    "    global red_light_violations\n",
    "    ego_position = ego.get_location()\n",
    "    traffic_lights = world.get_traffic_lights()\n",
    "    red_light_violation = False\n",
    "    for traffic_light in traffic_lights:\n",
    "        light_position = traffic_light.get_location()\n",
    "        distance_to_light = ego_position.distance(light_position)\n",
    "        if distance_to_light < 20.0:\n",
    "            if traffic_light.get_state() == carla.TrafficLightState.Red:\n",
    "                red_light_violation = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_IS(speed_limit, red_light_violations, collision_sensor):\n",
    "    global isc\n",
    "\n",
    "    velocity = ego.get_velocity()\n",
    "    speed = (velocity.x**2 + velocity.y**2 + velocity.z**2) ** 0.5 * 3.6\n",
    "    \n",
    "    if speed > speed_limit:\n",
    "        isc -= 1\n",
    "        \n",
    "    if red_light_violations:\n",
    "        isc -= 1\n",
    "\n",
    "    if collision_sensor.get_collision_count() > 0:\n",
    "        isc -= 10\n",
    "\n",
    "    if isc < 0:\n",
    "        isc = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*--driving score--*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_DS():\n",
    "    global RC\n",
    "    global IS\n",
    "    global dsc\n",
    "    dsc = (RC + IS) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*controlling based on vit_wp_model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started\n",
      "currently at: (960.0, 25.0), mid: (960.0, 540.0), going towards: (402.5, 570.6), with b:0, t:0.6, s:-0.3 and ISc:100, RCc: 35        \n",
      "RC is: 5.30, IS is: 15.13, DS is: 0.00, \n"
     ]
    }
   ],
   "source": [
    "print(\"started\")\n",
    "\n",
    "emb = None\n",
    "prev_emb = None\n",
    "image_1 = None\n",
    "\n",
    "RC = 0\n",
    "IS = 0\n",
    "DS = 0\n",
    "rcc = 0\n",
    "isc = 100\n",
    "\n",
    "start_time = time.time()\n",
    "pe = {}\n",
    "e = {}\n",
    "idx = 0\n",
    "\n",
    "speed_limit = 50\n",
    "red_light_violations = False\n",
    "\n",
    "set_cam_to_EGO(ego)\n",
    "camera.listen(image_to_emb) #changes emb, image_1 globally\n",
    "\n",
    "collision_sensor = CollisionSensor(ego)\n",
    "\n",
    "init_pos = ego.get_transform().location\n",
    "distance = 0.0\n",
    "fin_pos = init_pos\n",
    "\n",
    "while emb is None:\n",
    "    world.tick()\n",
    "\n",
    "if prev_emb:\n",
    "    for i in prev_emb.keys():\n",
    "        pe[i] = prev_emb[i]\n",
    "for i in emb.keys():\n",
    "    e[i] = emb[i]\n",
    "\n",
    "while True:\n",
    "    world.wait_for_tick()\n",
    "    e = emb['pixel_values']\n",
    "    # print(f\"unique emb['pixel_values]: {e}\")\n",
    "    if prev_emb:\n",
    "        pe = prev_emb['pixel_values']\n",
    "        # print(f\"unique prev_emb['pixel_values]: {pe}\")\n",
    "        if not torch.equal(e,pe):\n",
    "            idx = idx + 1\n",
    "\n",
    "            outputs = model_output_generation(e, driver_model)\n",
    "            waypoints = outputs.detach().numpy()\n",
    "\n",
    "            scene_h, scene_w = process_image(image_1).size\n",
    "            ego_x_mid = scene_h/2\n",
    "            ego_y_mid = scene_w/2\n",
    "\n",
    "            ego_x_position = ego_x_mid\n",
    "            ego_y_position = 25\n",
    "\n",
    "            waypoints = normalize_wp(waypoints, scene_w, scene_h)\n",
    "\n",
    "            intended_y_mid = np.mean([point[0] for point in waypoints])\n",
    "            intended_x_mid = np.mean([point[1] for point in waypoints])\n",
    "\n",
    "            curr_pos = ego.get_transform().location\n",
    "            dx = curr_pos.x - fin_pos.x\n",
    "            dy = curr_pos.y - fin_pos.y\n",
    "            dz = curr_pos.z - fin_pos.z\n",
    "            d = math.sqrt((dx ** 2) + (dy ** 2) + (dz ** 2))\n",
    "            distance = distance + d\n",
    "\n",
    "            evaluate_RC(intended_x_mid, ego_x_position, intended_y_mid, ego_y_position)\n",
    "            evaluate_IS(speed_limit, red_light_violations, collision_sensor)\n",
    "\n",
    "            throttle, brake = TandB(ego_y_mid, intended_y_mid, ego_y_position)\n",
    "\n",
    "            velocity = ego.get_velocity()\n",
    "            speed = math.sqrt((velocity.x**2 + velocity.y**2 + velocity.z**2)) / 5\n",
    "            s_power = (1 - brake) * (1 - speed)\n",
    "\n",
    "            steer = steering(ego_x_position, intended_x_mid, s_power)\n",
    "\n",
    "            # print(outputs, waypoints, new_wp)\n",
    "            print(f\"\\rcurrently at: ({ego_x_position:.1f}, {ego_y_position:.1f}), mid: ({ego_x_mid:.1f}, {ego_y_mid:.1f}), going towards: ({intended_x_mid:.1f}, {intended_y_mid:.1f}), with b:{brake}, t:{throttle:.1f}, s:{steer:.1f} and ISc:{isc}, RCc: {rcc}      \", end = \"\")\n",
    "\n",
    "            control = carla.VehicleControl()\n",
    "            waypoints = outputs.detach().numpy()\n",
    "            control.steer = steer\n",
    "            control.throttle = throttle\n",
    "            control.brake = brake\n",
    "            ego.apply_control(control)\n",
    "            set_cam_to_EGO(ego)\n",
    "            \n",
    "    if(time.time() >= start_time + 60):\n",
    "        break\n",
    "    prev_emb = emb\n",
    "\n",
    "if distance > 0:\n",
    "    RC = rcc * 100 / distance\n",
    "    IS = isc * 100 / distance\n",
    "    evaluate_DS()\n",
    "\n",
    "print(f\"\\nRC is: {RC:.2f}, IS is: {IS:.2f}, DS is: {DS:.2f}, \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "previous ones destroyed\n",
      "stopped\n"
     ]
    }
   ],
   "source": [
    "camera.stop()\n",
    "remove_all_actors()\n",
    "\n",
    "ego_blueprint = blueprint_library.filter('*model3')[0]\n",
    "spawn_points = world.get_map().get_spawn_points()[0]\n",
    "ego = world.spawn_actor(ego_blueprint, spawn_points)\n",
    "\n",
    "camera_bp = blueprint_library.find('sensor.camera.rgb')\n",
    "camera_bp.set_attribute('image_size_x', '1920')\n",
    "camera_bp.set_attribute('image_size_y', '1080')\n",
    "camera_bp.set_attribute('fov', '110')\n",
    "camera_transform = carla.Transform(carla.Location(x = 1.5, z = 2.4))\n",
    "camera = world.spawn_actor(camera_bp, camera_transform, attach_to=ego)\n",
    "\n",
    "set_cam_to_EGO(ego, 65)\n",
    "print(\"stopped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "currently being worked upon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started\n",
      "currently at: (960.0, 75.0), mid: (960.0, 540.0), going towards: (1638.2, 140.5), with b:0, t:0.3, s:0.2         "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[89], line 115\u001b[0m\n\u001b[0;32m    113\u001b[0m         control\u001b[38;5;241m.\u001b[39mbrake \u001b[38;5;241m=\u001b[39m brake\n\u001b[0;32m    114\u001b[0m         ego\u001b[38;5;241m.\u001b[39mapply_control(control)\n\u001b[1;32m--> 115\u001b[0m         \u001b[43mset_cam_to_EGO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mego\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m start_time \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m400\u001b[39m):\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[57], line 2\u001b[0m, in \u001b[0;36mset_cam_to_EGO\u001b[1;34m(EGO, angle)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mset_cam_to_EGO\u001b[39m(EGO, angle \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m65\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m     spectator \u001b[38;5;241m=\u001b[39m \u001b[43mworld\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_spectator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     vehicle_transform \u001b[38;5;241m=\u001b[39m EGO\u001b[38;5;241m.\u001b[39mget_transform()\n\u001b[0;32m      4\u001b[0m     spectator\u001b[38;5;241m.\u001b[39mset_transform(\n\u001b[0;32m      5\u001b[0m         carla\u001b[38;5;241m.\u001b[39mTransform(\n\u001b[0;32m      6\u001b[0m             EGO_relative_view(EGO),\n\u001b[0;32m      7\u001b[0m             carla\u001b[38;5;241m.\u001b[39mRotation(pitch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mangle, yaw \u001b[38;5;241m=\u001b[39m vehicle_transform\u001b[38;5;241m.\u001b[39mrotation\u001b[38;5;241m.\u001b[39myaw, roll \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      8\u001b[0m         )\n\u001b[0;32m      9\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"started\")\n",
    "\n",
    "emb = None\n",
    "prev_emb = None\n",
    "image_1 = None\n",
    "\n",
    "RC = 0\n",
    "IS = 0\n",
    "DS = 0\n",
    "rcc = 0\n",
    "isc = 100\n",
    "\n",
    "start_time = time.time()\n",
    "pe = {}\n",
    "e = {}\n",
    "idx = 0\n",
    "\n",
    "speed_limit = 50\n",
    "red_light_violations = False\n",
    "\n",
    "set_cam_to_EGO(ego)\n",
    "camera.listen(image_to_emb) #changes emb, image_1 globally\n",
    "\n",
    "collision_sensor = CollisionSensor(ego)\n",
    "\n",
    "init_pos = ego.get_transform().location\n",
    "distance = 0.0\n",
    "fin_pos = init_pos\n",
    "\n",
    "while emb is None:\n",
    "    world.tick()\n",
    "\n",
    "if prev_emb:\n",
    "    for i in prev_emb.keys():\n",
    "        pe[i] = prev_emb[i]\n",
    "for i in emb.keys():\n",
    "    e[i] = emb[i]\n",
    "\n",
    "while True:\n",
    "    world.wait_for_tick()\n",
    "    e = emb['pixel_values']\n",
    "    # print(f\"unique emb['pixel_values]: {e}\")\n",
    "    if prev_emb:\n",
    "        pe = prev_emb['pixel_values']\n",
    "        # print(f\"unique prev_emb['pixel_values]: {pe}\")\n",
    "        if not torch.equal(e,pe):\n",
    "            idx = idx + 1\n",
    "\n",
    "            outputs = model_output_generation(e, driver_model)\n",
    "            waypoints = outputs.detach().numpy()\n",
    "\n",
    "            scene_x, scene_y = process_image(image_1).size\n",
    "            # print(scene_x, scene_y)\n",
    "            ego_x_mid = scene_x/2\n",
    "            ego_y_mid = scene_y/2\n",
    "\n",
    "            ego_x_position = ego_x_mid\n",
    "            ego_y_position = 75\n",
    "\n",
    "            maxiy = -float('inf')\n",
    "            maxix = -float('inf')\n",
    "            for i in waypoints:\n",
    "                if maxix < i[0]:\n",
    "                    maxix = i[0]\n",
    "                if maxiy < i[1]:\n",
    "                    maxiy = i[1]\n",
    "\n",
    "            sumx = 0\n",
    "            sumy = 0\n",
    "            for i in waypoints:\n",
    "                i[0] = i[0] * scene_x / maxix\n",
    "                i[1] = i[1] * scene_y / maxiy\n",
    "                sumx = sumx + i[0]\n",
    "                sumy = sumy + i[1]\n",
    "\n",
    "            # waypoints = normalize_wp(waypoints, scene_x, scene_y)\n",
    "\n",
    "            intended_x_mid = sumx / 10\n",
    "            intended_y_mid = sumy / 10\n",
    "            intended_y_mid = scene_y - intended_y_mid\n",
    "                \n",
    "            # print(waypoints)\n",
    "\n",
    "            # print(intended_x_mid, intended_y_mid)\n",
    "\n",
    "            # break\n",
    "\n",
    "            curr_pos = ego.get_transform().location\n",
    "            dx = curr_pos.x - fin_pos.x\n",
    "            dy = curr_pos.y - fin_pos.y\n",
    "            dz = curr_pos.z - fin_pos.z\n",
    "            d = math.sqrt((dx ** 2) + (dy ** 2) + (dz ** 2))\n",
    "            distance = distance + d\n",
    "\n",
    "            evaluate_RC(intended_x_mid, ego_x_position, intended_y_mid, ego_y_position)\n",
    "            evaluate_IS(speed_limit, red_light_violations, collision_sensor)\n",
    "\n",
    "            throttle, brake = TandB(ego_y_mid, intended_y_mid, ego_y_position)\n",
    "\n",
    "            velocity = ego.get_velocity()\n",
    "            speed = math.sqrt((velocity.x**2 + velocity.y**2 + velocity.z**2)) / 5\n",
    "            s_power = (1 - brake) * (1 - speed)\n",
    "\n",
    "            steer = steering(ego_x_position, intended_x_mid, s_power)\n",
    "\n",
    "            # print(outputs, waypoints, new_wp)\n",
    "            print(f\"\\rcurrently at: ({ego_x_position:.1f}, {ego_y_position:.1f}), mid: ({ego_x_mid:.1f}, {ego_y_mid:.1f}), going towards: ({intended_x_mid:.1f}, {intended_y_mid:.1f}), with b:{brake}, t:{throttle:.1f}, s:{steer:.1f}      \", end = \"\")\n",
    "\n",
    "            control = carla.VehicleControl()\n",
    "            waypoints = outputs.detach().numpy()\n",
    "            control.steer = steer\n",
    "            control.throttle = throttle\n",
    "            control.brake = brake\n",
    "            ego.apply_control(control)\n",
    "            set_cam_to_EGO(ego)\n",
    "            \n",
    "    if(time.time() >= start_time + 400):\n",
    "        break\n",
    "    prev_emb = emb\n",
    "if(distance > 0):\n",
    "    RC = rcc * 100 / distance\n",
    "    IS = isc * 100 / distance\n",
    "evaluate_DS()\n",
    "\n",
    "print(f\"\\nRC is: {RC:.2f}, DS is: {DS:.2f}, IS is: {IS:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "previous ones destroyed\n",
      "stopped\n"
     ]
    }
   ],
   "source": [
    "camera.stop()\n",
    "remove_all_actors()\n",
    "\n",
    "ego_blueprint = blueprint_library.filter('*model3')[0]\n",
    "spawn_points = world.get_map().get_spawn_points()[0]\n",
    "ego = world.spawn_actor(ego_blueprint, spawn_points)\n",
    "\n",
    "camera_bp = blueprint_library.find('sensor.camera.rgb')\n",
    "camera_bp.set_attribute('image_size_x', '1920')\n",
    "camera_bp.set_attribute('image_size_y', '1080')\n",
    "camera_bp.set_attribute('fov', '110')\n",
    "camera_transform = carla.Transform(carla.Location(x = 1.5, z = 2.4))\n",
    "camera = world.spawn_actor(camera_bp, camera_transform, attach_to=ego)\n",
    "\n",
    "set_cam_to_EGO(ego, 65)\n",
    "print(\"stopped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
