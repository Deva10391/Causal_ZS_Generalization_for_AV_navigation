{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import cv2\n",
    "import gym\n",
    "import math\n",
    "import time\n",
    "import json\n",
    "import h5py\n",
    "import carla\n",
    "import heapq\n",
    "import torch\n",
    "import random\n",
    "import requests\n",
    "import kagglehub\n",
    "import threading\n",
    "import transformers\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from PIL import Image\n",
    "from sklearn.decomposition import PCA\n",
    "from collections import Counter, deque\n",
    "from torchvision import models, transforms\n",
    "from skimage.morphology import skeletonize\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from huggingface_hub import hf_hub_download\n",
    "from transformers.image_transforms import rgb_to_id\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from transformers import pipeline, AutoModelForSemanticSegmentation, SegformerForSemanticSegmentation, SegformerConfig, CLIPProcessor, CLIPModel, AutoModelForSeq2SeqLM, AutoTokenizer, AutoImageProcessor, DetrImageProcessor, DetrForObjectDetection, DetrForSegmentation, TimeSeriesTransformerForPrediction, TimeSeriesTransformerConfig\n",
    "\n",
    "from torchvision import transforms as A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A1 = \"VL pretrained-M fine-tuned with semi-disentangled outputs\"\n",
    "A2 = \"Future Trajectory Prediction by LSTM using Multi Modal Inputs\"\n",
    "A3 = \"Future Waypoint Prediction using BEV representations and GPS mechanism\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ALGORITHM 1: VL pretrained-M fine-tuned with semi-disentangled outputs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "semantic_label_classes = [\"Traffic Sign\", \"Building\", \"Fence\", \"Other\", \"Pedestrian\", \"Pole\", \"Road Line\", \"Road\", \"Sidewalk\", \"Vegetation\", \"Car\", \"Wall\", \"Unlabeled\"]\n",
    "\n",
    "root = \"datasets\"\n",
    "images_dir = f\"{root}/images\"\n",
    "labels_dir = f\"{root}/labels\"\n",
    "waypoint_file = \"A1_dataset.json\"\n",
    "batch_size = 2\n",
    "\n",
    "skip = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Creation (IMAGE to EMBEDDING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = kagglehub.dataset_download(\"albertozorzetto/carla-densely-annotated-driving-dataset\")\n",
    "# print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Download the CARLA dataset using the Kaggle API\n",
    "# dataset_name = \"albertozorzetto/carla-densely-annotated-driving-dataset\"\n",
    "# destination_path = \"/content/datasets\"\n",
    "\n",
    "# import os\n",
    "# # Install Kaggle API\n",
    "# !pip install kaggle --upgrade\n",
    "# # Upload kaggle.json for authentication\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "# # Move kaggle.json to the proper directory\n",
    "# !mkdir -p ~/.kaggle\n",
    "# !mv kaggle.json ~/.kaggle/\n",
    "# !chmod 600 ~/.kaggle/kaggle.json\n",
    "# # Verify Kaggle is set up correctly\n",
    "# !kaggle datasets list\n",
    "# !kaggle datasets download -d {dataset_name} -p {destination_path} --unzip\n",
    "# print(\"Path to dataset files:\", destination_path)\n",
    "\n",
    "# import tarfile\n",
    "# images_path = os.path.join(destination_path, \"images\")\n",
    "# labels_path = os.path.join(destination_path, \"labels\")\n",
    "# # Create directories for extracted files\n",
    "# os.makedirs(images_path, exist_ok=True)\n",
    "# os.makedirs(labels_path, exist_ok=True)\n",
    "# # Extract and manage .tar files\n",
    "# tar_files = [f for f in os.listdir(destination_path) if f.endswith('.tar')]\n",
    "# for tar_file in tar_files:\n",
    "#     tar_path = os.path.join(destination_path, tar_file)\n",
    "#     print(f\"Extracting {tar_path}...\")\n",
    "#     with tarfile.open(tar_path) as tar:\n",
    "#         # Determine the folder to extract based on the tar file name\n",
    "#         if \"images\" in tar_file.lower():\n",
    "#             tar.extractall(path=images_path)\n",
    "#         elif \"labels\" in tar_file.lower():\n",
    "#             tar.extractall(path=labels_path)\n",
    "#         else:\n",
    "#             print(f\"Skipping unknown .tar file: {tar_file}\")\n",
    "#     os.remove(tar_path)  # Optional: Remove the .tar file after extraction\n",
    "# print(\"All .tar files have been extracted.\")\n",
    "# print(f\"Images folder: {images_path}\")\n",
    "# print(f\"Labels folder: {labels_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Saving the {way points} for passing to the LLM*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triangular_condition(x_check, y_check):\n",
    "    margin = 250\n",
    "    n = 5\n",
    "    if margin <= x_check <= 224 - margin:\n",
    "        y_inc = (n * x_check / 8) if x_check < 400 else ((n * 100) - (n * x_check / 8))\n",
    "        if y_check <= y_inc:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_main_waypoint(waypoints, skeleton):\n",
    "    ego_pos = [400,600]\n",
    "    if len(waypoints) == 0:\n",
    "        return None\n",
    "\n",
    "    filtered_points = []\n",
    "    for x,y in waypoints:\n",
    "        if triangular_condition(x,600 - y):\n",
    "            # print(x,y)\n",
    "            filtered_points.append((x,y)) \n",
    "\n",
    "    # plt.imshow(skeleton, cmap=\"gray\")\n",
    "    # plt.grid(True)\n",
    "    # for idx in range(0, len(filtered_points)):\n",
    "    #     x,y = filtered_points[idx]\n",
    "    #     plt.scatter(x, y, c=\"blue\", marker=\"o\")\n",
    "    # plt.show()\n",
    "\n",
    "    min_d_idx = None\n",
    "    min_d = float('inf')\n",
    "    for idx in range(0, len(filtered_points)):\n",
    "        d = (((x - ego_pos[0]) ** 2) + ((y - ego_pos[1]) ** 2)) ** (1 / 2)\n",
    "        if min_d > d:\n",
    "            min_d_idx = idx\n",
    "            min_d = d\n",
    "\n",
    "    if filtered_points == []:\n",
    "        return ego_pos\n",
    "    for val in filtered_points:\n",
    "        if val[0] > 390 and val[0] < 410:\n",
    "            return float(val[0]), float(val[1])\n",
    "    if len(filtered_points) <=2:\n",
    "        return float(ego_pos[0]), float(ego_pos[1])\n",
    "    mean_x = sum(ele[0] for ele in filtered_points) / len(filtered_points)\n",
    "    mean_y = sum(ele[1] for ele in filtered_points) / len(filtered_points)\n",
    "    return float(mean_x), float(mean_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_waypoints(label, num_waypoints):\n",
    "    if len(label.shape) == 3:\n",
    "        label = cv2.cvtColor(label, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    road_mask = (label == 90).astype(np.uint8)\n",
    "    \n",
    "    contours, _ = cv2.findContours(road_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if not contours:\n",
    "        return []\n",
    "    \n",
    "    largest_contour = max(contours, key=cv2.contourArea)\n",
    "    binary_image = (road_mask > 0).astype(np.uint8)\n",
    "    skeleton = skeletonize(binary_image).astype(np.uint8)\n",
    "    \n",
    "    y_coords, x_coords = np.where(skeleton > 0)\n",
    "    path_points = np.column_stack((x_coords, y_coords))\n",
    "    \n",
    "    x_min, x_max = path_points[:, 0].min(), path_points[:, 0].max()\n",
    "    y_min, y_max = path_points[:, 1].min(), path_points[:, 1].max()\n",
    "\n",
    "    x_range = (x_max - x_min) * 0.25\n",
    "    y_range = (y_max - y_min) * 0.25\n",
    "\n",
    "    # path_points = path_points[\n",
    "    #     (path_points[:, 0] > x_min + x_range) & (path_points[:, 0] < x_max - x_range) &\n",
    "    #     (path_points[:, 1] > y_min + y_range) & (path_points[:, 1] < y_max - y_range)\n",
    "    # ]\n",
    "\n",
    "    if len(path_points) == 0:\n",
    "        return []\n",
    "\n",
    "    sampled_indices = np.linspace(0, len(path_points) - 1, num_waypoints, dtype=int)\n",
    "    waypoints = path_points[sampled_indices]\n",
    "\n",
    "    num_selected_waypoints = min(25, len(path_points))\n",
    "    sampled_indices = np.linspace(0, len(path_points) - 1, num_selected_waypoints, dtype=int)\n",
    "    waypoints = path_points[sampled_indices]\n",
    "    \n",
    "    # print(\"Min/Max path points:\", path_points.min(), path_points.max())\n",
    "\n",
    "    # plt.imshow(skeleton, cmap=\"gray\")\n",
    "    # plt.scatter(x_coords, y_coords, c=\"red\", marker=\"o\")\n",
    "    # plt.show()\n",
    "\n",
    "    main_waypoint = get_main_waypoint(waypoints, skeleton)\n",
    "    # print(\"Main Waypoint:\", main_waypoint)\n",
    "\n",
    "    waypoints = main_waypoint\n",
    "\n",
    "    # print(main_waypoint[0], main_waypoint[1])\n",
    "\n",
    "    # if len(waypoints) > 0:\n",
    "    #     plt.figure(figsize=(10, 10))\n",
    "    #     plt.imshow(label, cmap=\"gray\")\n",
    "    #     x = waypoints[0]\n",
    "    #     y = waypoints[1]\n",
    "    #     plt.scatter(x, y, c='red', marker='o')\n",
    "    #     plt.text(x, y, f'({x:.1f}, {y:.1f})', fontsize=8, color='yellow', ha='right', va='bottom')\n",
    "    #     plt.title(\"Extracted Waypoints\")\n",
    "    #     plt.show()\n",
    "\n",
    "    return waypoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(labels_dir, output_dir, num_waypoints):\n",
    "\n",
    "    for i in range(0, 28):\n",
    "        k = \"0\" if i <= 9 else \"\"\n",
    "        curr_folder_labels = f\"{labels_dir}/Video_0{k}{i}\"\n",
    "        labels_path = [file for file in os.listdir(curr_folder_labels) if file.endswith(\".png\")]\n",
    "        labels_path.sort()\n",
    "        labels_path = labels_path[::skip]\n",
    "\n",
    "        for label_file in labels_path:\n",
    "            label_path = os.path.join(curr_folder_labels, label_file)\n",
    "            label = Image.open(label_path)\n",
    "            label_np = np.array(label)\n",
    "            waypoints = generate_waypoints(label_np, num_waypoints=num_waypoints)\n",
    "            if waypoints == []:\n",
    "                waypoints = [400,600]\n",
    "            output_label_path = os.path.join(output_dir, label_file)\n",
    "            dataset_waypoints = [{\n",
    "                \"label\": output_label_path,\n",
    "                \"waypoints\": waypoints\n",
    "            }]\n",
    "            # break\n",
    "\n",
    "            with open(os.path.join(output_dir, waypoint_file), 'a') as wf:\n",
    "                json.dump(dataset_waypoints, wf)\n",
    "                wf.write(\"\\n\")\n",
    "                # break\n",
    "\n",
    "        if(i<27):\n",
    "            print(f\"\\rProgress: {((i + 1) / 0.27):.2f} %\", end = \"\")\n",
    "        # break\n",
    "\n",
    "    print(f\"\\nDone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_dataset(labels_dir, root, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_the_wps_A1(labels_dir, output_dir, num_waypoints):\n",
    "    dataset_waypoints = []\n",
    "    folder_name = f\"{labels_dir}/Video_000\"\n",
    "    file_1 = os.path.join(folder_name, \"v000_0000.png\")\n",
    "\n",
    "    folder_name = f\"{labels_dir}/Video_007\"\n",
    "    file_2 = os.path.join(folder_name, \"v007_0019.png\")\n",
    "\n",
    "    generate_waypoints(np.array(Image.open(file_1)), num_waypoints = num_waypoints)\n",
    "    generate_waypoints(np.array(Image.open(file_2)), num_waypoints = num_waypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_the_wps_A1(labels_dir, root, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "getting dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, images_dir):\n",
    "        self.data = []\n",
    "\n",
    "        for i in range(0, 28):\n",
    "            k = \"0\" if i <= 9 else \"\"\n",
    "            curr_folder_images = f\"{images_dir}/Video_0{k}{i}\"\n",
    "            images_path = [file for file in os.listdir(curr_folder_images) if file.endswith(\".png\")]\n",
    "            images_path.sort()\n",
    "            images_path = images_path[::skip]\n",
    "\n",
    "            for image_file in images_path:\n",
    "                image_path = os.path.join(curr_folder_images, image_file)\n",
    "                image = Image.open(image_path)\n",
    "                image_embeddings = processor(text=semantic_label_classes, images=image, return_tensors=\"pt\", padding=True)\n",
    "                ei = image_embeddings['pixel_values']\n",
    "\n",
    "                with open(os.path.join(root, waypoint_file), \"r\") as wf:\n",
    "                    data_list = wf.read().splitlines()\n",
    "                    for data in data_list:\n",
    "                        data = json.loads(data)[0]\n",
    "                        if data['label'] == f\"datasets\\\\{image_file}\":\n",
    "                            corresponding_waypoints = data['waypoints']\n",
    "                            cw = corresponding_waypoints\n",
    "                            # print('match found')\n",
    "                            self.data.append((ei,cw))\n",
    "                            break\n",
    "                break # for first image in all folders\n",
    "            # break\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        embeddings, waypoints = self.data[idx]\n",
    "        return embeddings, waypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split indices for train and validation\n",
    "dataset = SegmentationDataset(images_dir)\n",
    "remaining_indices, test_indices = train_test_split(range(len(dataset)), test_size=0.1, random_state=42)\n",
    "\n",
    "test_dataset = Subset(dataset, test_indices)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, drop_last=True)\n",
    "\n",
    "train_indices, val_indices = train_test_split(remaining_indices, test_size=0.3, random_state=42)\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(pixel_values):\n",
    "    mini = pixel_values.min()\n",
    "    maxi = pixel_values.max()\n",
    "    pixel_values = (pixel_values - mini) / (maxi - mini)\n",
    "    return pixel_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OUR VIT WP MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VitWPModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VitWPModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(512)\n",
    "        self.conv3 = nn.Conv2d(512, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "        self.conv5 = nn.Conv2d(512, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 512)\n",
    "        self.fc2 = nn.Linear(512, 2)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        b = pixel_values.shape[0]\n",
    "        x = torch.relu(self.conv1(pixel_values))\n",
    "        x = self.bn1(x)\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        \n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.bn2(x)\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = self.bn3(x)\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "\n",
    "        x = torch.relu(self.conv4(x))\n",
    "        x = self.bn4(x)\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "\n",
    "        x = torch.relu(self.conv5(x))\n",
    "        x = self.bn5(x)\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        output = self.fc2(x)\n",
    "        return output.view(b,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_train = VitWPModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*--training set ups--*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wp_generation(pixel_values, model):\n",
    "    pixel_values = pixel_values.squeeze(1)\n",
    "    outputs = model(pixel_values)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model_to_train, train_loader, optimizer, criterion):\n",
    "    model_to_train.train()\n",
    "    running_loss = 0.0\n",
    "    running_accuracy = 0.0\n",
    "\n",
    "    for data in train_loader:\n",
    "        p_val = data[0]\n",
    "        target = torch.stack(data[1], dim=1)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = wp_generation(p_val, model_to_train)\n",
    "        loss = criterion(outputs.float(), target.float())\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # break\n",
    "    return running_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model_to_validate, val_loader, optimizer, criterion):\n",
    "    model_to_validate.eval()\n",
    "    running_loss = 0.0\n",
    "    running_accuracy = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            p_val = data[0]\n",
    "            target = torch.stack(data[1], dim=1)\n",
    "\n",
    "            outputs = wp_generation(p_val, model_to_validate)\n",
    "            loss = criterion(outputs.float(), target.float())\n",
    "            running_loss += loss.item()\n",
    "            # break\n",
    "    return running_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model_to_test, dataloader, optimizer, criterion):\n",
    "    model_to_test.eval()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            p_val = data[0]\n",
    "            target = torch.stack(data[1], dim=1)\n",
    "\n",
    "            outputs = wp_generation(p_val, model_to_test)\n",
    "            print(outputs)\n",
    "            print(target)\n",
    "            loss = criterion(outputs.float(), target.float())\n",
    "            running_loss += loss.item()\n",
    "            break\n",
    "    return running_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_save = best_model_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_to_save, 'models/VitWPModel.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_model = model_to_save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ALGORITHM 2: Future Trajectory Prediction by LSTM using Multi Modal Inputs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_processor = AutoImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "detr_model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "\n",
    "root = \"datasets\"\n",
    "images_dir = f\"{root}/images\"\n",
    "labels_dir = f\"{root}/labels\"\n",
    "bev_file = \"A2_dataset_m.json\"\n",
    "batch_size = 1\n",
    "\n",
    "skip = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Creation (IMAGE to TEMPORAL DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "image > depth map and object detection > bird's eye view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_depth_map(image):\n",
    "    estimator = pipeline(task=\"depth-estimation\", model=\"Intel/dpt-large\")\n",
    "    result = estimator(images=image)\n",
    "    depth_map = result['predicted_depth']\n",
    "    return depth_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_depth_map(depth_map, cord_x = 400, cord_y = 75, c = 'white'):\n",
    "    depth_map = depth_map.max() - depth_map\n",
    "    height, width = depth_map.shape\n",
    "    depth_at_cord = depth_map[cord_y-1, cord_x-1]\n",
    "    print(f\"Depth at co-ordinate ({cord_x}, {cord_y}) of the image: {depth_at_cord:.2f} meters\")\n",
    "    plt.scatter(cord_x, cord_y, color=c, marker='x')\n",
    "    plt.imshow(depth_map, cmap = 'inferno')\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def object_detection(image):\n",
    "    inputs = image_processor(images=image, return_tensors=\"pt\")\n",
    "    outputs = detr_model(**inputs)\n",
    "    target_sizes = torch.tensor([image.size[::-1]])\n",
    "    objects = image_processor.post_process_object_detection(outputs, threshold=0.9, target_sizes=target_sizes)[0]\n",
    "\n",
    "    return objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def future_positions(objects, alpha = 0.1):\n",
    "    trajectories = []\n",
    "    \n",
    "    for obj in objects:\n",
    "        smoothed_positions = [obj]\n",
    "        for t in range(0, len(obj)):\n",
    "            smoothed_pos = alpha * obj[t] + (1 - alpha) * smoothed_positions[-1]\n",
    "            smoothed_positions.append(smoothed_pos)\n",
    "        future_positions = [smoothed_positions[-1]]\n",
    "        trajectories.append(future_positions)\n",
    "\n",
    "    new = []\n",
    "    for i in trajectories:\n",
    "        new.append(i[0].detach().numpy())\n",
    "\n",
    "    trajectories = torch.tensor(new, dtype = torch.float32)\n",
    "        \n",
    "    return trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def depth_map_to_bev(depth_map, objects, trajectories):\n",
    "    f_x = 1000\n",
    "    f_y = 1000\n",
    "    c_x = 400\n",
    "    c_y = 300\n",
    "    bev_size = 200\n",
    "    scale_factor = 15\n",
    "\n",
    "    height, width = depth_map.shape\n",
    "    bev_map = np.zeros((bev_size, bev_size), dtype=np.float32)\n",
    "\n",
    "    x_coords, y_coords = np.meshgrid(np.arange(width), np.arange(height))\n",
    "\n",
    "    depth = depth_map[y_coords, x_coords].detach().numpy()\n",
    "    X = (x_coords - c_x) * depth / f_x\n",
    "    Y = (y_coords - c_y) * depth / f_y\n",
    "\n",
    "    bev_x = np.clip((X * scale_factor).astype(int), 0, bev_size - 1)\n",
    "    bev_y = np.clip((Y * scale_factor).astype(int), 0, bev_size - 1)\n",
    "\n",
    "    for i in range(height * width):\n",
    "        bev_map[bev_y.flat[i], bev_x.flat[i]] = max(bev_map[bev_y.flat[i], bev_x.flat[i]], depth.flat[i])\n",
    "\n",
    "    for obj, traj in zip(objects, trajectories):\n",
    "        x_min, y_min, x_max, y_max = obj\n",
    "        x_min, y_min, x_max, y_max = x_min.item(), y_min.item(), x_max.item(), y_max.item()\n",
    "\n",
    "        traj_x_min, traj_y_min, traj_x_max, traj_y_max = traj\n",
    "        traj_x_min, traj_y_min, traj_x_max, traj_y_max = traj_x_min.item(), traj_y_min.item(), traj_x_max.item(), traj_y_max.item()\n",
    "\n",
    "        obj_depths = depth_map[int(y_min):int(y_max), int(x_min):int(x_max)].detach().numpy()\n",
    "        if obj_depths.size > 0:\n",
    "            mean_depth = np.mean(obj_depths)\n",
    "        else:\n",
    "            mean_depth = 0\n",
    "        \n",
    "        X_min = (x_min - c_x) * mean_depth / f_x\n",
    "        X_max = (x_max - c_x) * mean_depth / f_x\n",
    "        Y_min = (y_min - c_y) * mean_depth / f_y\n",
    "        Y_max = (y_max - c_y) * mean_depth / f_y\n",
    "        Z = mean_depth\n",
    "        bev_x_min = int(100 + X_min * scale_factor)\n",
    "        bev_x_max = int(100 + X_max * scale_factor)\n",
    "        bev_y_min = int(100 + Y_min * scale_factor)\n",
    "        bev_y_max = int(100 + Y_max * scale_factor)\n",
    "\n",
    "        traj_X_min = (traj_x_min - c_x) * mean_depth / f_x\n",
    "        traj_X_max = (traj_x_max - c_x) * mean_depth / f_x\n",
    "        traj_Y_min = (traj_y_min - c_y) * mean_depth / f_y\n",
    "        traj_Y_max = (traj_y_max - c_y) * mean_depth / f_y\n",
    "        traj_Z = mean_depth\n",
    "        traj_bev_x_min = int(100 + traj_X_min * scale_factor)\n",
    "        traj_bev_x_max = int(100 + traj_X_max * scale_factor)\n",
    "        traj_bev_y_min = int(100 + traj_Y_min * scale_factor)\n",
    "        traj_bev_y_max = int(100 + traj_Y_max * scale_factor)\n",
    "\n",
    "        mean_depth = torch.tensor(mean_depth, dtype = torch.float32)\n",
    "        if mean_depth > 4:\n",
    "            bev_map[bev_y_min-10:bev_y_max+10, bev_x_min-10:bev_x_max+10] = 0 # bev_map.max() - mean_depth\n",
    "            bev_map[traj_bev_y_min-10:traj_bev_y_max+10, traj_bev_x_min-10:traj_bev_x_max+10] = 0 # bev_map.max() - mean_depth\n",
    "\n",
    "    bev_map = np.clip(bev_map, 0, 255)\n",
    "    bev_map = bev_map.astype(np.uint8)\n",
    "\n",
    "    return bev_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_bev(bev_map):\n",
    "    bev_map = bev_map.max() - bev_map\n",
    "    bev_map = cv2.normalize(bev_map, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    bev_map = np.uint8(bev_map)\n",
    "    plt.imshow(bev_map, cmap=\"jet\", interpolation='nearest')\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_waypoint(bev_map):\n",
    "    bev_map = bev_map.max() - bev_map\n",
    "    bev_map = cv2.normalize(bev_map, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    bev_map = np.uint8(bev_map)\n",
    "    x_mid = 100\n",
    "    dist_from_x_mid = float('inf')\n",
    "\n",
    "    filtered_ones = []\n",
    "    for x in range(0,200,4):\n",
    "        for y in range(0,200,4):\n",
    "            b = bev_map[y,x]\n",
    "            if b < 80:\n",
    "                dist_from_x_mid = abs(x_mid - x)\n",
    "                x_for_filtering = x\n",
    "                y_for_filtering = y\n",
    "                # plt.scatter(x_for_filtering * 4, y_for_filtering * 3, color = 'black', marker='x')\n",
    "                filtered_ones.append([y_for_filtering, x_for_filtering])\n",
    "                # break\n",
    "\n",
    "    fof_cam = []\n",
    "    for i in filtered_ones:\n",
    "        fy = i[0]\n",
    "        fx = i[1]\n",
    "        fof_cam.append(bev_map[fy, fx])\n",
    "    fof_center = []\n",
    "    for i in range(0, len(filtered_ones)):\n",
    "        curr_x = filtered_ones[i][1]\n",
    "        fof_center.append(abs(x_mid - curr_x))\n",
    "\n",
    "    mini = min(fof_center)\n",
    "    mini_idxs = []\n",
    "    mini_val = float('inf')\n",
    "    for i in range(0, len(fof_center)):\n",
    "        if fof_center[i] <= mini_val:\n",
    "            mini_val = fof_center[i]\n",
    "    for i in range(0, len(fof_center)):\n",
    "        if(fof_center[i] == mini_val):\n",
    "            mini_idxs.append(i)\n",
    "    \n",
    "    max_x = -float('inf')\n",
    "    for i in mini_idxs:\n",
    "        cords = filtered_ones[i]\n",
    "        if cords[1] > max_x:\n",
    "            max_x = cords[1]\n",
    "            sel_y = cords[0]\n",
    "    cord_x = max_x\n",
    "    cord_y = sel_y\n",
    "\n",
    "    x_above = y_for_filtering\n",
    "    \n",
    "    # plt.scatter(cord_x * 4, cord_y * 3, color = 'black', marker='x')\n",
    "    # plt.imshow(depth_map.max() - depth_map, cmap = 'inferno')\n",
    "    # plt.grid(True)\n",
    "    # plt.show()\n",
    "\n",
    "    return cord_x * 4, cord_y * 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Saving BEV details*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bev_for_A2(image):\n",
    "    # depth_map = image_to_depth_map(image)\n",
    "    # objects = object_detection(image)\n",
    "    # trajectories = future_positions(objects['boxes'])\n",
    "    # bev_map = depth_map_to_bev(depth_map, objects['boxes'], trajectories)\n",
    "    # print(tuple(bev_map).shape)\n",
    "    bev_map = bev_generation(image, image_to_bev_model)\n",
    "    return bev_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_waypoints(file, image_name):\n",
    "    with open(os.path.join('datasets','A1_dataset.json'), 'r') as rf:\n",
    "        for data in rf:\n",
    "            data = json.loads(data)\n",
    "            if data[0]['label'] == image_name:\n",
    "                x, y = data[0]['waypoints']\n",
    "                val = [x, y]\n",
    "                # print(f\"match found: {val}\")\n",
    "                return val\n",
    "\n",
    "    return [400, 600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_bev_dataset(images_dir, output_dir):\n",
    "\n",
    "    for i in range(0, 28):\n",
    "        k = \"0\" if i <= 9 else \"\"\n",
    "        curr_folder_images = f\"{images_dir}/Video_0{k}{i}\"\n",
    "        images_path = [file for file in os.listdir(curr_folder_images) if file.endswith(\".png\")]\n",
    "        images_path.sort()\n",
    "        images_path = images_path[::skip]\n",
    "\n",
    "        for image_file in images_path:\n",
    "            image_path = os.path.join(curr_folder_images, image_file)\n",
    "            image = Image.open(image_path)\n",
    "            image_np = np.array(image)\n",
    "\n",
    "            bev_map = generate_bev_for_A2(image)\n",
    "            wp_x, wp_y = generate_waypoints(image_path, f\"datasets\\\\{image_file}\")\n",
    "            wp_x = wp_x / 4\n",
    "            wp_y = wp_y / 3\n",
    "\n",
    "            output_image_path = os.path.join(curr_folder_images, image_file)\n",
    "            bevs = ({\n",
    "                \"image\": output_image_path,\n",
    "                \"bev_map\": bev_map.tolist(),\n",
    "                \"way_point\": [wp_x, wp_y]\n",
    "            })\n",
    "            # break\n",
    "\n",
    "            with open(os.path.join(output_dir, \"A2_dataset_m.json\"), 'a') as wf:\n",
    "                json.dump(bevs, wf)\n",
    "                wf.write(\"\\n\")\n",
    "            break\n",
    "\n",
    "        if(i<27):\n",
    "            print(f\"\\rProgress: {((i + 1) / 0.27):.2f} %\", end = \"\")\n",
    "        # break\n",
    "\n",
    "    print(f\"\\nDone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_bev_dataset(images_dir, root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_the_wps_A2(images_dir, output_dir):\n",
    "\n",
    "    folder_name = f\"{images_dir}/Video_000\"\n",
    "    file_1 = os.path.join(folder_name, \"v000_0000.png\")\n",
    "    bev_1 = generate_bev_for_A2(Image.open(file_1))\n",
    "    wp_x, wp_y = generate_waypoints(file_1, \"datasets\\\\v000_0000.png\")\n",
    "    wp_x = wp_x / 4\n",
    "    wp_y = wp_y / 3\n",
    "\n",
    "    plt.figure(figsize = (4,4))\n",
    "    plt.imshow(bev_1)\n",
    "    plt.scatter(wp_x, wp_y, c = 'red', marker = 'o')\n",
    "    plt.show()\n",
    "    \n",
    "    output_image_path = os.path.join(folder_name, \"v000_0000.png\")\n",
    "    bevs_1 = [{\n",
    "        \"image\": output_image_path,\n",
    "        \"bev_map\": bev_1.tolist(),\n",
    "        \"way_point\": [wp_x, wp_y]\n",
    "    }]\n",
    "\n",
    "    folder_name = f\"{images_dir}/Video_007\"\n",
    "    file_2 = os.path.join(folder_name, \"v007_0019.png\")\n",
    "    bev_2 = generate_bev_for_A2(Image.open(file_2))\n",
    "    wp_x, wp_y = generate_waypoints(file_2, \"datasets\\\\v007_0019.png\")\n",
    "    wp_x = wp_x / 4\n",
    "    wp_y = wp_y / 3\n",
    "\n",
    "    plt.figure(figsize = (4,4))\n",
    "    plt.imshow(bev_2)\n",
    "    plt.scatter(wp_x, wp_y, c = 'red', marker = 'o')\n",
    "    plt.show()\n",
    "    \n",
    "    output_image_path = os.path.join(folder_name, file_2)\n",
    "    bevs_2 = [{\n",
    "        \"image\": output_image_path,\n",
    "        \"bev_map\": bev_2.tolist(),\n",
    "        \"way_point\": [wp_x, wp_y]\n",
    "    }]\n",
    "\n",
    "    # print(bevs_1[0]['image'], bevs_1[0]['way_point'])\n",
    "    # print(bevs_2[0]['image'], bevs_2[0]['way_point'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_the_wps_A2(images_dir, root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "getting dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BEVDataset(Dataset):\n",
    "    def __init__(self, root, bev_file, sequence_length = 5):\n",
    "        self.data = []\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "        with open(os.path.join(root, bev_file), \"r\") as wf:\n",
    "            images = []\n",
    "            bev_maps = []\n",
    "            waypoints = []\n",
    "            for data in wf:\n",
    "                data = json.loads(data)\n",
    "                # print(data.keys())\n",
    "\n",
    "                image = data['image']\n",
    "                bev_map = torch.tensor(data['bev_map'])\n",
    "                way_point = torch.tensor(data['way_point'])\n",
    "\n",
    "                images.append(image)\n",
    "                bev_maps.append(bev_map)\n",
    "                waypoints.append(way_point)\n",
    "\n",
    "                if len(images) >= self.sequence_length:\n",
    "                    sequence_images = images[-self.sequence_length:]\n",
    "                    sequence_bev_maps = bev_maps[-self.sequence_length:]\n",
    "                    sequence_waypoints = waypoints[-self.sequence_length:]\n",
    "                    self.data.append([sequence_images, sequence_bev_maps, sequence_waypoints])\n",
    "                    images = []\n",
    "                    bev_maps = []\n",
    "                    waypoints = []\n",
    "                    # print(self.data)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split indices for train and validation\n",
    "dataset = BEVDataset(root, bev_file, sequence_length = 5)\n",
    "remaining_indices, test_indices = train_test_split(range(len(dataset)), test_size=0.1, random_state=42)\n",
    "\n",
    "test_dataset = Subset(dataset, test_indices)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, drop_last=True)\n",
    "\n",
    "train_indices, val_indices = train_test_split(remaining_indices, test_size=0.5, random_state=42)\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BEV to image model (for faster operation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageToBEV(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageToBEV, self).__init__()\n",
    "        self.backbone = models.resnet18(pretrained=True)\n",
    "        self.backbone = nn.Sequential(*list(self.backbone.children())[:-2])\n",
    "        self.conv1x1 = nn.Conv2d(512, 256, kernel_size=1)\n",
    "        \n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 512, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(512, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 1, kernel_size=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.conv1x1(x)\n",
    "        x = self.upsample(x)\n",
    "        x = F.interpolate(x, size=(200, 200), mode='bilinear', align_corners=False)\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_to_bev_model = ImageToBEV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, bev_file):\n",
    "        self.data = []\n",
    "\n",
    "        with open(os.path.join(root, bev_file), \"r\") as wf:\n",
    "            for data in wf:\n",
    "                data = json.loads(data)\n",
    "                data = data[0]\n",
    "                image = np.array(Image.open(data['image']))\n",
    "                bev_map = torch.tensor(data['bev_map'])\n",
    "\n",
    "                self.data.append([image, bev_map])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_to_bev_model = torch.load('models/ImageToBEV.pth', map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageset = ImageDataset(bev_file)\n",
    "image_loader = DataLoader(imageset, batch_size = batch_size, shuffle = True, pin_memory=True, drop_last=True)\n",
    "\n",
    "dataloader = image_loader\n",
    "train_loader = image_loader\n",
    "val_loader = image_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bev_generation(image, model):\n",
    "    return model(torch.tensor(np.array(image, dtype = np.float32) / 255.0, dtype=torch.float32).unsqueeze(0).permute(0,3,1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, image_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in image_loader:\n",
    "        img = data[0]\n",
    "        bev = data[1]\n",
    "        optimizer.zero_grad()\n",
    "        pred = bev_generation(img[0], model)\n",
    "        loss = criterion(pred, bev[0].float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(image_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, image_loader, optimizer, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in image_loader:\n",
    "            img = data[0]\n",
    "            bev = data[1]\n",
    "            pred = bev_generation(img[0], model)\n",
    "            loss = criterion(pred, bev[0].float())\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(image_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, image_loader, optimizer, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in image_loader:\n",
    "            img = data[0]\n",
    "            bev = data[1]\n",
    "            pred = bev_generation(img[0], model)\n",
    "            print(pred)\n",
    "            print(waypoints)\n",
    "            loss = criterion(pred, bev[0].float())\n",
    "            total_loss += loss.item()\n",
    "            break\n",
    "    return total_loss / len(image_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_train = image_to_bev_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_save = best_model_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_to_save, 'models/ImageToBEV.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OUR TEMPORAL FUSION MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size=3):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.conv = nn.Conv2d(input_dim + hidden_dim, hidden_dim * 4, kernel_size, padding=padding)\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        h, c = hidden\n",
    "        combined = torch.cat([x, h], dim=1)\n",
    "        conv_out = self.conv(combined)\n",
    "        i, f, g, o = torch.chunk(conv_out, 4, dim=1)\n",
    "        i, f, g, o = torch.sigmoid(i), torch.sigmoid(f), torch.tanh(g), torch.sigmoid(o)\n",
    "        c_next = f * c + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "        return h_next, c_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim, sequence_length):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.output_dim = output_dim\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "        self.lstm_cells = nn.ModuleList([\n",
    "            ConvLSTMCell(input_dim if i == 0 else hidden_dim, hidden_dim)\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "        self.fc = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, c, h, w = x.size()\n",
    "        h_states = [torch.zeros(b, self.hidden_dim, h, w, device=x.device) for _ in range(self.num_layers)]\n",
    "        c_states = [torch.zeros(b, self.hidden_dim, h, w, device=x.device) for _ in range(self.num_layers)]\n",
    "        \n",
    "        for time_step in range(t):\n",
    "            inp = x[:, time_step]\n",
    "            for i, cell in enumerate(self.lstm_cells):\n",
    "                h_states[i], c_states[i] = cell(inp, (h_states[i], c_states[i]))\n",
    "                inp = h_states[i]\n",
    "        \n",
    "        out = torch.flatten(inp, start_dim=1)\n",
    "\n",
    "        if self.fc is None:\n",
    "            self.fc = nn.Linear(self.hidden_dim * h * w, self.output_dim * self.sequence_length).to(x.device)\n",
    "\n",
    "        return self.fc(out).contiguous().view(b, t, self.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim, hidden_dim, num_layers, output_dim, seq_length = 1, 64, 2, 2, 5\n",
    "model_to_train = ConvLSTM(input_dim, hidden_dim, num_layers, output_dim, seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*--training set ups--*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wp_generation(bev, model):\n",
    "    return model(bev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, train_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        bev_maps = torch.stack(data[1], dim=1)\n",
    "        waypoints = torch.stack(data[2], dim=1).to(torch.float32)\n",
    "        bev_maps = bev_maps.unsqueeze(2)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pred = wp_generation(bev_maps, model)\n",
    "        loss = criterion(pred, waypoints)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, val_loader, optimizer, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            bev_maps = torch.stack(data[1], dim=1)\n",
    "            waypoints = torch.stack(data[2], dim=1).to(torch.float32)\n",
    "            bev_maps = bev_maps.unsqueeze(2)\n",
    "            \n",
    "            pred = wp_generation(bev_maps, model)\n",
    "            loss = criterion(pred, waypoints)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataloader, optimizer, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            bev_maps = torch.stack(data[1], dim=1)\n",
    "            waypoints = torch.stack(data[2], dim=1).to(torch.float32)\n",
    "            bev_maps = bev_maps.unsqueeze(2)\n",
    "            \n",
    "            pred = wp_generation(bev_maps, model)\n",
    "            print(pred)\n",
    "            print(waypoints)\n",
    "            loss = criterion(pred, waypoints)\n",
    "            total_loss += loss.item()\n",
    "            break\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_save = best_model_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_to_save, 'models/ConvLSTM.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_model = model_to_save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ALGORITHM 3: Future Waypoint Prediction using BEV representations and GPS mechanism**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_processor = AutoImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "detr_model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "\n",
    "root = \"datasets\"\n",
    "images_dir = f\"{root}/images\"\n",
    "labels_dir = f\"{root}/labels\"\n",
    "bev_t1_t2_file = \"A3_dataset_m.json\"\n",
    "batch_size = 2\n",
    "\n",
    "skip = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Creation (IMAGE to t1, t2 BEV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "image and navigation targets > bird's eye view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_depth_map(image):\n",
    "    estimator = pipeline(task=\"depth-estimation\", model=\"Intel/dpt-large\")\n",
    "    result = estimator(images=image)\n",
    "    depth_map = result['predicted_depth']\n",
    "    return depth_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def object_detection(image):\n",
    "    inputs = image_processor(images=image, return_tensors=\"pt\")\n",
    "    outputs = detr_model(**inputs)\n",
    "    target_sizes = torch.tensor([image.size[::-1]])\n",
    "    objects = image_processor.post_process_object_detection(outputs, threshold=0.9, target_sizes=target_sizes)[0]\n",
    "\n",
    "    return objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def future_positions(objects, alpha = 0.1, diff = 5):\n",
    "    trajectories_t1 = []\n",
    "    trajectories_t2 = []\n",
    "    \n",
    "    for obj in objects:\n",
    "        smoothed_positions_t1 = [obj]\n",
    "        smoothed_positions_t2 = [obj]\n",
    "        for t in range(0, len(obj)):\n",
    "            smoothed_pos_t1 = alpha * obj[t] + (1 - alpha) * smoothed_positions_t1[-1]\n",
    "            smoothed_pos_t2 = (diff * alpha) * obj[t] + (1 - (diff * alpha)) * smoothed_positions_t2[-1]\n",
    "            smoothed_positions_t1.append(smoothed_pos_t1)\n",
    "            smoothed_positions_t2.append(smoothed_pos_t2)\n",
    "        future_positions_t1 = [smoothed_positions_t1[-1]]\n",
    "        future_positions_t2 = [smoothed_positions_t2[-1]]\n",
    "        trajectories_t1.append(future_positions_t1)\n",
    "        trajectories_t2.append(future_positions_t2)\n",
    "\n",
    "    new_t1 = []\n",
    "    new_t2 = []\n",
    "    for t1, t2 in zip(trajectories_t1, trajectories_t2):\n",
    "        new_t1.append(t1[0].detach().numpy())\n",
    "        new_t2.append(t2[0].detach().numpy())\n",
    "\n",
    "    trajectories_t1 = torch.tensor(new_t1, dtype = torch.float32)\n",
    "    trajectories_t2 = torch.tensor(new_t2, dtype = torch.float32)\n",
    "        \n",
    "    return trajectories_t1, trajectories_t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def depth_map_to_bev(depth_map, objects, trajectories_t1, trajectories_t2):\n",
    "    f_x = 1000\n",
    "    f_y = 1000\n",
    "    c_x = 400\n",
    "    c_y = 300\n",
    "    bev_size = 200\n",
    "    scale_factor = 15\n",
    "\n",
    "    height, width = depth_map.shape\n",
    "    bev_map_t1 = np.zeros((bev_size, bev_size), dtype=np.float32)\n",
    "    bev_map_t2 = np.zeros((bev_size, bev_size), dtype=np.float32)\n",
    "\n",
    "    x_coords, y_coords = np.meshgrid(np.arange(width), np.arange(height))\n",
    "\n",
    "    depth = depth_map[y_coords, x_coords].detach().numpy()\n",
    "    X = (x_coords - c_x) * depth / f_x\n",
    "    Y = (y_coords - c_y) * depth / f_y\n",
    "\n",
    "    bev_x = np.clip((X * scale_factor).astype(int), 0, bev_size - 1)\n",
    "    bev_y = np.clip((Y * scale_factor).astype(int), 0, bev_size - 1)\n",
    "\n",
    "    for i in range(height * width):\n",
    "        bev_map_t1[bev_y.flat[i], bev_x.flat[i]] = max(bev_map_t1[bev_y.flat[i], bev_x.flat[i]], depth.flat[i])\n",
    "        bev_map_t2[bev_y.flat[i], bev_x.flat[i]] = max(bev_map_t2[bev_y.flat[i], bev_x.flat[i]], depth.flat[i])\n",
    "\n",
    "    for obj, traj_t1, traj_t2 in zip(objects, trajectories_t1, trajectories_t2):\n",
    "        x_min, y_min, x_max, y_max = obj\n",
    "        x_min, y_min, x_max, y_max = x_min.item(), y_min.item(), x_max.item(), y_max.item()\n",
    "\n",
    "        traj_x_min_t1, traj_y_min_t1, traj_x_max_t1, traj_y_max_t1 = traj_t1\n",
    "        traj_x_min_t1, traj_y_min_t1, traj_x_max_t1, traj_y_max_t1 = traj_x_min_t1.item(), traj_y_min_t1.item(), traj_x_max_t1.item(), traj_y_max_t1.item()\n",
    "\n",
    "        traj_x_min_t2, traj_y_min_t2, traj_x_max_t2, traj_y_max_t2 = traj_t2\n",
    "        traj_x_min_t2, traj_y_min_t2, traj_x_max_t2, traj_y_max_t2 = traj_x_min_t2.item(), traj_y_min_t2.item(), traj_x_max_t2.item(), traj_y_max_t2.item()\n",
    "\n",
    "        obj_depths = depth_map[int(y_min):int(y_max), int(x_min):int(x_max)].detach().numpy()\n",
    "        if obj_depths.size > 0:\n",
    "            mean_depth = np.mean(obj_depths)\n",
    "        else:\n",
    "            mean_depth = 0\n",
    "        \n",
    "        X_min = (x_min - c_x) * mean_depth / f_x\n",
    "        X_max = (x_max - c_x) * mean_depth / f_x\n",
    "        Y_min = (y_min - c_y) * mean_depth / f_y\n",
    "        Y_max = (y_max - c_y) * mean_depth / f_y\n",
    "        Z = mean_depth\n",
    "        bev_x_min = int(100 + X_min * scale_factor)\n",
    "        bev_x_max = int(100 + X_max * scale_factor)\n",
    "        bev_y_min = int(100 + Y_min * scale_factor)\n",
    "        bev_y_max = int(100 + Y_max * scale_factor)\n",
    "\n",
    "        traj_X_min_t1 = (traj_x_min_t1 - c_x) * mean_depth / f_x\n",
    "        traj_X_max_t1 = (traj_x_max_t1 - c_x) * mean_depth / f_x\n",
    "        traj_Y_min_t1 = (traj_y_min_t1 - c_y) * mean_depth / f_y\n",
    "        traj_Y_max_t1 = (traj_y_max_t1 - c_y) * mean_depth / f_y\n",
    "        traj_Z_t1 = mean_depth\n",
    "        traj_bev_x_min_t1 = int(100 + traj_X_min_t1 * scale_factor)\n",
    "        traj_bev_x_max_t1 = int(100 + traj_X_max_t1 * scale_factor)\n",
    "        traj_bev_y_min_t1 = int(100 + traj_Y_min_t1 * scale_factor)\n",
    "        traj_bev_y_max_t1 = int(100 + traj_Y_max_t1 * scale_factor)\n",
    "\n",
    "        traj_X_min_t2 = (traj_x_min_t2 - c_x) * mean_depth / f_x\n",
    "        traj_X_max_t2 = (traj_x_max_t2 - c_x) * mean_depth / f_x\n",
    "        traj_Y_min_t2 = (traj_y_min_t2 - c_y) * mean_depth / f_y\n",
    "        traj_Y_max_t2 = (traj_y_max_t2 - c_y) * mean_depth / f_y\n",
    "        traj_Z_t2 = mean_depth\n",
    "        traj_bev_x_min_t2 = int(100 + traj_X_min_t2 * scale_factor)\n",
    "        traj_bev_x_max_t2 = int(100 + traj_X_max_t2 * scale_factor)\n",
    "        traj_bev_y_min_t2 = int(100 + traj_Y_min_t2 * scale_factor)\n",
    "        traj_bev_y_max_t2 = int(100 + traj_Y_max_t2 * scale_factor)\n",
    "\n",
    "        mean_depth = torch.tensor(mean_depth, dtype = torch.float32)\n",
    "        if mean_depth > 4:\n",
    "            # bev_map_t1[bev_y_min-10:bev_y_max+10, bev_x_min-10:bev_x_max+10] = 0 # bev_map_t1.max() - mean_depth\n",
    "            bev_map_t1[traj_bev_y_min_t1-10:traj_bev_y_max_t1+10, traj_bev_x_min_t1-10:traj_bev_x_max_t1+10] = 0 # bev_map_t1.max() - mean_depth\n",
    "            \n",
    "            # bev_map_t2[bev_y_min-10:bev_y_max+10, bev_x_min-10:bev_x_max+10] = 0 # bev_map_t2.max() - mean_depth\n",
    "            bev_map_t2[traj_bev_y_min_t2-10:traj_bev_y_max_t2+10, traj_bev_x_min_t2-10:traj_bev_x_max_t2+10] = 0 # bev_map_t2.max() - mean_depth\n",
    "\n",
    "    bev_map_t1 = np.clip(bev_map_t1, 0, 255)\n",
    "    bev_map_t1 = bev_map_t1.astype(np.uint8)\n",
    "\n",
    "    bev_map_t2 = np.clip(bev_map_t2, 0, 255)\n",
    "    bev_map_t2 = bev_map_t2.astype(np.uint8)\n",
    "\n",
    "    return bev_map_t1, bev_map_t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_waypoint(bev_map, x_mid):\n",
    "    bev_map = bev_map.max() - bev_map\n",
    "    bev_map = cv2.normalize(bev_map, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    bev_map = np.uint8(bev_map)\n",
    "    dist_from_x_mid = float('inf')\n",
    "\n",
    "    filtered_ones = []\n",
    "    for x in range(0,200,4):\n",
    "        for y in range(0,200,4):\n",
    "            b = bev_map[y,x]\n",
    "            if b < 80:\n",
    "                dist_from_x_mid = abs(x_mid - x)\n",
    "                x_for_filtering = x\n",
    "                y_for_filtering = y\n",
    "                # plt.scatter(x_for_filtering * 4, y_for_filtering * 3, color = 'black', marker='x')\n",
    "                filtered_ones.append([y_for_filtering, x_for_filtering])\n",
    "                # break\n",
    "\n",
    "    fof_cam = []\n",
    "    for i in filtered_ones:\n",
    "        fy = i[0]\n",
    "        fx = i[1]\n",
    "        fof_cam.append(bev_map[fy, fx])\n",
    "    fof_center = []\n",
    "    for i in range(0, len(filtered_ones)):\n",
    "        curr_x = filtered_ones[i][1]\n",
    "        fof_center.append(abs(x_mid - curr_x))\n",
    "\n",
    "    mini = min(fof_center)\n",
    "    mini_idxs = []\n",
    "    mini_val = float('inf')\n",
    "    for i in range(0, len(fof_center)):\n",
    "        if fof_center[i] <= mini_val:\n",
    "            mini_val = fof_center[i]\n",
    "    for i in range(0, len(fof_center)):\n",
    "        if(fof_center[i] == mini_val):\n",
    "            mini_idxs.append(i)\n",
    "    \n",
    "    max_x = -float('inf')\n",
    "    for i in mini_idxs:\n",
    "        cords = filtered_ones[i]\n",
    "        if cords[1] > max_x:\n",
    "            max_x = cords[1]\n",
    "            sel_y = cords[0]\n",
    "    cord_x = max_x\n",
    "    cord_y = sel_y\n",
    "\n",
    "    x_above = y_for_filtering\n",
    "    \n",
    "    # plt.scatter(cord_x * 4, cord_y * 3, color = 'black', marker='x')\n",
    "    # plt.imshow(depth_map.max() - depth_map, cmap = 'inferno')\n",
    "    # plt.grid(True)\n",
    "    # plt.show()\n",
    "\n",
    "    return cord_x * 4, cord_y * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_bev(bev_map):\n",
    "    bev_map = bev_map.max() - bev_map\n",
    "    bev_map = cv2.normalize(bev_map, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    bev_map = np.uint8(bev_map)\n",
    "    plt.figure(figsize = (3,3))\n",
    "    plt.imshow(bev_map, cmap=\"jet\", interpolation='nearest')\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_depth_map(depth_map, cord_x = 400, cord_y = 75, c = 'white'):\n",
    "    depth_map = depth_map.max() - depth_map\n",
    "    height, width = depth_map.shape\n",
    "    depth_at_cord = depth_map[cord_y-1, cord_x-1]\n",
    "    print(f\"Depth at co-ordinate ({cord_x}, {cord_y}) of the image: {depth_at_cord:.2f} meters\")\n",
    "    plt.scatter(cord_x, cord_y, color=c, marker='x')\n",
    "    plt.imshow(depth_map, cmap = 'inferno')\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Saving t1, t1 BEV details*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bev_for_A3(image):\n",
    "    # depth_map = image_to_depth_map(image)\n",
    "    # objects = object_detection(image)\n",
    "    # trajectories_t1, trajectories_t2 = future_positions(objects['boxes'], alpha = 0.3, diff = 0.3)\n",
    "    # bev_map_t1, bev_map_t2 = depth_map_to_bev(depth_map, objects['boxes'], trajectories_t1, trajectories_t2)\n",
    "    bev_map = bev_t1_t2_generation(image, image_to_bev_t1_t2_model).squeeze(0)\n",
    "    return bev_map[0], bev_map[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_waypoints(file, image_name):\n",
    "    with open(os.path.join('datasets','A1_dataset.json'), 'r') as rf:\n",
    "        for data in rf:\n",
    "            data = json.loads(data)\n",
    "            if data[0]['label'] == image_name:\n",
    "                x, y = data[0]['waypoints']\n",
    "                val = [x, y]\n",
    "                # print(f\"match found: {val}\")\n",
    "                return val\n",
    "\n",
    "    return [400, 600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_t1_t2_bev_dataset(images_dir, root, bev_t1_t2_file):\n",
    "\n",
    "    for i in range(0, 28):\n",
    "        k = \"0\" if i <= 9 else \"\"\n",
    "        curr_folder_images = f\"{images_dir}/Video_0{k}{i}\"\n",
    "        images_path = [file for file in os.listdir(curr_folder_images) if file.endswith(\".png\")]\n",
    "        images_path.sort()\n",
    "        images_path = images_path[::skip]\n",
    "        x_val = 20\n",
    "\n",
    "        for image_file in images_path:\n",
    "            if x_val >= 180:\n",
    "                x_val = 20\n",
    "            bevs = []\n",
    "            image_path = os.path.join(curr_folder_images, image_file)\n",
    "            image = Image.open(image_path)\n",
    "            image_np = np.array(image)\n",
    "            bev_map_t1, bev_map_t2 = generate_bev_for_A3(image)\n",
    "\n",
    "            wp_x_t1, wp_y_t1 = generate_waypoints(image_path, f\"datasets\\\\{image_file}\")\n",
    "            times = 1 + ((x_val - (wp_x_t1 / 4)) / 200)\n",
    "            wp_x_t2 = wp_x_t1 * times\n",
    "            wp_y_t2 = wp_y_t1 * times\n",
    "\n",
    "            output_image_path = os.path.join(curr_folder_images, image_file)\n",
    "            bevs.append({\n",
    "                \"target\": x_val,\n",
    "                \"image\": output_image_path,\n",
    "                \"bev_map_1\": bev_map_t1.tolist(),\n",
    "                \"bev_map_2\": bev_map_t2.tolist(),\n",
    "                \"way_points\": [wp_x_t1, wp_y_t1, wp_x_t2, wp_y_t2] # mapping that SOAB to both t1, t2 :)\n",
    "            })\n",
    "            # break\n",
    "\n",
    "            with open(os.path.join(root, bev_t1_t2_file), 'a') as wf:\n",
    "                json.dump(bevs, wf)\n",
    "                wf.write(\"\\n\")\n",
    "            break\n",
    "\n",
    "            x_val = x_val + 40\n",
    "\n",
    "        if(i<27):\n",
    "            print(f\"\\rProgress: {((i + 1) / 0.27):.2f} %\", end = \"\")\n",
    "        # break\n",
    "\n",
    "    print(f\"\\nDone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_t1_t2_bev_dataset(images_dir, root, bev_t1_t2_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_the_wps_A3(images_dir, output_dir):\n",
    "\n",
    "    folder_name = f\"{images_dir}/Video_000\"\n",
    "    file_1 = os.path.join(folder_name, \"v000_0000.png\")\n",
    "    bev_1, _ = generate_bev_for_A3(Image.open(file_1))\n",
    "    wp_x, wp_y = generate_waypoints(file_1, \"datasets\\\\v000_0000.png\")\n",
    "    wp_x = wp_x / 4\n",
    "    wp_y = wp_y / 3\n",
    "\n",
    "    plt.figure(figsize = (4,4))\n",
    "    plt.imshow(bev_1)\n",
    "    plt.scatter(wp_x, wp_y, c = 'red', marker = 'o')\n",
    "    plt.show()\n",
    "    \n",
    "    output_image_path = os.path.join(folder_name, \"v000_0000.png\")\n",
    "    bevs_1 = [{\n",
    "        \"image\": output_image_path,\n",
    "        \"bev_map\": bev_1.tolist(),\n",
    "        \"way_point\": [wp_x, wp_y]\n",
    "    }]\n",
    "\n",
    "    folder_name = f\"{images_dir}/Video_007\"\n",
    "    file_2 = os.path.join(folder_name, \"v007_0019.png\")\n",
    "    _, bev_2 = generate_bev_for_A3(Image.open(file_2))\n",
    "    wp_x, wp_y = generate_waypoints(file_2, \"datasets\\\\v007_0019.png\")\n",
    "    times = 1 + ((20 - wp_x) / 200)\n",
    "    wp_x = wp_x / 4\n",
    "    wp_y = wp_y / 3\n",
    "\n",
    "    plt.figure(figsize = (4,4))\n",
    "    plt.imshow(bev_2)\n",
    "    plt.scatter(wp_x, wp_y, c = 'red', marker = 'o')\n",
    "    plt.show()\n",
    "    \n",
    "    output_image_path = os.path.join(folder_name, file_2)\n",
    "    bevs_2 = [{\n",
    "        \"image\": output_image_path,\n",
    "        \"bev_map\": bev_2.tolist(),\n",
    "        \"way_point\": [wp_x, wp_y]\n",
    "    }]\n",
    "\n",
    "    # print(bevs_1[0]['image'], bevs_1[0]['way_point'])\n",
    "    # print(bevs_2[0]['image'], bevs_2[0]['way_point'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_the_wps_A3(images_dir, root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "getting dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BEV_t1_t2_Dataset(Dataset):\n",
    "    def __init__(self, root, bev_t1_t2_file):\n",
    "        self.data = []\n",
    "\n",
    "        with open(os.path.join(root, bev_t1_t2_file), \"r\") as wf:\n",
    "            for data in wf:\n",
    "                data = json.loads(data)\n",
    "                data = data[0]\n",
    "                # print(data.keys())\n",
    "\n",
    "                target = data['target']\n",
    "                image = data['image']\n",
    "                bev_map_1 = torch.tensor(data['bev_map_1'])\n",
    "                bev_map_2 = torch.tensor(data['bev_map_2']) # not used here\n",
    "                way_point = torch.tensor(data['way_points'])\n",
    "                \n",
    "                self.data.append([target, image, bev_map_1, way_point])\n",
    "                # break\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split indices for train and validation\n",
    "dataset = BEV_t1_t2_Dataset(root, bev_t1_t2_file)\n",
    "remaining_indices, test_indices = train_test_split(range(len(dataset)), test_size=0.1, random_state=42)\n",
    "\n",
    "test_dataset = Subset(dataset, test_indices)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, drop_last=True)\n",
    "\n",
    "train_indices, val_indices = train_test_split(remaining_indices, test_size=0.3, random_state=42)\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BEV_t1_t2 to image model (for faster operation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageToBEV_t1_t2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageToBEV_t1_t2, self).__init__()\n",
    "        self.backbone = models.resnet18(pretrained=True)\n",
    "        self.backbone = nn.Sequential(*list(self.backbone.children())[:-2])\n",
    "        self.conv1x1 = nn.Conv2d(512, 256, kernel_size=1)\n",
    "        \n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 512, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(512, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 2, kernel_size=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.conv1x1(x)\n",
    "        x = self.upsample(x)\n",
    "        x = F.interpolate(x, size=(200, 200), mode='bilinear', align_corners=False)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_to_bev_t1_t2_model = ImageToBEV_t1_t2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Imaget1t2Dataset(Dataset):\n",
    "    def __init__(self, bev_file):\n",
    "        self.data = []\n",
    "\n",
    "        with open(os.path.join(root, bev_file), \"r\") as wf:\n",
    "            for data in wf:\n",
    "                data = json.loads(data)\n",
    "                # print(data)\n",
    "                data = data[0]\n",
    "                image = np.array(Image.open(data['image']))\n",
    "                bev_map_1 = torch.tensor(data['bev_map_1'])\n",
    "                bev_map_2 = torch.tensor(data['bev_map_2'])\n",
    "\n",
    "                self.data.append([image, torch.stack((bev_map_1, bev_map_2), dim = 0)])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_to_bev_t1_t2_model = torch.load('models/ImageToBEV_t1_t2.pth', map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_train = image_to_bev_t1_t2_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imaget1t2set = Imaget1t2Dataset(bev_t1_t2_file)\n",
    "imaget1t2_loader = DataLoader(imaget1t2set, batch_size = batch_size, shuffle = True, pin_memory=True, drop_last=True)\n",
    "\n",
    "test_loader = imaget1t2_loader\n",
    "train_loader = imaget1t2_loader\n",
    "val_loader = imaget1t2_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bev_t1_t2_generation(image, model):\n",
    "    np_image = np.array(image, dtype = np.float32) / 255.0\n",
    "    tensor_image = torch.tensor(np_image, dtype=torch.float32)\n",
    "    # print(tensor_image.shape)\n",
    "    input_image = tensor_image.unsqueeze(0).permute(0,3,1,2)\n",
    "    return model(input_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, image_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in image_loader:\n",
    "        img = data[0]\n",
    "        bev = data[1]\n",
    "        optimizer.zero_grad()\n",
    "        pred = bev_t1_t2_generation(img, model)\n",
    "        loss = criterion(pred, bev.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(image_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, image_loader, optimizer, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in image_loader:\n",
    "            img = data[0]\n",
    "            bev = data[1]\n",
    "            pred = bev_t1_t2_generation(img, model)\n",
    "            loss = criterion(pred, bev.float())\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(image_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, image_loader, optimizer, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in image_loader:\n",
    "            img = data[0]\n",
    "            bev = data[1]\n",
    "            pred = bev_t1_t2_generation(img, model)\n",
    "            print(pred.shape)\n",
    "            print(bev.shape)\n",
    "            loss = criterion(pred, bev.float())\n",
    "            total_loss += loss.item()\n",
    "            break\n",
    "    return total_loss / len(image_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_save = best_model_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_to_save, 'models/ImageToBEV_t1_t2.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OUR CNN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_WPModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_WPModel, self).__init__()\n",
    "\n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "        self.resnet_features = nn.Sequential(*list(resnet.children())[:-1])\n",
    "\n",
    "        # self.conv_layers = nn.Sequential(\n",
    "        #     nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
    "        #     nn.BatchNorm2d(32),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.MaxPool2d(2, 2),\n",
    "        #     nn.Conv2d(32, 256, kernel_size=3, stride=1, padding=1),\n",
    "        #     nn.BatchNorm2d(256),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.MaxPool2d(2, 2),\n",
    "        #     nn.Conv2d(256, 1024, kernel_size=3, stride=1, padding=1),\n",
    "        #     nn.BatchNorm2d(1024),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.MaxPool2d(2, 2),\n",
    "        #     nn.Conv2d(1024, 256, kernel_size=3, stride=1, padding=1),\n",
    "        #     nn.BatchNorm2d(256),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.MaxPool2d(2, 2),\n",
    "        #     nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "        #     nn.BatchNorm2d(512),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.MaxPool2d(2, 2),\n",
    "        # )\n",
    "        \n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(512 + 1, 512),\n",
    "            # nn.Linear(512 * 6 * 6 + 1, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 4),\n",
    "            nn.Dropout(0.25),\n",
    "        )\n",
    "\n",
    "    def forward(self, pixel_values, target_loc):\n",
    "        x = self.resnet_features(pixel_values)\n",
    "        # x = self.conv_layers(pixel_values)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.cat((x, target_loc.unsqueeze(0).view(-1, 1)), dim=1)\n",
    "        output = self.fc_layers(x)\n",
    "        \n",
    "        return output.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_train = CNN_WPModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*--training set ups--*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wp_generation(values, locs, model):\n",
    "    return model(values, locs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, train_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        target_loc = data[0]\n",
    "        bev_maps = data[2].float()\n",
    "        waypoints = data[3].float()\n",
    "        bev_maps = bev_maps.unsqueeze(0).permute(1,0,2,3)\n",
    "        bev_maps = bev_maps.repeat(1, 3, 1, 1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pred = wp_generation(bev_maps, target_loc, model)\n",
    "        loss = criterion(pred, waypoints)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, val_loader, optimizer, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            target_loc = data[0]\n",
    "            bev_maps = data[2].float()\n",
    "            waypoints = data[3].float()\n",
    "            bev_maps = bev_maps.unsqueeze(0).permute(1,0,2,3)\n",
    "            bev_maps = bev_maps.repeat(1, 3, 1, 1)\n",
    "            \n",
    "            pred = wp_generation(bev_maps, target_loc, model)\n",
    "            loss = criterion(pred, waypoints)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataloader, optimizer, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            target_loc = data[0]\n",
    "            bev_maps = data[2].float()\n",
    "            waypoints = data[3].float()\n",
    "            bev_maps = bev_maps.unsqueeze(0).permute(1,0,2,3)\n",
    "            bev_maps = bev_maps.repeat(1, 3, 1, 1)\n",
    "            \n",
    "            pred = wp_generation(bev_maps, target_loc, model)\n",
    "            print(pred)\n",
    "            print(waypoints)\n",
    "            loss = criterion(pred, waypoints)\n",
    "            total_loss += loss.item()\n",
    "            break\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_save = best_model_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_to_save, 'models/CNN_WPModel.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_model = model_to_save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Management Section**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 6\n",
    "num_epochs = 50\n",
    "lr = 1e0\n",
    "optimizer = optim.Adam(model_to_train.parameters(), lr=lr)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = test(model_to_train, test_loader, optimizer, criterion)\n",
    "print(f\"test loss is: {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = test(model_to_train, test_loader, optimizer, criterion)\n",
    "print(f\"test loss is: {test_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_train_loss = float('inf')\n",
    "best_val_loss = float('inf')\n",
    "lr = 1e-2\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "  lr /= 1.1\n",
    "  optimizer = optim.Adam(model_to_train.parameters(), lr=lr)\n",
    "  \n",
    "  train_loss = training(model_to_train, train_loader, optimizer, criterion)\n",
    "  val_loss = validation(model_to_train, val_loader, optimizer, criterion)\n",
    "  \n",
    "  print(f\"\\rEpoch {epoch + 1}/{num_epochs}, train-Loss: {train_loss:.3f}, val-Loss: {val_loss:.3f} at lr: {lr:.7f}\", end = 20*\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_train_loss = float('inf')\n",
    "best_val_loss = float('inf')\n",
    "iterastions = 1\n",
    "for i in range(0,iterations):\n",
    "  lr = lr / 10\n",
    "  optimizer = optim.Adam(model_to_train.parameters(), lr=lr)\n",
    "  patience = 10\n",
    "  print(f\"Setting Learning rate to: {lr}\")\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "    best_model_trained = model_to_train\n",
    "    if(patience == 0):\n",
    "      break\n",
    "    \n",
    "    train_loss = training(model_to_train, train_loader, optimizer, criterion)\n",
    "    val_loss = validation(model_to_train, val_loader, optimizer, criterion)\n",
    "    \n",
    "    if(train_loss > (best_train_loss * 0.95) or (val_loss > (best_val_loss * 0.95))):\n",
    "      model_to_train = best_model_trained\n",
    "      patience -= 1\n",
    "    else:\n",
    "      best_train_loss = train_loss\n",
    "      best_val_loss = val_loss\n",
    "      patience = 10\n",
    "      # torch.save(model_to_train.state_dict(), '/content/drive/MyDrive/my_model.pth')\n",
    "    print(f\"\\rEpoch {epoch + 1}/{num_epochs}, train-Loss: {train_loss:.3f}, val-Loss: {val_loss:.3f}\", end = 20*\" \")#, patience: {patience}\", end = \"\")\n",
    "    # break\n",
    "  print(\"\\n\")\n",
    "  # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e0\n",
    "ctr = 0\n",
    "best_train_loss = float('inf')\n",
    "best_val_loss = float('inf')\n",
    "for i in range(0,iterations):\n",
    "  lr = lr / 10\n",
    "  optimizer = optim.Adam(model_to_train.parameters(), lr=lr)\n",
    "  patience = 10\n",
    "  ctr = ctr + 1\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "    best_model_trained = model_to_train\n",
    "    if(patience == 0):\n",
    "      break\n",
    "    \n",
    "    train_loss = training(model_to_train, train_loader, optimizer, criterion)\n",
    "    val_loss = validation(model_to_train, val_loader, optimizer, criterion)\n",
    "    \n",
    "    if(train_loss > (best_train_loss * 0.95) or (val_loss > (best_val_loss * 0.95))):\n",
    "      model_to_train = best_model_trained\n",
    "      patience =-1\n",
    "    else:\n",
    "      best_train_loss = train_loss\n",
    "      best_val_loss = val_loss\n",
    "      best_model_trained = model_to_train\n",
    "      patience = 10\n",
    "\n",
    "    print(f\"\\r{i + 1}> Epoch {epoch + 1}/{num_epochs}, train-Loss: {train_loss:.1f}, val-Loss: {val_loss:.1f}     \", end = \"\")\n",
    "  print(\"\")\n",
    "  torch.save(best_model_trained, f\"models/A1/update count - {ctr}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e1\n",
    "ctr = 0\n",
    "best_train_loss = float('inf')\n",
    "best_val_loss = float('inf')\n",
    "for i in range(0,iterations):\n",
    "  lr = lr / 10\n",
    "  optimizer = optim.Adam(model_to_train.parameters(), lr=lr)\n",
    "  patience = 5\n",
    "  ctr = ctr + 1\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "    best_model_trained = model_to_train\n",
    "    if(patience == 0):\n",
    "      break\n",
    "    \n",
    "    train_loss = training(model_to_train, train_loader, optimizer, criterion)\n",
    "    val_loss = validation(model_to_train, val_loader, optimizer, criterion)\n",
    "    \n",
    "    if(train_loss > (best_train_loss * 0.95) or (val_loss > (best_val_loss * 0.95))):\n",
    "      model_to_train = best_model_trained\n",
    "      patience -= 1\n",
    "    else:\n",
    "      best_train_loss = train_loss\n",
    "      best_val_loss = val_loss\n",
    "      best_model_trained = model_to_train\n",
    "      patience = 5\n",
    "\n",
    "    print(f\"\\r{i + 1}> Epoch {epoch + 1}/{num_epochs}, train-Loss: {train_loss:.1f}, val-Loss: {val_loss:.1f} (patience: {patience})     \", end = \"\")\n",
    "  print(\"\")\n",
    "  torch.save(best_model_trained, f\"models/A2/update count - {ctr}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e2\n",
    "ctr = 0\n",
    "best_train_loss = float('inf')\n",
    "best_val_loss = float('inf')\n",
    "for i in range(0,iterations):\n",
    "  lr = lr / 10\n",
    "  optimizer = optim.Adam(model_to_train.parameters(), lr=lr)\n",
    "  patience = 10\n",
    "  ctr = ctr + 1\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "    best_model_trained = model_to_train\n",
    "    if(patience == 0):\n",
    "      break\n",
    "    \n",
    "    train_loss = training(model_to_train, train_loader, optimizer, criterion)\n",
    "    val_loss = validation(model_to_train, val_loader, optimizer, criterion)\n",
    "    \n",
    "    if(train_loss > (best_train_loss * 0.95) or (val_loss > (best_val_loss * 0.95))):\n",
    "      model_to_train = best_model_trained\n",
    "      patience =-1\n",
    "    else:\n",
    "      best_train_loss = train_loss\n",
    "      best_val_loss = val_loss\n",
    "      best_model_trained = model_to_train\n",
    "      patience = 10\n",
    "\n",
    "    print(f\"\\r{i + 1}> Epoch {epoch + 1}/{num_epochs}, train-Loss: {train_loss:.1f}, val-Loss: {val_loss:.1f}     \", end = \"\")\n",
    "  print(\"\")\n",
    "  torch.save(best_model_trained, f\"models/A3/update count - {ctr}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------- REDUNENT CODE ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VitWPModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VitWPModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(512)\n",
    "        self.conv3 = nn.Conv2d(512, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "        self.conv5 = nn.Conv2d(512, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 512)\n",
    "        self.fc2 = nn.Linear(512, 2)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        b = pixel_values.shape[0]\n",
    "        x = torch.relu(self.conv1(pixel_values))\n",
    "        x = self.bn1(x)\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        \n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.bn2(x)\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = self.bn3(x)\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "\n",
    "        x = torch.relu(self.conv4(x))\n",
    "        x = self.bn4(x)\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "\n",
    "        x = torch.relu(self.conv5(x))\n",
    "        x = self.bn5(x)\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        output = self.fc2(x)\n",
    "        return output.view(b,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BEVToImage(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BEVToImage, self).__init__()\n",
    "        self.backbone = models.resnet18(pretrained=True)\n",
    "        self.backbone = nn.Sequential(*list(self.backbone.children())[:-2])\n",
    "        self.conv1x1 = nn.Conv2d(512, 256, kernel_size=1)\n",
    "        \n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 512, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(512, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 1, kernel_size=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.conv1x1(x)\n",
    "        x = self.upsample(x)\n",
    "        x = F.interpolate(x, size=(200, 200), mode='bilinear', align_corners=False)\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size=3):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.conv = nn.Conv2d(input_dim + hidden_dim, hidden_dim * 4, kernel_size, padding=padding)\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        h, c = hidden\n",
    "        combined = torch.cat([x, h], dim=1)\n",
    "        conv_out = self.conv(combined)\n",
    "        i, f, g, o = torch.chunk(conv_out, 4, dim=1)\n",
    "        i, f, g, o = torch.sigmoid(i), torch.sigmoid(f), torch.tanh(g), torch.sigmoid(o)\n",
    "        c_next = f * c + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "        return h_next, c_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim, sequence_length):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.output_dim = output_dim\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "        self.lstm_cells = nn.ModuleList([\n",
    "            ConvLSTMCell(input_dim if i == 0 else hidden_dim, hidden_dim)\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "        self.fc = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, c, h, w = x.size()\n",
    "        h_states = [torch.zeros(b, self.hidden_dim, h, w, device=x.device) for _ in range(self.num_layers)]\n",
    "        c_states = [torch.zeros(b, self.hidden_dim, h, w, device=x.device) for _ in range(self.num_layers)]\n",
    "        \n",
    "        for time_step in range(t):\n",
    "            inp = x[:, time_step]\n",
    "            for i, cell in enumerate(self.lstm_cells):\n",
    "                h_states[i], c_states[i] = cell(inp, (h_states[i], c_states[i]))\n",
    "                inp = h_states[i]\n",
    "        \n",
    "        out = torch.flatten(inp, start_dim=1)\n",
    "\n",
    "        if self.fc is None:\n",
    "            self.fc = nn.Linear(self.hidden_dim * h * w, self.output_dim * self.sequence_length).to(x.device)\n",
    "        output = self.fc(out).contiguous().view(b, -1, self.output_dim)\n",
    "        # print(output[0][0])\n",
    "        return output[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageToBEV_t1_t2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageToBEV_t1_t2, self).__init__()\n",
    "        self.backbone = models.resnet18(pretrained=True)\n",
    "        self.backbone = nn.Sequential(*list(self.backbone.children())[:-2])\n",
    "        self.conv1x1 = nn.Conv2d(512, 256, kernel_size=1)\n",
    "        \n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 512, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(512, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 1, kernel_size=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.conv1x1(x)\n",
    "        x = self.upsample(x)\n",
    "        x = F.interpolate(x, size=(200, 200), mode='bilinear', align_corners=False)\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_WPModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_WPModel, self).__init__()\n",
    "\n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "        self.resnet_features = nn.Sequential(*list(resnet.children())[:-1])\n",
    "\n",
    "        # self.conv_layers = nn.Sequential(\n",
    "        #     nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
    "        #     nn.BatchNorm2d(32),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.MaxPool2d(2, 2),\n",
    "        #     nn.Conv2d(32, 256, kernel_size=3, stride=1, padding=1),\n",
    "        #     nn.BatchNorm2d(256),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.MaxPool2d(2, 2),\n",
    "        #     nn.Conv2d(256, 1024, kernel_size=3, stride=1, padding=1),\n",
    "        #     nn.BatchNorm2d(1024),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.MaxPool2d(2, 2),\n",
    "        #     nn.Conv2d(1024, 256, kernel_size=3, stride=1, padding=1),\n",
    "        #     nn.BatchNorm2d(256),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.MaxPool2d(2, 2),\n",
    "        #     nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "        #     nn.BatchNorm2d(512),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.MaxPool2d(2, 2),\n",
    "        # )\n",
    "        \n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(512 + 1, 512),\n",
    "            # nn.Linear(512 * 6 * 6 + 1, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 4),\n",
    "            nn.Dropout(0.25),\n",
    "        )\n",
    "\n",
    "    def forward(self, pixel_values, target_loc):\n",
    "        x = self.resnet_features(pixel_values)\n",
    "        # x = self.conv_layers(pixel_values)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.cat((x, target_loc.unsqueeze(0).view(-1, 1)), dim=1)\n",
    "        output = self.fc_layers(x)\n",
    "        \n",
    "        return output.view(-1,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CarLA simulation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "semantic_label_classes = [\"Traffic Sign\", \"Building\", \"Fence\", \"Other\", \"Pedestrian\", \"Pole\", \"Road Line\", \"Road\", \"Sidewalk\", \"Vegetation\", \"Car\", \"Wall\", \"Unlabeled\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = carla.Client('localhost', 2000)\n",
    "world = client.get_world()\n",
    "blueprint_library = world.get_blueprint_library()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_all_actors():\n",
    "    actors = world.get_actors()\n",
    "    for actor in actors:\n",
    "        if 'vehicle' in actor.type_id:\n",
    "            actor.destroy()\n",
    "    world.tick()\n",
    "    # print(f\"\\rprevious ones destroyed\", end = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initiating_carla(world, blueprint_library):\n",
    "    ego_blueprint = blueprint_library.filter('*model3')[0]\n",
    "    spawn_points = world.get_map().get_spawn_points()[0]\n",
    "    global ego\n",
    "    ego = world.spawn_actor(ego_blueprint, spawn_points)\n",
    "\n",
    "    camera_bp = blueprint_library.find('sensor.camera.rgb')\n",
    "    camera_bp.set_attribute('image_size_x', '800')\n",
    "    camera_bp.set_attribute('image_size_y', '600')\n",
    "    camera_bp.set_attribute('fov', '110')\n",
    "    camera_transform = carla.Transform(carla.Location(x = 1.5, z = 2.4))\n",
    "    global camera\n",
    "    camera = world.spawn_actor(camera_bp, camera_transform, attach_to=ego)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_all_actors()\n",
    "initiating_carla(world, blueprint_library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EGO_relative_view(EGO, z=10):\n",
    "    view = EGO.get_transform().location + carla.Location(x=0, y=0, z=z)\n",
    "    return view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_cam_to_EGO(EGO, angle = 65, z=10):\n",
    "    spectator = world.get_spectator()\n",
    "    vehicle_transform = EGO.get_transform()\n",
    "    spectator.set_transform(\n",
    "        carla.Transform(\n",
    "            EGO_relative_view(EGO, z),\n",
    "            carla.Rotation(pitch = -angle, yaw = vehicle_transform.rotation.yaw, roll = 0)\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def camera_to_image(image):\n",
    "    global image_captured\n",
    "    array = np.frombuffer(image.raw_data, dtype=np.uint8)\n",
    "    array = array.reshape((image.height, image.width, 4))\n",
    "    image_captured = Image.fromarray(array[:, :, :3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_cam_to_EGO(ego)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "functions for model output to control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def steering(x, intended_x, s_power):\n",
    "    towards = intended_x - x\n",
    "    steer = towards * 0.0002\n",
    "    steer = steer * (1 + s_power) / 1.5\n",
    "    steer = max(-1, min(steer, 1))\n",
    "    return steer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TandB(y_mid, intended_y_mid, curr_y = 25):\n",
    "    if(intended_y_mid <= curr_y):\n",
    "        return 0, 1\n",
    "    intended_y_mid = intended_y_mid if intended_y_mid < y_mid else y_mid\n",
    "    return max(0.3, min((y_mid - intended_y_mid) / (2 * y_mid), 1)), 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def control_ego(steer, throttle, brake, rev=False):\n",
    "    # print(f\"stbr: {steer}, {throttle}, {brake}, {rev}\")\n",
    "    control = carla.VehicleControl()\n",
    "    # print(f\"un-initialized: {control}\")\n",
    "    control.steer = min(max(steer, -1), 1)\n",
    "    control.throttle = throttle\n",
    "    control.brake = brake\n",
    "    control.reverse = rev\n",
    "\n",
    "    # print(f\"control: {control}\")\n",
    "\n",
    "    return control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculation of metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*--route completion--*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_RC(intended_x_mid, ego_x_position, intended_y_mid, ego_y_position, th, d):\n",
    "    global dist_covered\n",
    "    if intended_y_mid > ego_y_position:\n",
    "        dist = math.sqrt(((intended_x_mid - ego_x_position) ** 2) + ((intended_y_mid - ego_y_position) ** 2))\n",
    "        if dist <= th:\n",
    "            dist_covered = dist_covered + d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*--infraction score--*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollisionSensor(object):\n",
    "    def __init__(self, vehicle):\n",
    "        self.vehicle = vehicle\n",
    "        self.world = vehicle.get_world()\n",
    "        self.collision_history = []\n",
    "\n",
    "        blueprint = self.world.get_blueprint_library().find('sensor.other.collision')\n",
    "        self.sensor = self.world.spawn_actor(blueprint, carla.Transform(), attach_to=self.vehicle)\n",
    "        self.sensor.listen(self._on_collision)\n",
    "\n",
    "    def _on_collision(self, event):\n",
    "        self.collision_history.append(event)\n",
    "        \n",
    "    def get_collision_count(self):\n",
    "        return len(self.collision_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_red_light_violation():\n",
    "    global red_light_violations\n",
    "    ego_position = ego.get_location()\n",
    "    traffic_lights = world.get_traffic_lights()\n",
    "    red_light_violations = False\n",
    "    for traffic_light in traffic_lights:\n",
    "        light_position = traffic_light.get_location()\n",
    "        distance_to_light = ego_position.distance_func(light_position)\n",
    "        if distance_to_light < 20.0:\n",
    "            if traffic_light.get_state() == carla.TrafficLightState.Red:\n",
    "                red_light_violations = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_lane_violation():\n",
    "    global lane_violations\n",
    "    if ego.is_outside_lane():\n",
    "        lane_violations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_tailgating():\n",
    "    global tailgating_violations\n",
    "    front_vehicle = ego.get_closest_vehicle_ahead()\n",
    "    if front_vehicle:\n",
    "        distance = ego.get_location().distance_func(front_vehicle.get_location())\n",
    "        speed = ego.get_velocity().length()\n",
    "        safe_distance = max(5, speed * 0.5)  # Example: 0.5 sec rule\n",
    "        if distance < safe_distance:\n",
    "            tailgating_violations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_illegal_turns():\n",
    "    global illegal_turn_violations\n",
    "    if ego.made_illegal_turn():  # Assuming a function to detect illegal turns\n",
    "        illegal_turn_violations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_stop_sign_violation():\n",
    "    global stop_sign_violations\n",
    "    stop_signs = world.get_stop_signs()\n",
    "    ego_position = ego.get_location()\n",
    "    for stop_sign in stop_signs:\n",
    "        stop_position = stop_sign.get_location()\n",
    "        distance = ego_position.distance_func(stop_position)\n",
    "        if distance < 10.0 and ego.get_velocity().length() > 0.5:  # Didn't stop\n",
    "            stop_sign_violations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_IS(speed_limit, red_light_violations, collision_sensor, lane_violations, tailgating_violations, illegal_turn_violations, stop_sign_violations):\n",
    "    global isc\n",
    "\n",
    "    velocity = ego.get_velocity()\n",
    "    speed = (velocity.x**2 + velocity.y**2 + velocity.z**2) ** 0.5 * 3.6\n",
    "    \n",
    "    if speed > speed_limit:\n",
    "        isc += 1\n",
    "        \n",
    "    if red_light_violations:\n",
    "        isc += 1\n",
    "\n",
    "    if collision_sensor.get_collision_count() > 0:\n",
    "        ego_drive(ego, True, th=1)\n",
    "        time.sleep(1.5)\n",
    "        ego_stop(ego)\n",
    "        isc += 10\n",
    "        collision_sensor.collision_history = []\n",
    "\n",
    "    if lane_violations:\n",
    "        isc += 2\n",
    "\n",
    "    if tailgating_violations:\n",
    "        isc += 2\n",
    "\n",
    "    if illegal_turn_violations:\n",
    "        isc += 2\n",
    "\n",
    "    if stop_sign_violations:\n",
    "        isc += 2\n",
    "\n",
    "    if isc > 100:\n",
    "        isc = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*--driving score--*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_DS(RC, IS):\n",
    "    return RC * (100 - IS) / 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "navigation section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_relative_to_ego(ego_transform, point_world):\n",
    "    ego_loc = ego_transform.location\n",
    "    ego_rot = ego_transform.rotation\n",
    "    yaw_rad = np.radians(ego_rot.yaw)\n",
    "    R = np.array([\n",
    "        [np.cos(-yaw_rad), -np.sin(-yaw_rad)],\n",
    "        [np.sin(-yaw_rad),  np.cos(-yaw_rad)]\n",
    "    ])\n",
    "    translated_point = np.array([point_world.x - ego_loc.x, point_world.y - ego_loc.y])\n",
    "    # print(\"from trte\", R, translated_point)\n",
    "    local_point = R @ translated_point\n",
    "    \n",
    "    return local_point[1], local_point[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ego_rotation(local_x, local_y):\n",
    "    # print(\"from ego_rotation\", local_x, local_y)\n",
    "    rel = math.degrees(math.atan(local_x / local_y))\n",
    "    return rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_points_from_gps(target):\n",
    "    ego_transform = ego.get_transform()\n",
    "    point_world = carla.Location(*target)\n",
    "\n",
    "    local_x, local_y = target_relative_to_ego(ego_transform, point_world)\n",
    "    # print(\"from tpfg\", local_x, local_y)\n",
    "    dist_from_ego = ((local_x ** 2) + (local_y ** 2)) ** (1/2)\n",
    "    rel = ego_rotation(local_x, local_y)\n",
    "\n",
    "    rel = max(min(rel, 50), -50)\n",
    "    local_y = max(min(local_y, 37.5), 0)\n",
    "\n",
    "    x_to_plot = 400 + (8 * rel)\n",
    "    y_to_plot = 600 - (25 * local_y)\n",
    "\n",
    "    return dist_from_ego, rel, x_to_plot, y_to_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view(angle, local_x, local_y):\n",
    "    theta = np.radians(angle)\n",
    "    rotated_x = local_x * np.cos(theta) - local_y * np.sin(theta)\n",
    "    rotated_y = local_x * np.sin(theta) + local_y * np.cos(theta)\n",
    "    return torch.tensor((rotated_x.item(), rotated_y.item()), dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bev_on_graph(local_x, local_y, scale=3, angle=0):\n",
    "    # print(\"from bef_fg\", local_x, local_y)\n",
    "\n",
    "    rotated_x, rotated_y = view(angle, local_x, local_y)\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.scatter(0, 0, color='red', label='origin', s=100)\n",
    "    plt.scatter(rotated_x, rotated_y, color='blue', label='target')\n",
    "    plt.xlim(-scale * 400, scale * 400)\n",
    "    plt.ylim(-scale * 400, scale * 400)\n",
    "    plt.xlabel(\"X (meters)\")\n",
    "    plt.ylabel(\"Y (meters)\")\n",
    "    plt.gca().invert_xaxis()\n",
    "    # plt.gca().invert_yaxis()\n",
    "    plt.axhline(0, color='black',linewidth=1)\n",
    "    plt.axvline(0, color='black',linewidth=1)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_road_map(vehicle, world):\n",
    "    curr_loc = vehicle.get_location()\n",
    "    map = world.get_map()\n",
    "    wps = map.get_waypoint(curr_loc,project_to_road=True, lane_type=(carla.LaneType.Driving | carla.LaneType.Sidewalk))\n",
    "\n",
    "    my_geolocation = map.transform_to_geolocation(vehicle.get_transform().location)\n",
    "    all_map_waypoints = map.generate_waypoints(2.0)\n",
    "    waypoints_on_map = map.get_topology()\n",
    "\n",
    "    return waypoints_on_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_func(loc1, loc2):\n",
    "    # print(loc1, loc2)\n",
    "    return math.sqrt((loc1[0] - loc2[0]) ** 2 + (loc1[1] - loc2[1]) ** 2 + (loc1[2] - loc2[2]) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_graph(junction_waypoints): #ego_loc, junction_waypoints):\n",
    "    graph = {}\n",
    "    first = junction_waypoints[0][0].transform.location\n",
    "    to = tuple((first.x, first.y, first.z))\n",
    "    \n",
    "    # ego_loc_tuple = tuple((ego_loc.x, ego_loc.y, ego_loc.z)).\n",
    "    # graph[ego_loc_tuple] = []\n",
    "    # graph[ego_loc_tuple].append((to, distance_func(ego_loc_tuple, to)))\n",
    "    # makes a list for starting position as key\n",
    "    # then appends all points connected to that key with distance 0.0\n",
    "    \n",
    "    for start_wp, end_wp in junction_waypoints:\n",
    "        start_loc = (start_wp.transform.location.x, start_wp.transform.location.y, start_wp.transform.location.z)\n",
    "        end_loc = (end_wp.transform.location.x, end_wp.transform.location.y, end_wp.transform.location.z)\n",
    "        dist = distance_func(start_loc, end_loc)\n",
    "        \n",
    "        if start_loc not in graph:\n",
    "            graph[start_loc] = []\n",
    "            \n",
    "        graph[start_loc].append((end_loc, dist))\n",
    "\n",
    "    d = float('inf')\n",
    "    for key in graph.keys():\n",
    "        to_loc, dis = graph[key][0]\n",
    "        if dis < d:\n",
    "            nearest_i = to_loc\n",
    "            d = dis\n",
    "    if nearest_i not in graph.keys():\n",
    "        graph[nearest_i] = []\n",
    "        \n",
    "    # graph[nearest_i].append((ego_loc_tuple, distance_func((ego_loc.x, ego_loc.y, ego_loc.z), nearest_i)))\n",
    "    \n",
    "    for k in graph.keys():\n",
    "        val = graph[k][0]\n",
    "    # print(f\"ego_loc: {ego_loc}\")\n",
    "    # print(f\"nearest_i: {nearest_i}\")\n",
    "\n",
    "    return graph, nearest_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dijkstra(graph, source):\n",
    "    dist = {v: float('inf') for v in graph}\n",
    "    prev = {v: None for v in graph}\n",
    "    dist[source] = 0\n",
    "\n",
    "    priority_queue = [(0, source)]\n",
    "    explored = set()\n",
    "\n",
    "    while priority_queue:\n",
    "        current_dist, v = heapq.heappop(priority_queue)\n",
    "\n",
    "        if v in explored:\n",
    "            continue\n",
    "        explored.add(v)\n",
    "\n",
    "        for neighbor, length in graph[v]:\n",
    "            if dist[v] + length < dist[neighbor]:\n",
    "                dist[neighbor] = dist[v] + length\n",
    "                prev[neighbor] = v\n",
    "                heapq.heappush(priority_queue, (dist[neighbor], neighbor))\n",
    "\n",
    "    return dist, prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_path(previous_nodes, start, goal):\n",
    "    path = []\n",
    "    current_node = goal[0][0]\n",
    "\n",
    "    if current_node not in previous_nodes:\n",
    "        return []\n",
    "\n",
    "    while current_node is not None:\n",
    "        path.append(current_node)\n",
    "        current_node = previous_nodes.get(current_node)\n",
    "        if current_node == start:\n",
    "            path.append(start)\n",
    "            break\n",
    "        if current_node is None:\n",
    "            return []\n",
    "\n",
    "    path.reverse()\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_from_line(p1, p2, i):\n",
    "    x0, y0, _ = i\n",
    "    x1, y1, _ = p1\n",
    "    x2, y2, _ = p2\n",
    "    num = abs((y2 - y1) * x0 - (x2 - x1) * y0 + x2 * y1 - y2 * x1)\n",
    "    den = math.sqrt((y2 - y1) ** 2 + (x2 - x1) ** 2)\n",
    "    if den == 0:\n",
    "        return float('inf'), p1\n",
    "    dist = num / den\n",
    "    X = p1 if ((x0 - x1)**2 + (y0 - y1)**2) < ((x0 - x2)**2 + (y0 - y2)**2) else p2\n",
    "    \n",
    "    return dist, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(ego):\n",
    "    loc = ego.get_transform().location\n",
    "    loc = tuple((loc.x, loc.y, loc.z))\n",
    "    global path\n",
    "    if len(path) > 0:\n",
    "        dist = distance_func(loc, path[0])\n",
    "        # print(f\"{dist} apart\")\n",
    "        if dist < 10:\n",
    "            path.pop(0)\n",
    "            update(ego)\n",
    "        else:\n",
    "            new_path = [loc]\n",
    "            for i in path:\n",
    "                new_path.append(i)\n",
    "            path = new_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shortest_path(ego, initiation, count=7):\n",
    "    # curr_ego_loc = ego.get_transform().location\n",
    "    # initiation = tuple((curr_ego_loc.x, curr_ego_loc.y, curr_ego_loc.z))\n",
    "    global path\n",
    "\n",
    "    wps, nearest_i = make_graph(get_road_map(ego, world))#curr_ego_loc, get_road_map(ego, world))\n",
    "    ind = 0\n",
    "    for k in wps.keys():\n",
    "        ind += 1\n",
    "    i=0\n",
    "    for k in wps.keys():\n",
    "        if i < ind-1:\n",
    "            goal = wps[k]\n",
    "    \n",
    "    distances, prev_nodes = dijkstra(wps, (nearest_i[0], nearest_i[1], nearest_i[2]))\n",
    "    path = reconstruct_path(prev_nodes, (nearest_i[0], nearest_i[1], nearest_i[2]), goal)\n",
    "\n",
    "    mini = float('inf')\n",
    "    X = None\n",
    "    for i in range(0, len(path)-2, 2):\n",
    "        d, x = dist_from_line(path[i], path[i+1], initiation)\n",
    "        if d < mini:\n",
    "            mini = d\n",
    "            index = i\n",
    "            X = x\n",
    "    path = path[index:]\n",
    "    path[0] = initiation\n",
    "    path[1] = X\n",
    "    # print(path)\n",
    "    update(ego)\n",
    "\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ego_drive(ego, rev=False, steer=0.0, th=0.5, brake=0.0, control=carla.VehicleControl()):\n",
    "    update(ego)\n",
    "    control.reverse = rev\n",
    "    control.steer = steer\n",
    "    control.throttle = th\n",
    "    control.brake = brake\n",
    "    ego.apply_control(control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ego_stop(ego, control=carla.VehicleControl()):\n",
    "    control.steer = 0\n",
    "    control.throttle = 0.0\n",
    "    control.brake = 1\n",
    "    ego.apply_control(control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ego_rev(ego, st=1, wait=3.0):\n",
    "    global path\n",
    "    global dfp_time\n",
    "    control = carla.VehicleControl()\n",
    "    _, angle, x, y = target_points_from_gps(path[1])\n",
    "    ego_drive(ego, True, st, control=control, th=0.6, brake=0.0)\n",
    "\n",
    "    while True:\n",
    "        _, angle, x, y = target_points_from_gps(path[1])\n",
    "        if angle == 50 or angle == -50:\n",
    "            break\n",
    "        if time.time() - dfp_time > wait:\n",
    "            break\n",
    "    ego_drive(ego, False, -st, control=control, th=0.6, brake=0.0)\n",
    "    temp = time.time()\n",
    "    while True:\n",
    "        _, angle, x, y = target_points_from_gps(path[1])\n",
    "        if angle <= 10 and angle >= -10:\n",
    "            break\n",
    "        if time.time() - temp > wait:\n",
    "            break\n",
    "    ego_stop(ego, control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def status():\n",
    "    global ego\n",
    "    global goal\n",
    "    update(ego)\n",
    "\n",
    "    curr_ego_loc = ego.get_transform().location\n",
    "    initiation = tuple((curr_ego_loc.x, curr_ego_loc.y, curr_ego_loc.z))\n",
    "    \n",
    "    d = distance_func(goal, initiation)\n",
    "    if d < 15.0:\n",
    "        print('reached')\n",
    "        global start_time\n",
    "        start_time = -float('inf')\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def steering_2(x, intended_x, s_power):\n",
    "    towards = intended_x - x\n",
    "    steer = towards * 0.0002\n",
    "    steer = steer * (1 + s_power) / 1.5\n",
    "    steer = max(-1, min(steer, 1))\n",
    "    return steer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def steering(x, intended_x, s_power):\n",
    "    global steer\n",
    "    steer_2 = steering_2(x, intended_x, s_power)\n",
    "    # print(f\"\\r{steer_2:.2f} by gps, {steer:.2f} by env\", end = \"\")\n",
    "    return (steer + steer_2) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_from_point(ego, point):\n",
    "    ego_loc = ego.get_transform().location\n",
    "    ego_loc = tuple((ego_loc.x, ego_loc.y, ego_loc.z))\n",
    "    return distance_func(ego_loc, point), ego_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def go_rev(point, td=0.4):\n",
    "    global dfp\n",
    "    global prev_dfp\n",
    "    global dfp_time\n",
    "    global angle\n",
    "    global prev_point\n",
    "    \n",
    "    dfp, loc = distance_from_point(ego, point)\n",
    "    time_diff = time.time() - dfp_time\n",
    "    # print(f\"\\r{dfp:.2f} dfp; {prev_dfp:.2f} prev_dfp; {time_diff:.2f} time_diff; {angle:.2f} angle\", end = 50 * \" \")\n",
    "    if time_diff > td:\n",
    "        dfp_time = time.time()\n",
    "        if dfp <= prev_dfp:\n",
    "            st = 1.0\n",
    "            if angle < 0.0:\n",
    "                st = -st\n",
    "            ego_rev(ego, st, 10)\n",
    "            prev_point = loc\n",
    "        prev_dfp = dfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def details():\n",
    "    global steer\n",
    "    steer = 0.0\n",
    "    global dfp\n",
    "    dfp = 0.0\n",
    "    global prev_dfp\n",
    "    prev_dfp = 0.0\n",
    "    global prev_point\n",
    "    prev_point = ego.get_transform().location\n",
    "    prev_point = tuple((prev_point.x, prev_point.y, prev_point.z))\n",
    "    global dfp_time\n",
    "    dfp_time = time.time()\n",
    "\n",
    "    global path\n",
    "    path = []\n",
    "    global curr_ego_loc\n",
    "    curr_ego_loc = ego.get_transform().location\n",
    "    global initiation\n",
    "    initiation = tuple((curr_ego_loc.x, curr_ego_loc.y, curr_ego_loc.z))\n",
    "    path = shortest_path(ego, initiation)\n",
    "    global original_path\n",
    "    original_path = path\n",
    "    global goal\n",
    "    goal = path[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manage_steer(td=0.4, locs = [400, 300], reverse = False):\n",
    "    # if path != original_path:\n",
    "    #     print(f\"\\r{path}\", end=\"\")\n",
    "    #     original_path = path\n",
    "    locs = torch.tensor(locs, dtype = torch.float32)\n",
    "    s = status()\n",
    "    if s == True:\n",
    "        return locs\n",
    "    else:\n",
    "        if path and len(path) >= 1:\n",
    "            if reverse:\n",
    "                return\n",
    "            else:\n",
    "                global prev_point\n",
    "                go_rev(prev_point, td)\n",
    "                _,angle,_,_ = target_points_from_gps(path[1])\n",
    "        # print(f\"\\rtowards {path[1]} with an angle of {angle}\", end=\"\")\n",
    "        partiality = angle / 50.0\n",
    "        global steer\n",
    "        steer = partiality * 1.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "controlling based on models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*--self model--*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_emb(image):\n",
    "    return processor(text=semantic_label_classes,\n",
    "    images=image,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    )['pixel_values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_output_generation(image, vit_wp_model):\n",
    "    manage_steer()\n",
    "    pixel_values = image_to_emb(image).squeeze(1)\n",
    "    outputs = vit_wp_model(pixel_values)\n",
    "    op = outputs.squeeze()\n",
    "    op[0] = op[0] * 1.05\n",
    "    # op[0] = op[0] * 1.8\n",
    "    op[1] = 600 - op[1]\n",
    "    return op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_wp(way_p_1, top_w,top_h):\n",
    "    way_p = way_p_1\n",
    "\n",
    "    mini_f = float('inf')\n",
    "    maxi_f = -float('inf')\n",
    "    for i in way_p[0]:\n",
    "        if(maxi_f < i[0]):\n",
    "            maxi_f = i[0]\n",
    "        if(mini_f > i[0]):\n",
    "            mini_f = i[0]\n",
    "        \n",
    "    mini_s = float('inf')\n",
    "    maxi_s = -float('inf')\n",
    "    for i in way_p[1]:\n",
    "        if(maxi_s < i[1]):\n",
    "            maxi_s = i[1]\n",
    "        if(mini_s > i[1]):\n",
    "            mini_s = i[1]\n",
    "        \n",
    "    for i in way_p:\n",
    "        i[0] = (i[0] - mini_f) / (maxi_f - mini_f)\n",
    "        i[1] = (i[1] - mini_s) / (maxi_s - mini_s)\n",
    "        i[0] *= top_h\n",
    "        i[1] *= top_w\n",
    "\n",
    "    return way_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [],
   "source": [
    "def points():\n",
    "    return 400, 139"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [],
   "source": [
    "steer = 0.0\n",
    "dfp = 0.0\n",
    "prev_dfp = 0.0\n",
    "angle = 0.0\n",
    "prev_point = ego.get_transform().location\n",
    "prev_point = tuple((prev_point.x, prev_point.y, prev_point.z))\n",
    "dfp_time = time.time()\n",
    "\n",
    "path = []\n",
    "curr_ego_loc = ego.get_transform().location\n",
    "initiation = tuple((curr_ego_loc.x, curr_ego_loc.y, curr_ego_loc.z))\n",
    "path = shortest_path(ego, initiation)\n",
    "original_path = path\n",
    "goal = path[-1]\n",
    "\n",
    "details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [],
   "source": [
    "th = 15\n",
    "\n",
    "driver_model = VitWPModel()\n",
    "driver_model = torch.load('models/VitWPModel.pth', map_location=torch.device('cpu'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*--vit_wp_model--*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_emb(image):\n",
    "    return processor(text=semantic_label_classes,\n",
    "    images=image,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    )['pixel_values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_output_generation(image, vit_wp_model):\n",
    "    # manage_steer()\n",
    "    pixel_values = image_to_emb(image).squeeze(1)\n",
    "    outputs = vit_wp_model(pixel_values)\n",
    "    op = outputs.squeeze()\n",
    "    op[0] = op[0] * 1.05\n",
    "    # op[0] = op[0] * 1.8\n",
    "    op[1] = 600 - op[1]\n",
    "    return op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_wp(way_p_1, top_w,top_h):\n",
    "    way_p = way_p_1\n",
    "\n",
    "    mini_f = float('inf')\n",
    "    maxi_f = -float('inf')\n",
    "    for i in way_p[0]:\n",
    "        if(maxi_f < i[0]):\n",
    "            maxi_f = i[0]\n",
    "        if(mini_f > i[0]):\n",
    "            mini_f = i[0]\n",
    "        \n",
    "    mini_s = float('inf')\n",
    "    maxi_s = -float('inf')\n",
    "    for i in way_p[1]:\n",
    "        if(maxi_s < i[1]):\n",
    "            maxi_s = i[1]\n",
    "        if(mini_s > i[1]):\n",
    "            mini_s = i[1]\n",
    "        \n",
    "    for i in way_p:\n",
    "        i[0] = (i[0] - mini_f) / (maxi_f - mini_f)\n",
    "        i[1] = (i[1] - mini_s) / (maxi_s - mini_s)\n",
    "        i[0] *= top_h\n",
    "        i[1] *= top_w\n",
    "\n",
    "    return way_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [],
   "source": [
    "def points():\n",
    "    return 400, 139"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [],
   "source": [
    "th = 15\n",
    "\n",
    "details()\n",
    "driver_model = VitWPModel()\n",
    "driver_model = torch.load('models/VitWPModel.pth', map_location=torch.device('cpu'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*--conv_LSTM_model--*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_output_generation(image, driver_model):\n",
    "    # manage_steer()\n",
    "    bev = image_to_bev_model(torch.tensor(np.array(image, dtype = np.float32) / 255.0, dtype=torch.float32).unsqueeze(0).permute(0,3,1,2)).unsqueeze(0)\n",
    "    bev = bev.unsqueeze(0)\n",
    "    outputs = driver_model(bev.unsqueeze(0))\n",
    "    op = outputs.squeeze()\n",
    "    op[0] = 1.1 * op[0] / 50\n",
    "    op[1] = op[1] / 50\n",
    "    # op[1] = 600 - op[1]\n",
    "    return op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [],
   "source": [
    "def points():\n",
    "    return 400, 324.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\devas\\.conda\\envs\\carlaSim\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\devas\\.conda\\envs\\carlaSim\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "image_to_bev_model = BEVToImage()\n",
    "image_to_bev_model = torch.load('models/ImageToBEV.pth', map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [],
   "source": [
    "th = 750\n",
    "\n",
    "details()\n",
    "input_dim, hidden_dim, num_layers, output_dim, seq_length = 1, 64, 2, 2, 5\n",
    "driver_model = ConvLSTM(input_dim, hidden_dim, num_layers, output_dim, seq_length)\n",
    "driver_model = torch.load('models/ConvLSTM.pth', map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*--CNN_wp_model--*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_output_generation(image, model, locs = [400, 300]):\n",
    "    manage_steer()\n",
    "    locs = torch.tensor(locs, dtype = torch.float32)\n",
    "\n",
    "    np_image = np.array(image, dtype = np.float32) / 255.0\n",
    "    tensor_image = torch.tensor(np_image, dtype=torch.float32)\n",
    "    tensor_image = tensor_image.permute(2,0,1).unsqueeze(0)\n",
    "    bev_op = image_to_bev_t1_t2_model(tensor_image)\n",
    "\n",
    "    bev_maps = bev_op.squeeze()\n",
    "    bev = bev_maps.unsqueeze(0).permute(1,0,2,3).repeat(1, 3, 1, 1)\n",
    "\n",
    "    # print(bev.shape, locs.shape)\n",
    "    outputs = driver_model(bev, locs)\n",
    "    # print(outputs)\n",
    "    op = torch.tensor([outputs[:,0].mean(),outputs[:,1].mean()])\n",
    "    op[0] = op[0] * 1.8\n",
    "    op[1] = op[1]\n",
    "\n",
    "    # print(op)\n",
    "    return op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [],
   "source": [
    "def points():\n",
    "    return 400, 140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_to_bev_t1_t2_model = ImageToBEV_t1_t2()\n",
    "image_to_bev_t1_t2_model = torch.load('models/ImageToBEV_t1_t2.pth', map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [],
   "source": [
    "steer = 0.0\n",
    "dfp = 0.0\n",
    "prev_dfp = 0.0\n",
    "angle = 0.0\n",
    "prev_point = ego.get_transform().location\n",
    "prev_point = tuple((prev_point.x, prev_point.y, prev_point.z))\n",
    "dfp_time = time.time()\n",
    "\n",
    "path = []\n",
    "curr_ego_loc = ego.get_transform().location\n",
    "initiation = tuple((curr_ego_loc.x, curr_ego_loc.y, curr_ego_loc.z))\n",
    "path = shortest_path(ego, initiation)\n",
    "original_path = path\n",
    "goal = path[-1]\n",
    "\n",
    "details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [],
   "source": [
    "th = 200\n",
    "\n",
    "driver_model = CNN_WPModel()\n",
    "driver_model = torch.load('models/CNN_WPModel.pth', map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RUNNING THE SIMULATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_all_actors()\n",
    "initiating_carla(world, blueprint_library)\n",
    "set_cam_to_EGO(ego)\n",
    "details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # print(\"started\")\n",
    "\n",
    "    prev_image_captured = None\n",
    "    image_captured = None\n",
    "    init_pos = ego.get_transform().location\n",
    "\n",
    "    RC = 0\n",
    "    IS = 0\n",
    "    DS = 0\n",
    "    dist_covered = 0\n",
    "    distance = 0\n",
    "    isc = 0\n",
    "    speed_limit = 50\n",
    "\n",
    "    count_dist = 0\n",
    "    dist_counter = 0\n",
    "\n",
    "    red_light_violations = False\n",
    "    collision_sensor = CollisionSensor(ego)\n",
    "    lane_violations = False\n",
    "    tailgating_violations = False\n",
    "    illegal_turn_violations = False\n",
    "    stop_sign_violations = False\n",
    "    set_cam_to_EGO(ego)\n",
    "    camera.listen(camera_to_image) #changes image_captured globally\n",
    "\n",
    "    while image_captured is None:\n",
    "        world.tick()\n",
    "        start_time = time.time()\n",
    "\n",
    "    while True:\n",
    "        world.wait_for_tick()\n",
    "        if not prev_image_captured == image_captured:\n",
    "            dist_counter += 1\n",
    "\n",
    "            scene_x, scene_y = 224, 224\n",
    "            ego_x_mid = scene_x/2\n",
    "            ego_y_mid = scene_y/2\n",
    "\n",
    "            outputs = model_output_generation(image_captured, driver_model)\n",
    "            waypoint = abs(outputs.detach().numpy())\n",
    "\n",
    "            ego_x_position, ego_y_position = points()\n",
    "            \n",
    "            intended_x = waypoint[0]\n",
    "            intended_y = waypoint[1]\n",
    "\n",
    "            if dist_counter % 2 == 0:\n",
    "                init_pos = ego.get_transform().location\n",
    "                d = 0\n",
    "            else:\n",
    "                curr_pos = ego.get_transform().location\n",
    "                dx = abs(curr_pos.x - init_pos.x)\n",
    "                dy = abs(curr_pos.y - init_pos.y)\n",
    "                dz = abs(curr_pos.z - init_pos.z)\n",
    "                d = math.sqrt((dx ** 2) + (dy ** 2) + (dz ** 2))\n",
    "                distance += d\n",
    "                evaluate_RC(intended_x, ego_x_position, intended_y, ego_y_position, th, d)\n",
    "                evaluate_IS(speed_limit, red_light_violations, collision_sensor, lane_violations, tailgating_violations, illegal_turn_violations, stop_sign_violations)\n",
    "\n",
    "                count_dist += math.sqrt(((intended_x - ego_x_position) ** 2) + ((intended_y - ego_y_position) ** 2))\n",
    "\n",
    "                # if distance > 0:\n",
    "                #     print(f\"\\r{dist_covered:.2f} / {distance:.2f}, {isc:.2f}        \", end = \"\")\n",
    "                    # break\n",
    "\n",
    "            throttle, brake = TandB(ego_y_mid, intended_y, ego_y_position)\n",
    "            velocity = ego.get_velocity()\n",
    "            speed = math.sqrt((velocity.x**2 + velocity.y**2 + velocity.z**2)) / 5\n",
    "            s_power = (1 - brake) * (1 - speed)\n",
    "            steer = steering(ego_x_position, intended_x, s_power)\n",
    "\n",
    "            print(f\"\\rgoing towards: ({intended_x:.1f}, {intended_y:.1f}), with b:{brake}, t:{throttle:.1f}, s:{steer:.1f}      \", end = \"\")\n",
    "\n",
    "            ego.apply_control(control_ego(steer, throttle * 1.5, brake))\n",
    "            set_cam_to_EGO(ego)\n",
    "            \n",
    "            red_light_violations = False\n",
    "            collision_sensor = CollisionSensor(ego)\n",
    "            lane_violations = False\n",
    "            tailgating_violations = False\n",
    "            illegal_turn_violations = False\n",
    "            stop_sign_violations = False\n",
    "\n",
    "        if time.time() >= start_time + 1200:\n",
    "            break\n",
    "        prev_image_captured = image_captured\n",
    "        \n",
    "    if(distance > 0):\n",
    "        RC = dist_covered * 100 / distance\n",
    "        IS = isc\n",
    "        DS = evaluate_DS(RC, IS)\n",
    "\n",
    "    print(f\"\\nRC is: {RC:.2f}, IS is: {IS:.2f}, DS is: {DS:.2f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n{e}\")\n",
    "finally:\n",
    "    camera.stop()\n",
    "\n",
    "    ego.apply_control(control_ego(0,0,1))\n",
    "\n",
    "    # remove_all_actors()\n",
    "    # initiating_carla(world, blueprint_library)\n",
    "    # set_cam_to_EGO(ego, 65)\n",
    "\n",
    "    # print(\"stopped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # print(\"started\")\n",
    "\n",
    "    prev_image_captured = None\n",
    "    image_captured = None\n",
    "    init_pos = ego.get_transform().location\n",
    "\n",
    "    RC = 0\n",
    "    IS = 0\n",
    "    DS = 0\n",
    "    dist_covered = 0\n",
    "    distance = 0\n",
    "    isc = 0\n",
    "    speed_limit = 50\n",
    "\n",
    "    count_dist = 0\n",
    "    dist_counter = 0\n",
    "\n",
    "    red_light_violations = False\n",
    "    collision_sensor = CollisionSensor(ego)\n",
    "    lane_violations = False\n",
    "    tailgating_violations = False\n",
    "    illegal_turn_violations = False\n",
    "    stop_sign_violations = False\n",
    "    set_cam_to_EGO(ego)\n",
    "    camera.listen(camera_to_image) #changes image_captured globally\n",
    "    \n",
    "    yp = 85\n",
    "    xp = 33\n",
    "\n",
    "    while image_captured is None:\n",
    "        world.tick()\n",
    "        start_time = time.time()\n",
    "\n",
    "    while True:\n",
    "        world.wait_for_tick()\n",
    "        if not prev_image_captured == image_captured:\n",
    "            dist_counter += 1\n",
    "\n",
    "            scene_x, scene_y = 224, 224\n",
    "            ego_x_mid = scene_x/2\n",
    "            ego_y_mid = scene_y/2\n",
    "\n",
    "            outputs = model_output_generation(image_captured, driver_model)\n",
    "            waypoint = abs(outputs.detach().numpy())\n",
    "\n",
    "            ego_x_position, ego_y_position = points()\n",
    "            \n",
    "            intended_x = waypoint[0]\n",
    "            intended_y = waypoint[1]\n",
    "\n",
    "            if dist_counter % 2 == 0:\n",
    "                init_pos = ego.get_transform().location\n",
    "                d = 0\n",
    "            else:\n",
    "                curr_pos = ego.get_transform().location\n",
    "                dx = abs(curr_pos.x - init_pos.x)\n",
    "                dy = abs(curr_pos.y - init_pos.y)\n",
    "                dz = abs(curr_pos.z - init_pos.z)\n",
    "                d = math.sqrt((dx ** 2) + (dy ** 2) + (dz ** 2))\n",
    "                distance += d\n",
    "                evaluate_RC(intended_x, ego_x_position, intended_y, ego_y_position, th, d)\n",
    "                evaluate_IS(speed_limit, red_light_violations, collision_sensor, lane_violations, tailgating_violations, illegal_turn_violations, stop_sign_violations)\n",
    "\n",
    "                count_dist += math.sqrt(((intended_x - ego_x_position) ** 2) + ((intended_y - ego_y_position) ** 2))\n",
    "\n",
    "                # if distance > 0:\n",
    "                #     print(f\"\\r{dist_covered:.2f} / {distance:.2f}, {isc:.2f}        \", end = \"\")\n",
    "                    # break\n",
    "\n",
    "            throttle, brake = TandB(ego_y_mid, intended_y, ego_y_position)\n",
    "            velocity = ego.get_velocity()\n",
    "            speed = math.sqrt((velocity.x**2 + velocity.y**2 + velocity.z**2)) / 5\n",
    "            s_power = (1 - brake) * (1 - speed)\n",
    "            steer = steering(ego_x_position, intended_x, s_power)\n",
    "\n",
    "            print(f\"\\r{decision_of_reverse(image_captured)}, {decision_of_side_change(image_captured)}\", end=5*\" \")\n",
    "            # print(f\"\\r{decision_of_reverse(image_captured, yp, xp)}, {decision_of_side_change(image_captured, yp, xp)}\", end=5*\" \")\n",
    "            \n",
    "            # decision_of_reverse(image_captured, yp, xp // 3)\n",
    "            # decision_of_side_change(image_captured, yp, xp // 3)\n",
    "            \n",
    "            # print(f\"\\rgoing towards: ({intended_x:.1f}, {intended_y:.1f}), with b:{brake}, t:{throttle:.1f}, s:{steer:.1f}      \", end = \"\")\n",
    "\n",
    "            ego.apply_control(control_ego(steer, throttle * 1.5, brake))\n",
    "            set_cam_to_EGO(ego)\n",
    "            \n",
    "            red_light_violations = False\n",
    "            collision_sensor = CollisionSensor(ego)\n",
    "            lane_violations = False\n",
    "            tailgating_violations = False\n",
    "            illegal_turn_violations = False\n",
    "            stop_sign_violations = False\n",
    "\n",
    "        if time.time() >= start_time + 1200:\n",
    "            break\n",
    "        prev_image_captured = image_captured\n",
    "        \n",
    "    if(distance > 0):\n",
    "        RC = dist_covered * 100 / distance\n",
    "        IS = isc\n",
    "        DS = evaluate_DS(RC, IS)\n",
    "\n",
    "    print(f\"\\nRC is: {RC:.2f}, IS is: {IS:.2f}, DS is: {DS:.2f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n{e}\")\n",
    "finally:\n",
    "    camera.stop()\n",
    "\n",
    "    ego.apply_control(control_ego(0,0,1))\n",
    "\n",
    "    # remove_all_actors()\n",
    "    # initiating_carla(world, blueprint_library)\n",
    "    # set_cam_to_EGO(ego, 65)\n",
    "\n",
    "    # print(\"stopped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_all_actors()\n",
    "initiating_carla(world, blueprint_library)\n",
    "set_cam_to_EGO(ego)\n",
    "details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RC is: 81.51, IS is: 10.00, DS is: 73.36; 5+\n",
    "# RC is: 89.83, IS is: 20.00, DS is: 71.86; 21+\n",
    "# RC is: 41.85, IS is: 60.00, DS is: 16.74; 75+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "metric evaluation (final model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_count = 1\n",
    "averages_fm = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [],
   "source": [
    "averages_m_temp = [0,0,0]\n",
    "for my_i in range (0, iter_count):\n",
    "    try:\n",
    "        # print(\"started\")\n",
    "\n",
    "        prev_image_captured = None\n",
    "        image_captured = None\n",
    "        init_pos = ego.get_transform().location\n",
    "\n",
    "        RC = 0\n",
    "        IS = 0\n",
    "        DS = 0\n",
    "        dist_covered = 0\n",
    "        distance = 0\n",
    "        isc = 0\n",
    "        speed_limit = 50\n",
    "\n",
    "        count_dist = 0\n",
    "        dist_counter = 0\n",
    "                \n",
    "        red_light_violations = False\n",
    "        collision_sensor = CollisionSensor(ego)\n",
    "        lane_violations = False\n",
    "        tailgating_violations = False\n",
    "        illegal_turn_violations = False\n",
    "        stop_sign_violations = False\n",
    "        set_cam_to_EGO(ego)\n",
    "        camera.listen(camera_to_image) #changes image_captured globally\n",
    "        \n",
    "        yp = 78\n",
    "        xp = 25\n",
    "\n",
    "        while image_captured is None:\n",
    "            world.tick()\n",
    "            start_time = time.time()\n",
    "\n",
    "        while True:\n",
    "            world.wait_for_tick()\n",
    "            if not prev_image_captured == image_captured:\n",
    "                dist_counter += 1\n",
    "\n",
    "                scene_x, scene_y = 224, 224\n",
    "                ego_x_mid = scene_x/2\n",
    "                ego_y_mid = scene_y/2\n",
    "\n",
    "                outputs = model_output_generation(image_captured, driver_model)\n",
    "                waypoint = abs(outputs.detach().numpy())\n",
    "\n",
    "                ego_x_position, ego_y_position = points()\n",
    "                \n",
    "                intended_x = waypoint[0]\n",
    "                intended_y = waypoint[1]\n",
    "\n",
    "                if dist_counter % 2 == 0:\n",
    "                    init_pos = ego.get_transform().location\n",
    "                    d = 0\n",
    "                else:\n",
    "                    curr_pos = ego.get_transform().location\n",
    "                    dx = abs(curr_pos.x - init_pos.x)\n",
    "                    dy = abs(curr_pos.y - init_pos.y)\n",
    "                    dz = abs(curr_pos.z - init_pos.z)\n",
    "                    d = math.sqrt((dx ** 2) + (dy ** 2) + (dz ** 2))\n",
    "                    distance += d\n",
    "                    evaluate_RC(intended_x, ego_x_position, intended_y, ego_y_position, th, d)\n",
    "                    evaluate_IS(speed_limit, red_light_violations, collision_sensor, lane_violations, tailgating_violations, illegal_turn_violations, stop_sign_violations)\n",
    "\n",
    "                    count_dist += math.sqrt(((intended_x - ego_x_position) ** 2) + ((intended_y - ego_y_position) ** 2))\n",
    "\n",
    "                    # if distance > 0:\n",
    "                    #     print(f\"\\r{dist_covered:.2f} / {distance:.2f}, {isc:.2f}        \", end = \"\")\n",
    "                        # break\n",
    "\n",
    "                throttle, brake = TandB(ego_y_mid, intended_y, ego_y_position)\n",
    "                velocity = ego.get_velocity()\n",
    "                speed = math.sqrt((velocity.x**2 + velocity.y**2 + velocity.z**2)) / 5\n",
    "                s_power = (1 - brake) * (1 - speed)\n",
    "                steer = steering(ego_x_position, intended_x, s_power)\n",
    "                \n",
    "                if decision_of_reverse(image_captured, yp, xp):\n",
    "                    steer = decision_of_side_change(image_captured, yp, xp)\n",
    "                # print(f\"\\rgoing towards: ({intended_x:.1f}, {intended_y:.1f}), with b:{brake}, t:{throttle:.1f}, s:{steer:.1f}      \", end = \"\")\n",
    "\n",
    "                ego.apply_control(control_ego(steer, throttle, brake))\n",
    "                set_cam_to_EGO(ego)\n",
    "                \n",
    "                red_light_violations = False\n",
    "                collision_sensor = CollisionSensor(ego)\n",
    "                lane_violations = False\n",
    "                tailgating_violations = False\n",
    "                illegal_turn_violations = False\n",
    "                stop_sign_violations = False\n",
    "\n",
    "            if time.time() >= start_time + 120:\n",
    "                break\n",
    "            prev_image_captured = image_captured\n",
    "            \n",
    "        if(distance > 0):\n",
    "            RC = dist_covered * 100 / distance\n",
    "            IS = isc\n",
    "            DS = evaluate_DS(RC, IS)\n",
    "        averages_m_temp[0] += RC\n",
    "        averages_m_temp[1] += IS\n",
    "        averages_m_temp[2] += DS\n",
    "        # print(f\"\\nRC is: {RC:.2f}, IS is: {IS:.2f}, DS is: {DS:.2f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n{e}\")\n",
    "    finally:\n",
    "        camera.stop()\n",
    "\n",
    "        ego.apply_control(control_ego(0,0,1))\n",
    "\n",
    "        remove_all_actors()\n",
    "        initiating_carla(world, blueprint_library)\n",
    "        set_cam_to_EGO(ego, 65)\n",
    "\n",
    "        # print(\"stopped\")\n",
    "\n",
    "for my_i in range(len(averages_m_temp)):\n",
    "    averages_m_temp[my_i] = averages_m_temp[my_i] / iter_count\n",
    "averages_fm.append(averages_m_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [],
   "source": [
    "averages_m_temp = [0,0,0]\n",
    "for my_i in range (0, iter_count):\n",
    "    try:\n",
    "        # print(\"started\")\n",
    "\n",
    "        prev_image_captured = None\n",
    "        image_captured = None\n",
    "        init_pos = ego.get_transform().location\n",
    "\n",
    "        RC = 0\n",
    "        IS = 0\n",
    "        DS = 0\n",
    "        dist_covered = 0\n",
    "        distance = 0\n",
    "        isc = 0\n",
    "        speed_limit = 50\n",
    "\n",
    "        count_dist = 0\n",
    "        dist_counter = 0\n",
    "                \n",
    "        red_light_violations = False\n",
    "        collision_sensor = CollisionSensor(ego)\n",
    "        lane_violations = False\n",
    "        tailgating_violations = False\n",
    "        illegal_turn_violations = False\n",
    "        stop_sign_violations = False\n",
    "        set_cam_to_EGO(ego)\n",
    "        camera.listen(camera_to_image) #changes image_captured globally\n",
    "        \n",
    "        yp = 78\n",
    "        xp = 25\n",
    "\n",
    "        while image_captured is None:\n",
    "            world.tick()\n",
    "            start_time = time.time()\n",
    "\n",
    "        while True:\n",
    "            world.wait_for_tick()\n",
    "            if not prev_image_captured == image_captured:\n",
    "                dist_counter += 1\n",
    "\n",
    "                scene_x, scene_y = 224, 224\n",
    "                ego_x_mid = scene_x/2\n",
    "                ego_y_mid = scene_y/2\n",
    "\n",
    "                outputs = model_output_generation(image_captured, driver_model)\n",
    "                waypoint = abs(outputs.detach().numpy())\n",
    "\n",
    "                ego_x_position, ego_y_position = points()\n",
    "                \n",
    "                intended_x = waypoint[0]\n",
    "                intended_y = waypoint[1]\n",
    "\n",
    "                if dist_counter % 2 == 0:\n",
    "                    init_pos = ego.get_transform().location\n",
    "                    d = 0\n",
    "                else:\n",
    "                    curr_pos = ego.get_transform().location\n",
    "                    dx = abs(curr_pos.x - init_pos.x)\n",
    "                    dy = abs(curr_pos.y - init_pos.y)\n",
    "                    dz = abs(curr_pos.z - init_pos.z)\n",
    "                    d = math.sqrt((dx ** 2) + (dy ** 2) + (dz ** 2))\n",
    "                    distance += d\n",
    "                    evaluate_RC(intended_x, ego_x_position, intended_y, ego_y_position, th, d)\n",
    "                    evaluate_IS(speed_limit, red_light_violations, collision_sensor, lane_violations, tailgating_violations, illegal_turn_violations, stop_sign_violations)\n",
    "\n",
    "                    count_dist += math.sqrt(((intended_x - ego_x_position) ** 2) + ((intended_y - ego_y_position) ** 2))\n",
    "\n",
    "                    # if distance > 0:\n",
    "                    #     print(f\"\\r{dist_covered:.2f} / {distance:.2f}, {isc:.2f}        \", end = \"\")\n",
    "                        # break\n",
    "\n",
    "                throttle, brake = TandB(ego_y_mid, intended_y, ego_y_position)\n",
    "                velocity = ego.get_velocity()\n",
    "                speed = math.sqrt((velocity.x**2 + velocity.y**2 + velocity.z**2)) / 5\n",
    "                s_power = (1 - brake) * (1 - speed)\n",
    "                steer = steering(ego_x_position, intended_x, s_power)\n",
    "                \n",
    "                # print(f\"\\rgoing towards: ({intended_x:.1f}, {intended_y:.1f}), with b:{brake}, t:{throttle:.1f}, s:{steer:.1f}      \", end = \"\")\n",
    "\n",
    "                ego.apply_control(control_ego(steer, throttle, brake))\n",
    "                set_cam_to_EGO(ego)\n",
    "                \n",
    "                red_light_violations = False\n",
    "                collision_sensor = CollisionSensor(ego)\n",
    "                lane_violations = False\n",
    "                tailgating_violations = False\n",
    "                illegal_turn_violations = False\n",
    "                stop_sign_violations = False\n",
    "\n",
    "            if time.time() >= start_time + 120:\n",
    "                break\n",
    "            prev_image_captured = image_captured\n",
    "            \n",
    "        if(distance > 0):\n",
    "            RC = dist_covered * 100 / distance\n",
    "            IS = isc\n",
    "            DS = evaluate_DS(RC, IS)\n",
    "        averages_m_temp[0] += RC\n",
    "        averages_m_temp[1] += IS\n",
    "        averages_m_temp[2] += DS\n",
    "        # print(f\"\\nRC is: {RC:.2f}, IS is: {IS:.2f}, DS is: {DS:.2f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n{e}\")\n",
    "    finally:\n",
    "        camera.stop()\n",
    "\n",
    "        ego.apply_control(control_ego(0,0,1))\n",
    "\n",
    "        remove_all_actors()\n",
    "        initiating_carla(world, blueprint_library)\n",
    "        set_cam_to_EGO(ego, 65)\n",
    "\n",
    "        # print(\"stopped\")\n",
    "\n",
    "for my_i in range(len(averages_m_temp)):\n",
    "    averages_m_temp[my_i] = averages_m_temp[my_i] / iter_count\n",
    "averages_fm.append(averages_m_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [],
   "source": [
    "averages_m_temp = [0,0,0]\n",
    "for my_i in range (0, iter_count):\n",
    "    try:\n",
    "        # print(\"started\")\n",
    "\n",
    "        prev_image_captured = None\n",
    "        image_captured = None\n",
    "        init_pos = ego.get_transform().location\n",
    "\n",
    "        RC = 0\n",
    "        IS = 0\n",
    "        DS = 0\n",
    "        dist_covered = 0\n",
    "        distance = 0\n",
    "        isc = 0\n",
    "        speed_limit = 50\n",
    "\n",
    "        count_dist = 0\n",
    "        dist_counter = 0\n",
    "                \n",
    "        red_light_violations = False\n",
    "        collision_sensor = CollisionSensor(ego)\n",
    "        lane_violations = False\n",
    "        tailgating_violations = False\n",
    "        illegal_turn_violations = False\n",
    "        stop_sign_violations = False\n",
    "        set_cam_to_EGO(ego)\n",
    "        camera.listen(camera_to_image) #changes image_captured globally\n",
    "\n",
    "        while image_captured is None:\n",
    "            world.tick()\n",
    "            start_time = time.time()\n",
    "\n",
    "        while True:\n",
    "            world.wait_for_tick()\n",
    "            if not prev_image_captured == image_captured:\n",
    "                dist_counter += 1\n",
    "\n",
    "                scene_x, scene_y = 224, 224\n",
    "                ego_x_mid = scene_x/2\n",
    "                ego_y_mid = scene_y/2\n",
    "\n",
    "                outputs = model_output_generation(image_captured, driver_model)\n",
    "                waypoint = abs(outputs.detach().numpy())\n",
    "\n",
    "                ego_x_position, ego_y_position = points()\n",
    "                \n",
    "                intended_x = waypoint[0]\n",
    "                intended_y = waypoint[1]\n",
    "\n",
    "                if dist_counter % 2 == 0:\n",
    "                    init_pos = ego.get_transform().location\n",
    "                    d = 0\n",
    "                else:\n",
    "                    curr_pos = ego.get_transform().location\n",
    "                    dx = abs(curr_pos.x - init_pos.x)\n",
    "                    dy = abs(curr_pos.y - init_pos.y)\n",
    "                    dz = abs(curr_pos.z - init_pos.z)\n",
    "                    d = math.sqrt((dx ** 2) + (dy ** 2) + (dz ** 2))\n",
    "                    distance += d\n",
    "                    evaluate_RC(intended_x, ego_x_position, intended_y, ego_y_position, th, d)\n",
    "                    evaluate_IS(speed_limit, red_light_violations, collision_sensor, lane_violations, tailgating_violations, illegal_turn_violations, stop_sign_violations)\n",
    "\n",
    "                    count_dist += math.sqrt(((intended_x - ego_x_position) ** 2) + ((intended_y - ego_y_position) ** 2))\n",
    "\n",
    "                    # if distance > 0:\n",
    "                    #     print(f\"\\r{dist_covered:.2f} / {distance:.2f}, {isc:.2f}        \", end = \"\")\n",
    "                        # break\n",
    "\n",
    "                throttle, brake = TandB(ego_y_mid, intended_y, ego_y_position)\n",
    "                velocity = ego.get_velocity()\n",
    "                speed = math.sqrt((velocity.x**2 + velocity.y**2 + velocity.z**2)) / 5\n",
    "                s_power = (1 - brake) * (1 - speed)\n",
    "                steer = steering(ego_x_position, intended_x, s_power)\n",
    "\n",
    "                # print(f\"\\rgoing towards: ({intended_x:.1f}, {intended_y:.1f}), with b:{brake}, t:{throttle:.1f}, s:{steer:.1f}      \", end = \"\")\n",
    "\n",
    "                ego.apply_control(control_ego(steer, throttle, brake))\n",
    "                set_cam_to_EGO(ego)\n",
    "                \n",
    "                red_light_violations = False\n",
    "                collision_sensor = CollisionSensor(ego)\n",
    "                lane_violations = False\n",
    "                tailgating_violations = False\n",
    "                illegal_turn_violations = False\n",
    "                stop_sign_violations = False\n",
    "\n",
    "            if time.time() >= start_time + 120:\n",
    "                break\n",
    "            prev_image_captured = image_captured\n",
    "            \n",
    "        if(distance > 0):\n",
    "            RC = dist_covered * 100 / distance\n",
    "            IS = isc\n",
    "            DS = evaluate_DS(RC, IS)\n",
    "        averages_m_temp[0] += RC\n",
    "        averages_m_temp[1] += IS\n",
    "        averages_m_temp[2] += DS\n",
    "        # print(f\"\\nRC is: {RC:.2f}, IS is: {IS:.2f}, DS is: {DS:.2f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n{e}\")\n",
    "    finally:\n",
    "        camera.stop()\n",
    "\n",
    "        ego.apply_control(control_ego(0,0,1))\n",
    "\n",
    "        remove_all_actors()\n",
    "        initiating_carla(world, blueprint_library)\n",
    "        set_cam_to_EGO(ego, 65)\n",
    "\n",
    "        # print(\"stopped\")\n",
    "\n",
    "for my_i in range(len(averages_m_temp)):\n",
    "    averages_m_temp[my_i] = averages_m_temp[my_i] / iter_count\n",
    "averages_fm.append(averages_m_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [],
   "source": [
    "averages_m_temp = [0,0,0]\n",
    "for my_i in range (0, iter_count):\n",
    "    details()\n",
    "    try:\n",
    "        # print(\"started\")\n",
    "\n",
    "        prev_image_captured = None\n",
    "        image_captured = None\n",
    "        init_pos = ego.get_transform().location\n",
    "\n",
    "        RC = 0\n",
    "        IS = 0\n",
    "        DS = 0\n",
    "        dist_covered = 0\n",
    "        distance = 0\n",
    "        isc = 0\n",
    "        speed_limit = 50\n",
    "\n",
    "        count_dist = 0\n",
    "        dist_counter = 0\n",
    "                \n",
    "        red_light_violations = False\n",
    "        collision_sensor = CollisionSensor(ego)\n",
    "        lane_violations = False\n",
    "        tailgating_violations = False\n",
    "        illegal_turn_violations = False\n",
    "        stop_sign_violations = False\n",
    "        set_cam_to_EGO(ego)\n",
    "        camera.listen(camera_to_image) #changes image_captured globally\n",
    "\n",
    "        while image_captured is None:\n",
    "            world.tick()\n",
    "            start_time = time.time()\n",
    "\n",
    "        while True:\n",
    "            world.wait_for_tick()\n",
    "            if not prev_image_captured == image_captured:\n",
    "                dist_counter += 1\n",
    "\n",
    "                scene_x, scene_y = 224, 224\n",
    "                ego_x_mid = scene_x/2\n",
    "                ego_y_mid = scene_y/2\n",
    "\n",
    "                outputs = model_output_generation(image_captured, driver_model)\n",
    "                waypoint = abs(outputs.detach().numpy())\n",
    "\n",
    "                ego_x_position, ego_y_position = points()\n",
    "                \n",
    "                intended_x = waypoint[0]\n",
    "                intended_y = waypoint[1]\n",
    "\n",
    "                if dist_counter % 2 == 0:\n",
    "                    init_pos = ego.get_transform().location\n",
    "                    d = 0\n",
    "                else:\n",
    "                    curr_pos = ego.get_transform().location\n",
    "                    dx = abs(curr_pos.x - init_pos.x)\n",
    "                    dy = abs(curr_pos.y - init_pos.y)\n",
    "                    dz = abs(curr_pos.z - init_pos.z)\n",
    "                    d = math.sqrt((dx ** 2) + (dy ** 2) + (dz ** 2))\n",
    "                    distance += d\n",
    "                    evaluate_RC(intended_x, ego_x_position, intended_y, ego_y_position, th, d)\n",
    "                    evaluate_IS(speed_limit, red_light_violations, collision_sensor, lane_violations, tailgating_violations, illegal_turn_violations, stop_sign_violations)\n",
    "\n",
    "                    count_dist += math.sqrt(((intended_x - ego_x_position) ** 2) + ((intended_y - ego_y_position) ** 2))\n",
    "\n",
    "                    # if distance > 0:\n",
    "                    #     print(f\"\\r{dist_covered:.2f} / {distance:.2f}, {isc:.2f}        \", end = \"\")\n",
    "                        # break\n",
    "\n",
    "                throttle, brake = TandB(ego_y_mid, intended_y, ego_y_position)\n",
    "                velocity = ego.get_velocity()\n",
    "                speed = math.sqrt((velocity.x**2 + velocity.y**2 + velocity.z**2)) / 5\n",
    "                s_power = (1 - brake) * (1 - speed)\n",
    "                steer = steering(ego_x_position, intended_x, s_power)\n",
    "\n",
    "                # print(f\"\\rgoing towards: ({intended_x:.1f}, {intended_y:.1f}), with b:{brake}, t:{throttle:.1f}, s:{steer:.1f}      \", end = \"\")\n",
    "\n",
    "                ego.apply_control(control_ego(steer, throttle, brake))\n",
    "                set_cam_to_EGO(ego)\n",
    "                \n",
    "                red_light_violations = False\n",
    "                collision_sensor = CollisionSensor(ego)\n",
    "                lane_violations = False\n",
    "                tailgating_violations = False\n",
    "                illegal_turn_violations = False\n",
    "                stop_sign_violations = False\n",
    "\n",
    "            if time.time() >= start_time + 120:\n",
    "                break\n",
    "            prev_image_captured = image_captured\n",
    "            \n",
    "        if(distance > 0):\n",
    "            RC = dist_covered * 100 / distance\n",
    "            IS = isc\n",
    "            DS = evaluate_DS(RC, IS)\n",
    "        averages_m_temp[0] += RC\n",
    "        averages_m_temp[1] += IS\n",
    "        averages_m_temp[2] += DS\n",
    "        # print(f\"\\nRC is: {RC:.2f}, IS is: {IS:.2f}, DS is: {DS:.2f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n{e}\")\n",
    "    finally:\n",
    "        camera.stop()\n",
    "\n",
    "        ego.apply_control(control_ego(0,0,1))\n",
    "\n",
    "        remove_all_actors()\n",
    "        initiating_carla(world, blueprint_library)\n",
    "        set_cam_to_EGO(ego, 65)\n",
    "\n",
    "        # print(\"stopped\")\n",
    "\n",
    "for my_i in range(len(averages_m_temp)):\n",
    "    averages_m_temp[my_i] = averages_m_temp[my_i] / iter_count\n",
    "averages_fm.append(averages_m_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'o' is not a valid color value.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[638], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(averages_fm)):\n\u001b[1;32m----> 8\u001b[0m     \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverages_fm\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAlgorithm \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMetrics\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValues\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\devas\\.conda\\envs\\carlaSim\\lib\\site-packages\\matplotlib\\pyplot.py:2439\u001b[0m, in \u001b[0;36mbar\u001b[1;34m(x, height, width, bottom, align, data, **kwargs)\u001b[0m\n\u001b[0;32m   2435\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mbar)\n\u001b[0;32m   2436\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbar\u001b[39m(\n\u001b[0;32m   2437\u001b[0m         x, height, width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m, bottom\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m, align\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcenter\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   2438\u001b[0m         data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m-> 2439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbar\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2440\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbottom\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbottom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malign\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2441\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\devas\\.conda\\envs\\carlaSim\\lib\\site-packages\\matplotlib\\__init__.py:1474\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1471\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m   1472\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1473\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1474\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1476\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1477\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[0;32m   1478\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
      "File \u001b[1;32mc:\\Users\\devas\\.conda\\envs\\carlaSim\\lib\\site-packages\\matplotlib\\axes\\_axes.py:2433\u001b[0m, in \u001b[0;36mAxes.bar\u001b[1;34m(self, x, height, width, bottom, align, **kwargs)\u001b[0m\n\u001b[0;32m   2431\u001b[0m linewidth \u001b[38;5;241m=\u001b[39m itertools\u001b[38;5;241m.\u001b[39mcycle(np\u001b[38;5;241m.\u001b[39matleast_1d(linewidth))\n\u001b[0;32m   2432\u001b[0m hatch \u001b[38;5;241m=\u001b[39m itertools\u001b[38;5;241m.\u001b[39mcycle(np\u001b[38;5;241m.\u001b[39matleast_1d(hatch))\n\u001b[1;32m-> 2433\u001b[0m color \u001b[38;5;241m=\u001b[39m itertools\u001b[38;5;241m.\u001b[39mchain(itertools\u001b[38;5;241m.\u001b[39mcycle(\u001b[43mmcolors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_rgba_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolor\u001b[49m\u001b[43m)\u001b[49m),\n\u001b[0;32m   2434\u001b[0m                         \u001b[38;5;66;03m# Fallback if color == \"none\".\u001b[39;00m\n\u001b[0;32m   2435\u001b[0m                         itertools\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m   2436\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m edgecolor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2437\u001b[0m     edgecolor \u001b[38;5;241m=\u001b[39m itertools\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\devas\\.conda\\envs\\carlaSim\\lib\\site-packages\\matplotlib\\colors.py:471\u001b[0m, in \u001b[0;36mto_rgba_array\u001b[1;34m(c, alpha)\u001b[0m\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(c, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 471\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mc\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m is not a valid color value.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(c) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m4\u001b[39m), \u001b[38;5;28mfloat\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: 'o' is not a valid color value."
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "categories = [\"Route Completion\", \"Infraction Score\", \"Driving Score\"]\n",
    "color = [\"orange\", \"b\", \"g\",\"r\"]\n",
    "\n",
    "x = np.arange(len(categories))\n",
    "width = 0.1\n",
    "plt.figure(figsize=(8, 5))\n",
    "for i in range(0, len(averages_fm)):\n",
    "    plt.bar(x + (width * (i - 1)), averages_fm[i], width=width, label=f\"Algorithm {i+1}\", color=color[i])\n",
    "plt.xlabel(\"Metrics\")\n",
    "plt.ylabel(\"Values\")\n",
    "plt.ylim(0, 100)\n",
    "plt.xticks(x, categories)\n",
    "plt.title(\"Comparison of RC, IS, and DS for the Four Algorithms\")\n",
    "plt.legend()\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.75)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[41.13276661031995, 10.0, 37.01948994928795]\n",
      "[94.52484785820145, 0.0, 94.52484785820144]\n",
      "[48.218020587283085, 70.0, 14.465406176184926]\n",
      "[64.58470940023372, 20.0, 51.66776752018697]\n"
     ]
    }
   ],
   "source": [
    "for averages_fm_i in averages_fm:\n",
    "    print(averages_fm_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\"Route Completion\", \"Infraction Score\", \"Driving Score\"]\n",
    "color = [\"b\", \"g\",\"r\"]\n",
    "\n",
    "x = np.arange(len(categories))\n",
    "width = 0.1\n",
    "plt.figure(figsize=(8, 5))\n",
    "for i in range(0, len(averages_fm)):\n",
    "    plt.bar(x + (width * (i - 1)), averages_fm[i], width=width, label=f\"Algorithm {i+1}\", color=color[i])\n",
    "plt.xlabel(\"Metrics\")\n",
    "plt.ylabel(\"Values\")\n",
    "plt.ylim(0, 100)\n",
    "plt.xticks(x, categories)\n",
    "plt.title(\"Comparison of RC, IS, and DS for the three algorithms\")\n",
    "plt.legend()\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.75)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for averages_fm_i in averages_fm:\n",
    "    print(averages_fm_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "metric evaluation (intermediate models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = []\n",
    "for my_i in range (1,iterations+1):\n",
    "    driver_model = torch.load(f\"models/A1/update count - {my_i}.pth\", map_location=torch.device('cpu'), weights_only=False)\n",
    "    try:\n",
    "        # print(\"started\")\n",
    "\n",
    "        prev_image_captured = None\n",
    "        image_captured = None\n",
    "        init_pos = ego.get_transform().location\n",
    "\n",
    "        RC = 0\n",
    "        IS = 0\n",
    "        DS = 0\n",
    "        dist_covered = 0\n",
    "        distance = 0\n",
    "        isc = 0\n",
    "        speed_limit = 50\n",
    "\n",
    "        count_dist = 0\n",
    "        dist_counter = 0\n",
    "\n",
    "        red_light_violations = False\n",
    "        collision_sensor = CollisionSensor(ego)\n",
    "        lane_violations = False\n",
    "        tailgating_violations = False\n",
    "        illegal_turn_violations = False\n",
    "        stop_sign_violations = False\n",
    "        set_cam_to_EGO(ego)\n",
    "        camera.listen(camera_to_image) #changes image_captured globally\n",
    "\n",
    "        while image_captured is None:\n",
    "            world.tick()\n",
    "            start_time = time.time()\n",
    "\n",
    "        while True:\n",
    "            world.wait_for_tick()\n",
    "            if not prev_image_captured == image_captured:\n",
    "                dist_counter += 1\n",
    "\n",
    "                scene_x, scene_y = 224, 224\n",
    "                ego_x_mid = scene_x/2\n",
    "                ego_y_mid = scene_y/2\n",
    "\n",
    "                outputs = model_output_generation(image_captured, driver_model)\n",
    "                waypoint = abs(outputs.detach().numpy())\n",
    "\n",
    "                ego_x_position, ego_y_position = points()\n",
    "                \n",
    "                intended_x = waypoint[0]\n",
    "                intended_y = waypoint[1]\n",
    "\n",
    "                if dist_counter % 2 == 0:\n",
    "                    init_pos = ego.get_transform().location\n",
    "                    d = 0\n",
    "                else:\n",
    "                    curr_pos = ego.get_transform().location\n",
    "                    dx = abs(curr_pos.x - init_pos.x)\n",
    "                    dy = abs(curr_pos.y - init_pos.y)\n",
    "                    dz = abs(curr_pos.z - init_pos.z)\n",
    "                    d = math.sqrt((dx ** 2) + (dy ** 2) + (dz ** 2))\n",
    "                    distance += d\n",
    "                    evaluate_RC(intended_x, ego_x_position, intended_y, ego_y_position, th, d)\n",
    "                    evaluate_IS(speed_limit, red_light_violations, collision_sensor, lane_violations, tailgating_violations, illegal_turn_violations, stop_sign_violations)\n",
    "\n",
    "                    count_dist += math.sqrt(((intended_x - ego_x_position) ** 2) + ((intended_y - ego_y_position) ** 2))\n",
    "\n",
    "                    # if distance > 0:\n",
    "                    #     print(f\"\\r{dist_covered:.2f} / {distance:.2f}, {isc:.2f}        \", end = \"\")\n",
    "                        # break\n",
    "\n",
    "                throttle, brake = TandB(ego_y_mid, intended_y, ego_y_position)\n",
    "                velocity = ego.get_velocity()\n",
    "                speed = math.sqrt((velocity.x**2 + velocity.y**2 + velocity.z**2)) / 5\n",
    "                s_power = (1 - brake) * (1 - speed)\n",
    "                steer = steering(ego_x_position, intended_x, s_power)\n",
    "\n",
    "                # print(f\"\\rgoing towards: ({intended_x:.1f}, {intended_y:.1f}), with b:{brake}, t:{throttle:.1f}, s:{steer:.1f}      \", end = \"\")\n",
    "\n",
    "                ego.apply_control(control_ego(steer, throttle, brake))\n",
    "                set_cam_to_EGO(ego)\n",
    "                \n",
    "                red_light_violations = False\n",
    "                collision_sensor = CollisionSensor(ego)\n",
    "                lane_violations = False\n",
    "                tailgating_violations = False\n",
    "                illegal_turn_violations = False\n",
    "                stop_sign_violations = False\n",
    "\n",
    "            if time.time() >= start_time + 120:\n",
    "                break\n",
    "            prev_image_captured = image_captured\n",
    "            \n",
    "        if(distance > 0):\n",
    "            RC = dist_covered * 100 / distance\n",
    "            IS = isc\n",
    "            DS = evaluate_DS(RC, IS)\n",
    "        m1.append([RC, IS, DS])\n",
    "        # print(f\"\\nRC is: {RC:.2f}, IS is: {IS:.2f}, DS is: {DS:.2f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n{e}\")\n",
    "    finally:\n",
    "        camera.stop()\n",
    "\n",
    "        ego.apply_control(control_ego(0,0,1))\n",
    "\n",
    "        remove_all_actors()\n",
    "        initiating_carla(world, blueprint_library)\n",
    "        set_cam_to_EGO(ego, 65)\n",
    "\n",
    "        # print(\"stopped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 = []\n",
    "for my_i in range (1,iterations+1):\n",
    "    driver_model = torch.load(f\"models/A2/update count - {my_i}.pth\", map_location=torch.device('cpu'), weights_only=False)\n",
    "    try:\n",
    "        # print(\"started\")\n",
    "\n",
    "        prev_image_captured = None\n",
    "        image_captured = None\n",
    "        init_pos = ego.get_transform().location\n",
    "\n",
    "        RC = 0\n",
    "        IS = 0\n",
    "        DS = 0\n",
    "        dist_covered = 0\n",
    "        distance = 0\n",
    "        isc = 0\n",
    "        speed_limit = 50\n",
    "\n",
    "        count_dist = 0\n",
    "        dist_counter = 0\n",
    "\n",
    "        red_light_violations = False\n",
    "        collision_sensor = CollisionSensor(ego)\n",
    "        lane_violations = False\n",
    "        tailgating_violations = False\n",
    "        illegal_turn_violations = False\n",
    "        stop_sign_violations = False\n",
    "        set_cam_to_EGO(ego)\n",
    "        camera.listen(camera_to_image) #changes image_captured globally\n",
    "\n",
    "        while image_captured is None:\n",
    "            world.tick()\n",
    "            start_time = time.time()\n",
    "\n",
    "        while True:\n",
    "            world.wait_for_tick()\n",
    "            if not prev_image_captured == image_captured:\n",
    "                dist_counter += 1\n",
    "\n",
    "                scene_x, scene_y = 224, 224\n",
    "                ego_x_mid = scene_x/2\n",
    "                ego_y_mid = scene_y/2\n",
    "\n",
    "                outputs = model_output_generation(image_captured, driver_model)\n",
    "                waypoint = abs(outputs.detach().numpy())\n",
    "\n",
    "                ego_x_position, ego_y_position = points()\n",
    "                \n",
    "                intended_x = waypoint[0]\n",
    "                intended_y = waypoint[1]\n",
    "\n",
    "                if dist_counter % 2 == 0:\n",
    "                    init_pos = ego.get_transform().location\n",
    "                    d = 0\n",
    "                else:\n",
    "                    curr_pos = ego.get_transform().location\n",
    "                    dx = abs(curr_pos.x - init_pos.x)\n",
    "                    dy = abs(curr_pos.y - init_pos.y)\n",
    "                    dz = abs(curr_pos.z - init_pos.z)\n",
    "                    d = math.sqrt((dx ** 2) + (dy ** 2) + (dz ** 2))\n",
    "                    distance += d\n",
    "                    evaluate_RC(intended_x, ego_x_position, intended_y, ego_y_position, th, d)\n",
    "                    evaluate_IS(speed_limit, red_light_violations, collision_sensor, lane_violations, tailgating_violations, illegal_turn_violations, stop_sign_violations)\n",
    "\n",
    "                    count_dist += math.sqrt(((intended_x - ego_x_position) ** 2) + ((intended_y - ego_y_position) ** 2))\n",
    "\n",
    "                    # if distance > 0:\n",
    "                    #     print(f\"\\r{dist_covered:.2f} / {distance:.2f}, {isc:.2f}        \", end = \"\")\n",
    "                        # break\n",
    "\n",
    "                throttle, brake = TandB(ego_y_mid, intended_y, ego_y_position)\n",
    "                velocity = ego.get_velocity()\n",
    "                speed = math.sqrt((velocity.x**2 + velocity.y**2 + velocity.z**2)) / 5\n",
    "                s_power = (1 - brake) * (1 - speed)\n",
    "                steer = steering(ego_x_position, intended_x, s_power)\n",
    "\n",
    "                # print(f\"\\rgoing towards: ({intended_x:.1f}, {intended_y:.1f}), with b:{brake}, t:{throttle:.1f}, s:{steer:.1f}      \", end = \"\")\n",
    "\n",
    "                ego.apply_control(control_ego(steer, throttle, brake))\n",
    "                set_cam_to_EGO(ego)\n",
    "                \n",
    "                red_light_violations = False\n",
    "                collision_sensor = CollisionSensor(ego)\n",
    "                lane_violations = False\n",
    "                tailgating_violations = False\n",
    "                illegal_turn_violations = False\n",
    "                stop_sign_violations = False\n",
    "\n",
    "            if time.time() >= start_time + 120:\n",
    "                break\n",
    "            prev_image_captured = image_captured\n",
    "            \n",
    "        if(distance > 0):\n",
    "            RC = dist_covered * 100 / distance\n",
    "            IS = isc\n",
    "            DS = evaluate_DS(RC, IS)    \n",
    "        m2.append([RC, IS, DS])\n",
    "        # print(f\"\\nRC is: {RC:.2f}, IS is: {IS:.2f}, DS is: {DS:.2f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n{e}\")\n",
    "    finally:\n",
    "        camera.stop()\n",
    "\n",
    "        ego.apply_control(control_ego(0,0,1))\n",
    "\n",
    "        remove_all_actors()\n",
    "        initiating_carla(world, blueprint_library)\n",
    "        set_cam_to_EGO(ego, 65)\n",
    "\n",
    "        # print(\"stopped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m3 = []\n",
    "for my_i in range (1,iterations+1):\n",
    "    details()\n",
    "    try:\n",
    "        driver_model = torch.load(f\"models/A3/update count - {my_i}.pth\", map_location=torch.device('cpu'), weights_only=False)\n",
    "        # print(\"started\")\n",
    "\n",
    "        prev_image_captured = None\n",
    "        image_captured = None\n",
    "        init_pos = ego.get_transform().location\n",
    "\n",
    "        RC = 0\n",
    "        IS = 0\n",
    "        DS = 0\n",
    "        dist_covered = 0\n",
    "        distance = 0\n",
    "        isc = 0\n",
    "        speed_limit = 50\n",
    "\n",
    "        count_dist = 0\n",
    "        dist_counter = 0\n",
    "\n",
    "        red_light_violations = False\n",
    "        collision_sensor = CollisionSensor(ego)\n",
    "        lane_violations = False\n",
    "        tailgating_violations = False\n",
    "        illegal_turn_violations = False\n",
    "        stop_sign_violations = False\n",
    "        set_cam_to_EGO(ego)\n",
    "        camera.listen(camera_to_image) #changes image_captured globally\n",
    "\n",
    "        while image_captured is None:\n",
    "            world.tick()\n",
    "            start_time = time.time()\n",
    "\n",
    "        while True:\n",
    "            world.wait_for_tick()\n",
    "            if not prev_image_captured == image_captured:\n",
    "                dist_counter += 1\n",
    "\n",
    "                scene_x, scene_y = 224, 600\n",
    "                ego_x_mid = scene_x/2\n",
    "                ego_y_mid = scene_y/2\n",
    "\n",
    "                outputs = model_output_generation(image_captured, driver_model)\n",
    "                waypoint = abs(outputs.detach().numpy())\n",
    "\n",
    "                ego_x_position, ego_y_position = points()\n",
    "                \n",
    "                intended_x = waypoint[0]\n",
    "                intended_y = waypoint[1]\n",
    "\n",
    "                if dist_counter % 2 == 0:\n",
    "                    init_pos = ego.get_transform().location\n",
    "                    d = 0\n",
    "                else:\n",
    "                    curr_pos = ego.get_transform().location\n",
    "                    dx = abs(curr_pos.x - init_pos.x)\n",
    "                    dy = abs(curr_pos.y - init_pos.y)\n",
    "                    dz = abs(curr_pos.z - init_pos.z)\n",
    "                    d = math.sqrt((dx ** 2) + (dy ** 2) + (dz ** 2))\n",
    "                    distance += d\n",
    "                    evaluate_RC(intended_x, ego_x_position, intended_y, ego_y_position, th, d)\n",
    "                    evaluate_IS(speed_limit, red_light_violations, collision_sensor, lane_violations, tailgating_violations, illegal_turn_violations, stop_sign_violations)\n",
    "\n",
    "                    count_dist += math.sqrt(((intended_x - ego_x_position) ** 2) + ((intended_y - ego_y_position) ** 2))\n",
    "\n",
    "                    # if distance > 0:\n",
    "                    #     print(f\"\\r{dist_covered:.2f} / {distance:.2f}, {isc:.2f}        \", end = \"\")\n",
    "                        # break\n",
    "\n",
    "                throttle, brake = TandB(ego_y_mid, intended_y, ego_y_position)\n",
    "                velocity = ego.get_velocity()\n",
    "                speed = math.sqrt((velocity.x**2 + velocity.y**2 + velocity.z**2)) / 5\n",
    "                s_power = (1 - brake) * (1 - speed)\n",
    "                steer = steering(ego_x_position, intended_x, s_power)\n",
    "\n",
    "                # print(f\"\\rgoing towards: ({intended_x:.1f}, {intended_y:.1f}), with b:{brake}, t:{throttle:.1f}, s:{steer:.1f}      \", end = \"\")\n",
    "\n",
    "                ego.apply_control(control_ego(steer, throttle, brake))\n",
    "                set_cam_to_EGO(ego)\n",
    "                \n",
    "                red_light_violations = False\n",
    "                collision_sensor = CollisionSensor(ego)\n",
    "                lane_violations = False\n",
    "                tailgating_violations = False\n",
    "                illegal_turn_violations = False\n",
    "                stop_sign_violations = False\n",
    "\n",
    "                print(f\"\\r{len(m3)}, {(100 * (time.time() - start_time) / 270):.2f}% done\", end=\"\")\n",
    "\n",
    "            if time.time() >= start_time + 120:\n",
    "                break\n",
    "            prev_image_captured = image_captured\n",
    "            \n",
    "        if(distance > 0):\n",
    "            RC = dist_covered * 100 / distance\n",
    "            IS = isc\n",
    "            DS = evaluate_DS(RC, IS)\n",
    "        m3.append([RC, IS, DS])\n",
    "        # print(f\"\\nRC is: {RC:.2f}, IS is: {IS:.2f}, DS is: {DS:.2f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n{e}\")\n",
    "    finally:\n",
    "        camera.stop()\n",
    "\n",
    "        ego.apply_control(control_ego(0,0,1))\n",
    "\n",
    "        remove_all_actors()\n",
    "        initiating_carla(world, blueprint_library)\n",
    "        set_cam_to_EGO(ego, 65)\n",
    "\n",
    "        # print(\"stopped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M1,M2,M3 = m1,m2,m3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1,m2,m3 = M1,M2,M3\n",
    "a = 0\n",
    "for i,j,k in zip(m1,m2,m3):\n",
    "    a = a + 1\n",
    "    print(f\"epoch count {a}: \", end=\"\")\n",
    "    print(f\"[{i[0]:.2f}, {i[1]:.2f}, {i[2]:.2f}]\", end = \", \")\n",
    "    print(f\"[{j[0]:.2f}, {j[1]:.2f}, {j[2]:.2f}]\", end = \", \")\n",
    "    print(f\"[{k[0]:.2f}, {k[1]:.2f}, {k[2]:.2f}]\", end = \", \")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = [m1,m2,m3]\n",
    "for m in M:\n",
    "    x = 40 * (np.arange(len(m)) + 1)\n",
    "    y_values = np.array(m).T\n",
    "\n",
    "    markers = ['o', 's', 'D']\n",
    "    labels = ['RC', 'IS', 'DS']\n",
    "    jitter_strength = 0.5\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    for i, (y, marker, label) in enumerate(zip(y_values, markers, labels)):\n",
    "        y_jittered = [val + random.uniform(-jitter_strength, jitter_strength) for val in y]\n",
    "        plt.plot(x, y_jittered, marker=marker, linestyle='-', label=label)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Metric values\")\n",
    "    plt.ylim(-8, 109)\n",
    "    if m == M[0]:\n",
    "        A = A1\n",
    "    elif m == M[1]:\n",
    "        A = A2\n",
    "    elif m == M[2]:\n",
    "        A = A3\n",
    "    plt.title(f\"{A}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Route Completion', 'Infraction Score', 'Driving Score']\n",
    "A = [A1, A2, A3]\n",
    "M = [m1,m2,m3]\n",
    "M = np.array(M).T\n",
    "\n",
    "for i_ctr in range(0, len(A)):\n",
    "    x = 40 * (np.arange(iterations) + 1)\n",
    "    y_values = np.array(M[i_ctr]).T\n",
    "\n",
    "    markers = ['o', 's', 'D']\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    for i, (y, marker, a) in enumerate(zip(y_values, markers, A)):\n",
    "        plt.plot(x, y, marker=marker, linestyle='-', label=f\"Algorithm {i+1}\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Algorithm's metric values\")\n",
    "    plt.ylim(-8, 109)\n",
    "    plt.title(f\"{labels[i_ctr]}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modifications**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*grounging DINO*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import gym\n",
    "import time\n",
    "import carla\n",
    "import torch\n",
    "import random\n",
    "import requests\n",
    "import concurrent.futures\n",
    "\n",
    "from PIL import Image\n",
    "from ultralytics import YOLO\n",
    "from torch.optim import AdamW\n",
    "from concurrent.futures import ThreadPoolExecutor, Future\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from transformers import AutoProcessor, Mask2FormerForUniversalSegmentation, AutoModelForZeroShotObjectDetection, AutoImageProcessor, SegformerForSemanticSegmentation, SegformerImageProcessor\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('datasets/images/Video_000/v000_0000.png')\n",
    "# empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('datasets/images/Video_000/v000_0295.png')\n",
    "# bike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('datasets/images/Video_004/v004_0182.png')\n",
    "# turning car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('bdd10k/images/10k/train/0a14a567-fdd08dd2.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forming_x_ranges(x_mask_list):\n",
    "    pair = []\n",
    "    in_range = False\n",
    "    start = 0\n",
    "\n",
    "    for i, val in enumerate(x_mask_list):\n",
    "        if val == 1 and not in_range:\n",
    "            start = i\n",
    "            in_range = True\n",
    "        elif val == 0 and in_range:\n",
    "            pair.append((start, i - 1))\n",
    "            in_range = False\n",
    "\n",
    "    # If the list ends with 1s\n",
    "    if in_range:\n",
    "        pair.append((start, len(x_mask_list) - 1))\n",
    "\n",
    "    return pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finding_closest_x(pair):\n",
    "    centers = []\n",
    "    posi = 0\n",
    "    for p in pair:\n",
    "        c = (p[0] + p[1]) / 2\n",
    "        centers.append(c)\n",
    "\n",
    "    mini = float('inf')\n",
    "    for i in centers:\n",
    "        j = abs(400 - i)\n",
    "        if mini > j:\n",
    "            mini = j\n",
    "            posi = i\n",
    "    \n",
    "    return posi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finding_closest_y(image, y_max_list):\n",
    "    image_array = np.array(image)\n",
    "    height, width = image_array.shape[:2]\n",
    "\n",
    "    y_max = height / 2\n",
    "\n",
    "    for i in y_max_list:\n",
    "        y_max = max(i, y_max)\n",
    "    \n",
    "    # print(f\"y_max is:{y_max}, height: {height}\")\n",
    "                    \n",
    "    y_max = height - y_max\n",
    "\n",
    "    return y_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setting_controls(optimal_x, y_max_allowed, maxs, trigger=400, times=0.3):\n",
    "    x_max, y_max = maxs\n",
    "    margin = x_max * (1 - times) / 2\n",
    "    trigger = y_max - min(y_max, trigger)\n",
    "    throttle = min(max(0.3, y_max_allowed / ((y_max / 2))), 0.5)\n",
    "    steer = ((min(max(optimal_x, margin), x_max - margin) - (x_max / 2)) / x_max)\n",
    "    reverse = False\n",
    "\n",
    "    if y_max_allowed < trigger:\n",
    "        reverse = True\n",
    "        throttle = 0.5\n",
    "        steer = 0\n",
    "\n",
    "    return steer, throttle, reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*semantic segmentation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_processor_segm = AutoImageProcessor.from_pretrained(\"facebook/mask2former-swin-small-cityscapes-panoptic\")\n",
    "model_segm = Mask2FormerForUniversalSegmentation.from_pretrained(\"facebook/mask2former-swin-small-cityscapes-panoptic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_segs(pred_panoptic_map):\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.title(\"Panoptic Segmentation\")\n",
    "    plt.imshow(pred_panoptic_map.cpu().numpy(), cmap=\"tab20\")  # Each region has a unique ID\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_output(image, model=model_segm, image_processor=image_processor_segm):\n",
    "    inputs = image_processor(image, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    class_queries_logits = outputs.class_queries_logits\n",
    "    masks_queries_logits = outputs.masks_queries_logits\n",
    "\n",
    "    pred_panoptic_map = image_processor.post_process_panoptic_segmentation(\n",
    "        outputs,\n",
    "        target_sizes=[(image.height, image.width)],\n",
    "        label_ids_to_fuse=[]\n",
    "    )[0][\"segmentation\"]\n",
    "    # view_segs(pred_panoptic_map)\n",
    "    return pred_panoptic_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_bottom_center_uniform(pred, y_percent=90, x_margin_percent=33):\n",
    "    h, w = pred.shape\n",
    "    cut_h = int(h * (y_percent / 100))\n",
    "    cut_w = int(w * (x_margin_percent / 100))\n",
    "\n",
    "    bottom_center = pred[cut_h:, cut_w:-cut_w]\n",
    "\n",
    "    first_val = bottom_center[0, 0].item()\n",
    "    all_same = (bottom_center == first_val).all().item()\n",
    "\n",
    "    return all_same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_bottom_center_one_sided(pred, y_percent=65, x_margin_percent=16):\n",
    "    h, w = pred.shape\n",
    "    center_w = w // 2\n",
    "\n",
    "    cut_h = int(h * (y_percent / 100))\n",
    "    cut_w = int(w * (x_margin_percent / 100))\n",
    "\n",
    "    all_same_left_end = cut_w\n",
    "    all_same_right_start = w - cut_w\n",
    "    all_same_left_start = cut_w\n",
    "    all_same_right_end = w - cut_w\n",
    "\n",
    "    bottom_left = pred[cut_h:, cut_w:center_w]\n",
    "    bottom_right = pred[cut_h:, center_w:w - cut_w]\n",
    "\n",
    "    for i in reversed(range(bottom_left.shape[1])):\n",
    "        col = bottom_left[:, i]\n",
    "        if not (col == col[0]).all():\n",
    "            all_same_left_end = cut_w + i\n",
    "            break\n",
    "\n",
    "    for i in range(bottom_right.shape[1]):\n",
    "        col = bottom_right[:, i]\n",
    "        if not (col == col[0]).all():\n",
    "            all_same_right_start = center_w + i\n",
    "            break\n",
    "\n",
    "    x_range = all_same_right_start - all_same_left_end\n",
    "\n",
    "    if x_range < w / 3:\n",
    "        bottom_left_new = bottom_left[:, :all_same_left_end]\n",
    "        bottom_right_new = bottom_right[:, all_same_right_start - center_w:]\n",
    "\n",
    "        for i in range(bottom_left_new.shape[1]):\n",
    "            col = bottom_left_new[:, i]\n",
    "            if not (col == col[0]).all():\n",
    "                all_same_left_start = cut_w + i\n",
    "                # break\n",
    "        \n",
    "        for i in reversed(range(bottom_right_new.shape[1])):\n",
    "            col = bottom_right_new[:, i]\n",
    "            if not (col == col[0]).all():\n",
    "                all_same_right_end = all_same_right_start + i\n",
    "                # break\n",
    "\n",
    "    posi = ((all_same_right_start + all_same_left_end) / 2) - center_w\n",
    "    \n",
    "    # print(f\"\\r{all_same_left_start}, {all_same_left_end}, {all_same_right_start}, {all_same_right_end} | {x_range}, {center_w}, {cut_w}\", end=10*\" \")\n",
    "    return posi / center_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_of_reverse(image, yp=90, xp=33, model=model_segm, image_processor=image_processor_segm):\n",
    "    pred = model_output(image, model, image_processor)\n",
    "\n",
    "    return not check_bottom_center_uniform(pred, yp, xp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_of_side_change(image, yp=65, xp=16, model=model_segm, image_processor=image_processor_segm):\n",
    "    pred = model_output(image, model, image_processor)\n",
    "\n",
    "    return check_bottom_center_one_sided(pred, yp, xp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_segm = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = Image.open('datasets/images/Video_001/v001_0190.png')\n",
    "print(decision_of_side_change(img, 78, 25))\n",
    "print(decision_of_reverse(img, 65, 33))\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = image_captured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decision_of_reverse(img, 85, 33), decision_of_side_change(img, 85, 33))\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_bottom_center(pred, y_percent=75, x_margin_percent=20, title=\"Bottom-Center Region\"):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    h, w = pred.shape\n",
    "    cut_h = int(h * (y_percent / 100))\n",
    "    cut_w = int(w * (x_margin_percent / 100))\n",
    "\n",
    "    bottom_center = pred[cut_h:, cut_w:-cut_w]\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.title(title)\n",
    "    plt.imshow(bottom_center, cmap=\"tab20\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "pred = model_output(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_bottom_center(pred, y_percent=90, x_margin_percent=33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*grounding DINO*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"IDEA-Research/grounding-dino-tiny\"\n",
    "device = \"cpu\"\n",
    "# text_labels = \"Pedestrian, Cyclist, Car, Truck, Bus, Footpath, Motorcycle, Traffic Light, Stop Sign, Yield Sign, Intersection, Roundabout, Building, Tree, Bush, Pedestrian Button, Crosswalk Signal, Traffic Camera, Lane Change, Electric Vehicle Charging Station, Emergency Vehicle, Tow Truck, Traffic Signal, Autonomous Vehicle, GPS.\"\n",
    "text_labels=\"road, mainroad, driveway, path\"\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "encoded_text = processor(text=text_labels, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "obj_model = AutoModelForZeroShotObjectDetection.from_pretrained(model_id).to(device)\n",
    "obj_model.eval()\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify(image, model=obj_model):\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    inputs[\"input_ids\"] = encoded_text\n",
    "\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    results = processor.post_process_grounded_object_detection(\n",
    "        outputs,\n",
    "        inputs.input_ids,\n",
    "        box_threshold=0.25,\n",
    "        text_threshold=0.25,\n",
    "        target_sizes=[image.size[::-1]]\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showing_image_gDINO(image, results):\n",
    "    fig, ax = plt.subplots(1, figsize=(9, 6))\n",
    "    ax.imshow(image)\n",
    "\n",
    "    # Loop over the results and add bounding boxes\n",
    "    for result in results:\n",
    "        for box, score, label in zip(result[\"boxes\"], result[\"scores\"], result[\"labels\"]):\n",
    "            xmin, ymin, xmax, ymax = box\n",
    "            score = round(score.item(), 3)\n",
    "\n",
    "            # Create a rectangle for the bounding box\n",
    "            rect = patches.Rectangle(\n",
    "                (xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "                linewidth=2, edgecolor='r', facecolor='none', label=f\"Confidence: {score}\"\n",
    "            )\n",
    "            ax.add_patch(rect)\n",
    "            ax.text(\n",
    "                xmin, ymin - 25, f\"Label: {label} | Score: {score}\",\n",
    "                color='red', fontsize=10, fontweight='bold', backgroundcolor='white'\n",
    "            )\n",
    "\n",
    "    # Show plot with bounding boxes\n",
    "    # plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finding_proper_x(image, results, th_y=330):\n",
    "    width = image.size[1]\n",
    "\n",
    "    x_mask = np.ones(width, dtype=int)\n",
    "    ymax_list = np.zeros(width, dtype=int)\n",
    "\n",
    "    for result in results:\n",
    "        for box in result[\"boxes\"]:\n",
    "            xmin, _, xmax, ymax = map(int, box)\n",
    "            if ymax >= th_y:\n",
    "                x_mask[xmin:xmax] = 0\n",
    "                ymax_list[xmin:xmax] = ymax\n",
    "\n",
    "    x_mask_list = x_mask.tolist()\n",
    "    return x_mask_list, ymax_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def object_to_control(image, model):\n",
    "    results = identify(image, model)\n",
    "    showing_image_gDINO(image, results)\n",
    "    x_mask_list, y_max_list = finding_proper_x(image, results)\n",
    "    pair = forming_x_ranges(x_mask_list)\n",
    "    optimal_x = finding_closest_x(pair)\n",
    "    optimal_y = finding_closest_y(image, y_max_list)\n",
    "    print(results)\n",
    "    controls = setting_controls(optimal_x, optimal_y, image.size)\n",
    "\n",
    "    return controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_to_control(img, obj_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_captured = None\n",
    "camera.listen(camera_to_image) #changes image_captured globally\n",
    "while image_captured is None:\n",
    "    time.sleep(0.1)\n",
    "\n",
    "camera.stop()\n",
    "img = image_captured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image_captured)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ego_drive(ego, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ego_drive(ego, True, steer=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ego_drive(ego, brake=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ego_drive(ego)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*yolov8_*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_model = YOLO('yolov8n.pt') # n s m l x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finding_proper_x(image, results, th_y=330):\n",
    "    width = image.size[1]\n",
    "\n",
    "    x_mask = np.ones(width, dtype=int)\n",
    "    ymax_list = np.zeros(width, dtype=int)\n",
    "\n",
    "    for result in results:\n",
    "        box = result\n",
    "        xmin, _, xmax, ymax = map(int, box)\n",
    "        if ymax >= th_y:\n",
    "            x_mask[xmin:xmax] = 0\n",
    "            ymax_list[xmin:xmax] = ymax\n",
    "\n",
    "    x_mask_list = x_mask.tolist()\n",
    "    return x_mask_list, ymax_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def object_to_control(image, model):\n",
    "    results = model.predict(source=image, verbose =  False)[0]\n",
    "    results.show()\n",
    "    results = results.boxes.xyxy\n",
    "    x_mask_list, y_max_list = finding_proper_x(image, results)\n",
    "    pair = forming_x_ranges(x_mask_list)\n",
    "    optimal_x = finding_closest_x(pair)\n",
    "    optimal_y = finding_closest_y(image, y_max_list)\n",
    "    # print(results)\n",
    "    controls = setting_controls(optimal_x, optimal_y, image.size)\n",
    "    return controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_to_control(image_captured, obj_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nav_obj_to_final(nav_control, obj_control):\n",
    "    s_1,t_1,r_1 = nav_control\n",
    "    s_2,t_2,r_2 = obj_control\n",
    "\n",
    "    s = (s_1 + s_2) / 2\n",
    "    t = (t_1 + t_2) / 2\n",
    "    r = r_1 or r_2\n",
    "    \n",
    "    fo = [s,t,r]\n",
    "    # print(f\"\\r{nav_control}, {obj_control}, {fo}\", end= \"\")\n",
    "\n",
    "    return fo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # print(\"started\")\n",
    "\n",
    "    prev_image_captured = None\n",
    "    image_captured = None\n",
    "    init_pos = ego.get_transform().location\n",
    "\n",
    "    RC = 0\n",
    "    IS = 0\n",
    "    DS = 0\n",
    "    dist_covered = 0\n",
    "    distance = 0\n",
    "    isc = 0\n",
    "    speed_limit = 50\n",
    "\n",
    "    count_dist = 0\n",
    "    dist_counter = 0\n",
    "\n",
    "    red_light_violations = False\n",
    "    collision_sensor = CollisionSensor(ego)\n",
    "    lane_violations = False\n",
    "    tailgating_violations = False\n",
    "    illegal_turn_violations = False\n",
    "    stop_sign_violations = False\n",
    "    set_cam_to_EGO(ego)\n",
    "    camera.listen(camera_to_image) #changes image_captured globally\n",
    "\n",
    "    yp = 85\n",
    "    xp = 33\n",
    "\n",
    "    while image_captured is None:\n",
    "        world.tick()\n",
    "        start_time = time.time()\n",
    "\n",
    "    while True:\n",
    "        world.wait_for_tick()\n",
    "        \n",
    "        if time.time() >= start_time + 1200:\n",
    "            break\n",
    "\n",
    "        if not prev_image_captured == image_captured:\n",
    "            dist_counter += 1\n",
    "\n",
    "            scene_x, scene_y = 224, 224\n",
    "            ego_x_mid = scene_x/2\n",
    "            ego_y_mid = scene_y/2\n",
    "\n",
    "            outputs = model_output_generation(image_captured, driver_model)\n",
    "            waypoint = abs(outputs.detach().numpy())\n",
    "\n",
    "            ego_x_position, ego_y_position = points()\n",
    "            \n",
    "            intended_x = waypoint[0]\n",
    "            intended_y = waypoint[1]\n",
    "\n",
    "            if dist_counter % 2 == 0:\n",
    "                init_pos = ego.get_transform().location\n",
    "                d = 0\n",
    "            else:\n",
    "                curr_pos = ego.get_transform().location\n",
    "                dx = abs(curr_pos.x - init_pos.x)\n",
    "                dy = abs(curr_pos.y - init_pos.y)\n",
    "                dz = abs(curr_pos.z - init_pos.z)\n",
    "                d = math.sqrt((dx ** 2) + (dy ** 2) + (dz ** 2))\n",
    "                distance += d\n",
    "                evaluate_RC(intended_x, ego_x_position, intended_y, ego_y_position, th, d)\n",
    "                evaluate_IS(speed_limit, red_light_violations, collision_sensor, lane_violations, tailgating_violations, illegal_turn_violations, stop_sign_violations)\n",
    "\n",
    "                count_dist += math.sqrt(((intended_x - ego_x_position) ** 2) + ((intended_y - ego_y_position) ** 2))\n",
    "\n",
    "                # if distance > 0:\n",
    "                #     print(f\"\\r{dist_covered:.2f} / {distance:.2f}, {isc:.2f}        \", end = \"\")\n",
    "                    # break\n",
    "\n",
    "            throttle, brake = TandB(ego_y_mid, intended_y, ego_y_position)\n",
    "            velocity = ego.get_velocity()\n",
    "            speed = math.sqrt((velocity.x**2 + velocity.y**2 + velocity.z**2)) / 5\n",
    "            s_power = (1 - brake) * (1 - speed)\n",
    "            steer = steering(ego_x_position, intended_x, s_power)\n",
    "\n",
    "            if decision_of_reverse(image_captured, max(yp-20, 65), xp):\n",
    "                steer = decision_of_side_change(image_captured, yp, xp // 2)\n",
    "            elif decision_of_reverse(image_captured, min(yp+10, 95), xp) and not reverse:\n",
    "                brake = 1.0\n",
    "            print(f\"\\rb:{brake}, t:{throttle:.1f}, s:{steer:.1f}, r:{reverse}\", end = 20*\" \")\n",
    "\n",
    "            ego.apply_control(control_ego(steer, throttle, brake, reverse))\n",
    "            set_cam_to_EGO(ego)\n",
    "            \n",
    "            red_light_violations = False\n",
    "            collision_sensor = CollisionSensor(ego)\n",
    "            lane_violations = False\n",
    "            tailgating_violations = False\n",
    "            illegal_turn_violations = False\n",
    "            stop_sign_violations = False\n",
    "\n",
    "        prev_image_captured = image_captured\n",
    "        \n",
    "    if(distance > 0):\n",
    "        RC = dist_covered * 100 / distance\n",
    "        IS = isc\n",
    "        DS = evaluate_DS(RC, IS)\n",
    "\n",
    "    print(f\"\\nRC is: {RC:.2f}, IS is: {IS:.2f}, DS is: {DS:.2f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n{e}\")\n",
    "finally:\n",
    "    camera.stop()\n",
    "\n",
    "    ego.apply_control(control_ego(0,0,1))\n",
    "\n",
    "    # remove_all_actors()\n",
    "    # initiating_carla(world, blueprint_library)\n",
    "    # set_cam_to_EGO(ego, 65)\n",
    "\n",
    "    # print(\"stopped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rough work's rough work (and the cycle will continue ie - (rough work's ^ n) rough work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "\n",
    "model = models.segmentation.deeplabv3_resnet101(pretrained=True).eval()\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((520, 520)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# img = Image.open('your_image.jpg')\n",
    "input_tensor = preprocess(img).unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_tensor)['out'][0]\n",
    "segmentation = output.argmax(0).byte().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Resize segmentation map to original image size for proper visualization\n",
    "segmentation_resized = Image.fromarray(segmentation).resize(img.size, resample=Image.NEAREST)\n",
    "\n",
    "# Convert to NumPy array\n",
    "seg_np = np.array(segmentation_resized)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Original Image\")\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Segmentation Map\")\n",
    "plt.imshow(seg_np, cmap='tab20')  # Try 'nipy_spectral' or 'jet' for vibrant classes\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation\n",
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "\n",
    "# Load Mask2Former trained on COCO instance segmentation dataset\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"facebook/mask2former-swin-small-coco-instance\")\n",
    "model = Mask2FormerForUniversalSegmentation.from_pretrained(\n",
    "    \"facebook/mask2former-swin-small-coco-instance\"\n",
    ")\n",
    "\n",
    "# url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "# image = Image.open(requests.get(url, stream=True).raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = image_processor(image, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Model predicts class_queries_logits of shape `(batch_size, num_queries)`\n",
    "# and masks_queries_logits of shape `(batch_size, num_queries, height, width)`\n",
    "class_queries_logits = outputs.class_queries_logits\n",
    "masks_queries_logits = outputs.masks_queries_logits\n",
    "\n",
    "# Perform post-processing to get instance segmentation map\n",
    "pred_instance_map = image_processor.post_process_instance_segmentation(\n",
    "    outputs, target_sizes=[(image.height, image.width)]\n",
    ")[0]\n",
    "print(pred_instance_map.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "plt.title(\"Instance Segmentation\")\n",
    "plt.imshow(pred_instance_map['segmentation'].cpu().numpy(), cmap=\"tab20\")  # Each instance gets a different color\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation\n",
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "\n",
    "# Load Mask2Former trained on ADE20k semantic segmentation dataset\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"facebook/mask2former-swin-small-ade-semantic\")\n",
    "model = Mask2FormerForUniversalSegmentation.from_pretrained(\"facebook/mask2former-swin-small-ade-semantic\")\n",
    "\n",
    "# url = (\n",
    "#     \"https://huggingface.co/datasets/hf-internal-testing/fixtures_ade20k/resolve/main/ADE_val_00000001.jpg\"\n",
    "# )\n",
    "# image = Image.open(requests.get(url, stream=True).raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = image_processor(image, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Model predicts class_queries_logits of shape `(batch_size, num_queries)`\n",
    "# and masks_queries_logits of shape `(batch_size, num_queries, height, width)`\n",
    "class_queries_logits = outputs.class_queries_logits\n",
    "masks_queries_logits = outputs.masks_queries_logits\n",
    "\n",
    "# Perform post-processing to get semantic segmentation map\n",
    "pred_semantic_map = image_processor.post_process_semantic_segmentation(\n",
    "    outputs, target_sizes=[(image.height, image.width)]\n",
    ")[0]\n",
    "print(pred_semantic_map.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# pred_semantic_map: torch.Tensor, shape (H, W)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.title(\"Semantic Segmentation\")\n",
    "plt.imshow(pred_semantic_map.cpu().numpy(), cmap=\"tab20\")  # Or 'nipy_spectral'\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation\n",
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"facebook/mask2former-swin-small-cityscapes-panoptic\")\n",
    "model = Mask2FormerForUniversalSegmentation.from_pretrained(\"facebook/mask2former-swin-small-cityscapes-panoptic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = image_processor(image, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Model predicts class_queries_logits of shape `(batch_size, num_queries)`\n",
    "# and masks_queries_logits of shape `(batch_size, num_queries, height, width)`\n",
    "class_queries_logits = outputs.class_queries_logits\n",
    "masks_queries_logits = outputs.masks_queries_logits\n",
    "\n",
    "# Perform post-processing to get panoptic segmentation map\n",
    "pred_panoptic_map = image_processor.post_process_panoptic_segmentation(\n",
    "    outputs, target_sizes=[(image.height, image.width)]\n",
    ")[0][\"segmentation\"]\n",
    "print(pred_panoptic_map.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "plt.title(\"Panoptic Segmentation\")\n",
    "plt.imshow(pred_panoptic_map.cpu().numpy(), cmap=\"tab20\")  # Each region has a unique ID\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "from transformers import Mask2FormerForUniversalSegmentation, AutoImageProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "semantic_label_classes = [\"Traffic Sign\", \"Building\", \"Fence\", \"Other\", \"Pedestrian\", \"Pole\", \"Road Line\", \"Road\", \"Sidewalk\", \"Vegetation\", \"Car\", \"Wall\", \"Unlabeled\"]\n",
    "to_tensor = transforms.ToTensor()\n",
    "\n",
    "lr = 1e-3\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, images_dir):\n",
    "        self.data = []\n",
    "\n",
    "        for i in range(0, 28):\n",
    "            k = \"0\" if i <= 9 else \"\"\n",
    "            curr_folder_images = f\"{images_dir}/Video_0{k}{i}\"\n",
    "            curr_folder_labels = f\"{labels_dir}/Video_0{k}{i}\"\n",
    "            images_path = [file for file in os.listdir(curr_folder_images) if file.endswith(\".png\")]\n",
    "            labels_path = [file for file in os.listdir(curr_folder_labels) if file.endswith(\".png\")]\n",
    "            images_path.sort()\n",
    "            labels_path.sort()\n",
    "            images_path = images_path[::skip]\n",
    "            labels_path = labels_path[::skip]\n",
    "\n",
    "            for image_file, label_file in zip(images_path, labels_path):\n",
    "                image_path = os.path.join(curr_folder_images, image_file)\n",
    "                label_path = os.path.join(curr_folder_labels, label_file)\n",
    "\n",
    "                image = Image.open(image_path)\n",
    "                label = Image.open(label_path)\n",
    "                \n",
    "                image_embeddings = processor(text=semantic_label_classes, images=image, return_tensors=\"pt\", padding=True)\n",
    "                ei = image_embeddings['pixel_values']\n",
    "\n",
    "                with open(os.path.join(root, waypoint_file), \"r\") as wf:\n",
    "                    data_list = wf.read().splitlines()\n",
    "                    for data in data_list:\n",
    "                        data = json.loads(data)[0]\n",
    "                        if data['label'] == f\"datasets\\\\{image_file}\":\n",
    "                            corresponding_waypoints = data['waypoints']\n",
    "                            cw = corresponding_waypoints\n",
    "                            # print('match found')\n",
    "                            self.data.append((ei,cw))\n",
    "                            break\n",
    "                break # for first image in all folders\n",
    "            # break\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        embeddings, waypoints = self.data[idx]\n",
    "        return embeddings, waypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, root_img, root_lbl, processor, skip=50):\n",
    "        self.processor = processor\n",
    "        self.root_img = root_img\n",
    "        self.root_lbl = root_lbl\n",
    "        self.image_paths = []\n",
    "        self.label_paths = []\n",
    "        all_img_paths = []\n",
    "        for dirpath, _, filenames in os.walk(root_img):\n",
    "            for file in sorted(filenames):\n",
    "                img_path = os.path.join(dirpath, file)\n",
    "                lbl_path = img_path.replace(root_img, root_lbl)\n",
    "\n",
    "                if os.path.exists(lbl_path):\n",
    "                    all_img_paths.append((img_path, lbl_path))\n",
    "\n",
    "        sampled = all_img_paths[::skip]\n",
    "        self.image_paths = [img for img, _ in sampled]\n",
    "        self.label_paths = [lbl for _, lbl in sampled]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.image_paths[idx])\n",
    "        label_path = os.path.join(self.label_paths[idx])\n",
    "        \n",
    "        image = Image.open(image_path)\n",
    "        label = Image.open(label_path).convert('L')\n",
    "        label = to_tensor(label).squeeze(0)\n",
    "        \n",
    "        image_input = self.processor(image, return_tensors=\"pt\")\n",
    "        \n",
    "        return image_input, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SegmentationDataset(\"datasets/images\", \"datasets/labels_maxonly\", processor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "lr = 1e-3\n",
    "for _ in range(3):\n",
    "    for epoch in range(num_epochs):\n",
    "        counter=0\n",
    "        total_loss = 0.0\n",
    "        for batch in train_dataset:\n",
    "            counter+=1\n",
    "            inputs = batch[0][\"pixel_values\"].to(device)\n",
    "            labels = batch[1].to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                masks_queries_logits = outputs.masks_queries_logits\n",
    "                pred_panoptic_map = processor.post_process_panoptic_segmentation(\n",
    "                    outputs,\n",
    "                    target_sizes=[(600, 800)],\n",
    "                    label_ids_to_fuse=[]\n",
    "                )[0][\"segmentation\"]\n",
    "            \n",
    "            pred_panoptic_map = pred_panoptic_map.float().to(device)\n",
    "            pred_panoptic_map.requires_grad = True\n",
    "            \n",
    "            loss = loss_fn(pred_panoptic_map, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        print(f\"\\rEpoch: {epoch+1} lr: {lr} Loss: {(total_loss / counter):.4f}\", end=10*\" \")\n",
    "\n",
    "        if epoch != 0 and epoch % 8 == 0:\n",
    "            save_model(model, loss, optimizer, epoch, lr)\n",
    "\n",
    "    lr *= 1e-1\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
