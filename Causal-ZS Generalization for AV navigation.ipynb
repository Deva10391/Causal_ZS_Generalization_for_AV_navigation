{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ALGORITHM 1: VL pretrained-M fine tuned with semi-disentangled outputs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import torch\n",
    "import os\n",
    "import kagglehub\n",
    "import pickle\n",
    "import math\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "from sklearn.decomposition import PCA\n",
    "from skimage.morphology import skeletonize\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import CLIPProcessor, CLIPModel, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from torchvision import transforms as A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = kagglehub.dataset_download(\"albertozorzetto/carla-densely-annotated-driving-dataset\")\n",
    "# print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Creation (IMAGE form to EMBEDDING form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Download the CARLA dataset using the Kaggle API\n",
    "# dataset_name = \"albertozorzetto/carla-densely-annotated-driving-dataset\"\n",
    "# destination_path = \"/content/datasets\"\n",
    "\n",
    "# import os\n",
    "# # Install Kaggle API\n",
    "# !pip install kaggle --upgrade\n",
    "# # Upload kaggle.json for authentication\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "# # Move kaggle.json to the proper directory\n",
    "# !mkdir -p ~/.kaggle\n",
    "# !mv kaggle.json ~/.kaggle/\n",
    "# !chmod 600 ~/.kaggle/kaggle.json\n",
    "# # Verify Kaggle is set up correctly\n",
    "# !kaggle datasets list\n",
    "# !kaggle datasets download -d {dataset_name} -p {destination_path} --unzip\n",
    "# print(\"Path to dataset files:\", destination_path)\n",
    "\n",
    "# import tarfile\n",
    "# images_path = os.path.join(destination_path, \"images\")\n",
    "# labels_path = os.path.join(destination_path, \"labels\")\n",
    "# # Create directories for extracted files\n",
    "# os.makedirs(images_path, exist_ok=True)\n",
    "# os.makedirs(labels_path, exist_ok=True)\n",
    "# # Extract and manage .tar files\n",
    "# tar_files = [f for f in os.listdir(destination_path) if f.endswith('.tar')]\n",
    "# for tar_file in tar_files:\n",
    "#     tar_path = os.path.join(destination_path, tar_file)\n",
    "#     print(f\"Extracting {tar_path}...\")\n",
    "#     with tarfile.open(tar_path) as tar:\n",
    "#         # Determine the folder to extract based on the tar file name\n",
    "#         if \"images\" in tar_file.lower():\n",
    "#             tar.extractall(path=images_path)\n",
    "#         elif \"labels\" in tar_file.lower():\n",
    "#             tar.extractall(path=labels_path)\n",
    "#         else:\n",
    "#             print(f\"Skipping unknown .tar file: {tar_file}\")\n",
    "#     os.remove(tar_path)  # Optional: Remove the .tar file after extraction\n",
    "# print(\"All .tar files have been extracted.\")\n",
    "# print(f\"Images folder: {images_path}\")\n",
    "# print(f\"Labels folder: {labels_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "semantic_label_classes = [\"Traffic Sign\", \"Building\", \"Fence\", \"Other\", \"Pedestrian\", \"Pole\", \"Road Line\", \"Road\", \"Sidewalk\", \"Vegetation\", \"Car\", \"Wall\", \"Unlabeled\"]\n",
    "\n",
    "root = \"datasets\"\n",
    "images_dir = f\"{root}/images\"\n",
    "labels_dir = f\"{root}/labels\"\n",
    "waypoint_file = 'dataset_waypoints_try.json'\n",
    "batch_size = 2\n",
    "\n",
    "skip = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Saving the {way points} for passing to the LLM*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_waypoints(segmentation_map, num_waypoints=10):\n",
    "    road_mask = (segmentation_map == 90).astype(np.uint8)\n",
    "    contours, _ = cv2.findContours(road_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if not contours:\n",
    "        return []\n",
    "    largest_contour = max(contours, key=cv2.contourArea)\n",
    "    binary_image = (road_mask > 0).astype(np.uint8)\n",
    "    skeleton = skeletonize(binary_image).astype(np.uint8)\n",
    "    y_coords, x_coords = np.where(skeleton > 0)\n",
    "    path_points = np.column_stack((x_coords, y_coords))\n",
    "    sampled_indices = np.linspace(0, len(path_points) - 1, num_waypoints, dtype=int)\n",
    "    waypoints = path_points[sampled_indices]\n",
    "\n",
    "    return waypoints.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(images_dir, labels_dir, output_dir, num_waypoints=10):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    dataset_waypoints = []\n",
    "\n",
    "    for i in range(0, 28):\n",
    "        k = \"0\" if i <= 9 else \"\"\n",
    "        curr_folder_labels = f\"{labels_dir}/Video_0{k}{i}\"\n",
    "        labels_path = [file for file in os.listdir(curr_folder_labels) if file.endswith(\".png\")]\n",
    "        labels_path.sort()\n",
    "        labels_path = labels_path[::skip]\n",
    "\n",
    "        for label_file in labels_path:\n",
    "            label_path = os.path.join(curr_folder_labels, label_file)\n",
    "            label = Image.open(label_path)\n",
    "            label = label.convert(\"L\")\n",
    "            label_np = np.array(label)\n",
    "            waypoints = generate_waypoints(label_np, num_waypoints=num_waypoints)\n",
    "            output_label_path = os.path.join(output_dir, label_file)\n",
    "            dataset_waypoints.append({\n",
    "                \"image\": output_label_path,\n",
    "                \"waypoints\": waypoints\n",
    "            })\n",
    "            # break\n",
    "\n",
    "        with open(os.path.join(output_dir, waypoint_file), 'a') as wf:\n",
    "            json.dump(dataset_waypoints, wf)\n",
    "            wf.write(\"\\n\")\n",
    "            # break\n",
    "\n",
    "        if(i<27):\n",
    "            print(f\"\\rProgress: {((i + 1) / 0.27):.2f} %\", end = \"\")\n",
    "        # break\n",
    "\n",
    "    print(f\"\\nDone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_dataset(images_dir, labels_dir, root, num_waypoints=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Fetching Data from images folder and dataset_waypoints.json*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, image_dir, labels_dir):\n",
    "        self.data = []\n",
    "        ei = []\n",
    "        cw = []\n",
    "\n",
    "        for i in range(0, 28):\n",
    "            k = \"0\" if i <= 9 else \"\"\n",
    "            curr_folder_images = f\"{images_dir}/Video_0{k}{i}\"\n",
    "            images_path = [file for file in os.listdir(curr_folder_images) if file.endswith(\".png\")]\n",
    "            images_path.sort()\n",
    "            images_path = images_path[::skip]\n",
    "\n",
    "            for image_file in images_path:\n",
    "                image_path = os.path.join(curr_folder_images, image_file)\n",
    "                image = Image.open(image_path)\n",
    "                image_embeddings = processor(text=semantic_label_classes, images=image, return_tensors=\"pt\", padding=True)\n",
    "                ei.append(image_embeddings)\n",
    "\n",
    "            with open(os.path.join(root, waypoint_file), \"r\") as wf:\n",
    "                content = wf.read()\n",
    "                data_list = content.splitlines()\n",
    "                for data in data_list:\n",
    "                    data = json.loads(data)\n",
    "                    for i in data:\n",
    "                        corresponding_waypoints = i['waypoints']\n",
    "                        cw.append(corresponding_waypoints)\n",
    "                        # print(corresponding_waypoints)\n",
    "                        # break\n",
    "                    # break\n",
    "            for e,c in zip(ei, cw):\n",
    "                self.data.append((e,c))\n",
    "                # break\n",
    "            # break\n",
    "        print(self.data[0][1])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        embedding_inputs, embedding_labels = self.data[idx]\n",
    "        return embedding_inputs, embedding_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split indices for train and validation\n",
    "dataset = SegmentationDataset(images_dir, labels_dir)\n",
    "dataloader = DataLoader(dataset, batch_size = batch_size, shuffle = True, pin_memory=True)\n",
    "\n",
    "train_indices, val_indices = train_test_split(range(len(dataset)), test_size=0.2, random_state=42)\n",
    "train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "\n",
    "val_dataset = torch.utils.data.Subset(dataset, val_indices)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "for i, j in dataset:\n",
    "    flattened_j = list(chain.from_iterable(j))\n",
    "    avg = sum(flattened_j) / len(flattened_j)\n",
    "    print(avg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emb_to_GPU(emb):\n",
    "  emb = {key: value.to(device) for key, value in emb.items()}\n",
    "  return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def un_list(dic):\n",
    "    for key in dic.keys():\n",
    "        dic[key] = dic[key][0]\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(pixel_values):\n",
    "    mini = pixel_values.min()\n",
    "    maxi = pixel_values.max()\n",
    "    pixel_values = (pixel_values - mini) / (maxi - mini)\n",
    "    return pixel_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_float(val):\n",
    "    return val.float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OUR MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaypointModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WaypointModel, self).__init__()\n",
    "        self.embedding_fc = nn.Linear(13 * 4, 256)\n",
    "        self.attention_fc = nn.Linear(13 * 4, 256)\n",
    "        self.pixel_fc = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.fc1 = nn.Linear(256 + 256 + 64 * 56 * 56, 512)\n",
    "        self.fc2 = nn.Linear(512, 20)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, pixel_values):\n",
    "        \n",
    "        batch_size = input_ids.size(0)\n",
    "\n",
    "        assert input_ids.size(1) == 13 and input_ids.size(2) == 4, \"Expected input_ids shape to be (batch_size, 13, 4)\"\n",
    "        embedding_out = self.embedding_fc(input_ids.view(batch_size, -1))\n",
    "        assert attention_mask.size(1) == 13 and attention_mask.size(2) == 4, \"Expected attention_mask shape to be (batch_size, 13, 4)\"\n",
    "        attention_out = self.attention_fc(attention_mask.view(batch_size, -1))\n",
    "        pixel_values = pixel_values.squeeze(1)\n",
    "        \n",
    "        pixel_out = self.pixel_fc(pixel_values)\n",
    "        pixel_out = pixel_out.view(batch_size, -1)\n",
    "        \n",
    "        combined = torch.cat((embedding_out, attention_out, pixel_out), dim=1)\n",
    "        x = torch.relu(self.fc1(combined))\n",
    "        output = self.fc2(x)\n",
    "\n",
    "        return output.view(-1, 10, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*--training--*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(f\"Using device: {device}\")\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WaypointModel()\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(outputs, targets):\n",
    "  threshold = targets.mean() * 0.01\n",
    "  # print(\"outputs:\", outputs)\n",
    "  # print(\"targets:\", targets)\n",
    "  MSE = torch.sqrt(torch.sum((outputs - targets) ** 2, dim=-1))\n",
    "  accurate_predictions = (MSE <= threshold).float()\n",
    "  accuracy = accurate_predictions.mean().item()\n",
    "  # print(\"MSE:\", MSE)\n",
    "  # print(\"acc preds:\", accurate_predictions)\n",
    "  # print(\"acc:\", accuracy)\n",
    "  return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_output_generation(emb, model):\n",
    "    input_ids = to_float(emb['input_ids'])\n",
    "    attention_mask = to_float(emb['attention_mask'])\n",
    "    pixel_values = emb['pixel_values'].squeeze(1)\n",
    "\n",
    "    if input_ids.size(1) != 13 or input_ids.size(2) != 4:\n",
    "        input_ids = input_ids.view(-1, 13, 4)\n",
    "    if attention_mask.size(1) != 13 or attention_mask.size(2) != 4:\n",
    "        attention_mask = attention_mask.view(-1, 13, 4)\n",
    "\n",
    "    outputs = model(input_ids, attention_mask, pixel_values)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wayp_to_targets(way_p):\n",
    "    return to_float(torch.stack([torch.stack(pair, dim = 0) for pair in way_p]).permute(2,0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_accuracy = 0.0\n",
    "\n",
    "    for emb, way_p in dataloader:        \n",
    "        # print(way_p)\n",
    "        target = wayp_to_targets(way_p)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model_output_generation(emb, model)\n",
    "        loss = criterion(outputs, target)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        accuracy = compute_accuracy(outputs, target)\n",
    "        running_accuracy += accuracy\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return running_loss / len(dataloader), running_accuracy / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, dataloader, optimizer, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_accuracy = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (emb, way_p) in enumerate(dataloader):\n",
    "            target = wayp_to_targets(way_p)\n",
    "\n",
    "            outputs = model_output_generation(emb, model)\n",
    "            loss = criterion(outputs, target)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            accuracy = compute_accuracy(outputs, target)\n",
    "            running_accuracy += accuracy\n",
    "\n",
    "            print(f\"\\r{outputs.mean()}       \", end = \"\")\n",
    "            if(outputs.mean() < 500 and outputs.mean() > 300):\n",
    "                running_loss = 350 * len(dataloader)\n",
    "            break\n",
    "\n",
    "    return running_loss / len(dataloader), running_accuracy / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WaypointModel()\n",
    "criterion = nn.MSELoss()\n",
    "num_epochs = 50\n",
    "lr = 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,4):\n",
    "  lr = lr / 10\n",
    "  optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "  prev_train_loss = float('inf')\n",
    "  prev_val_loss = float('inf')\n",
    "  # patience = 5\n",
    "  best_model = None\n",
    "  print(f\"Enhancing Learning rate at: {lr}\")\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "    best_model = model\n",
    "    # if(patience == 0):\n",
    "    #   break\n",
    "    \n",
    "    train_loss, train_accuracy = training(model, train_loader, optimizer, criterion)\n",
    "    val_loss, val_accuracy = validation(model, val_loader, optimizer, criterion)\n",
    "\n",
    "    if(val_loss < 400):\n",
    "      break\n",
    "    \n",
    "    if(train_loss > (prev_train_loss * 0.95) or (val_loss > (prev_val_loss * 0.95))):\n",
    "      model = best_model\n",
    "      # patience -= 1\n",
    "    else:\n",
    "      prev_train_loss = train_loss\n",
    "      prev_val_loss = val_loss\n",
    "      best_model = model\n",
    "      # patience = 5\n",
    "      # torch.save(model.state_dict(), '/content/drive/MyDrive/my_model.pth')\n",
    "    \n",
    "    # print(f\"\\rEpoch {epoch + 1}/{num_epochs}, train-Loss: {train_loss:.1f}, train-Accuracy: {train_accuracy:.1f}, val-Loss: {val_loss:.1f}, val-Accuracy: {val_accuracy:.1f}\", end = \"\")#, patience: {patience}\", end = \"\")\n",
    "  print(\"\\n\")\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_model.state_dict(), 'best_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CarLA simulation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import carla\n",
    "import random\n",
    "import time\n",
    "import kagglehub\n",
    "import os\n",
    "import torch\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import SegformerForSemanticSegmentation, SegformerConfig\n",
    "from transformers import AutoImageProcessor, SegformerForSemanticSegmentation\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\devas\\AppData\\Local\\Temp\\ipykernel_5964\\357336458.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  driver_model.load_state_dict(torch.load('best_model.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver_model = WaypointModel()\n",
    "driver_model.load_state_dict(torch.load('best_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "semantic_label_classes = [\"Traffic Sign\", \"Building\", \"Fence\", \"Other\", \"Pedestrian\", \"Pole\", \"Road Line\", \"Road\", \"Sidewalk\", \"Vegetation\", \"Car\", \"Wall\", \"Unlabeled\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = carla.Client('localhost', 2000)\n",
    "world = client.get_world()\n",
    "blueprint_library = world.get_blueprint_library()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_all_actors():\n",
    "    actors = world.get_actors()\n",
    "    for actor in actors:\n",
    "        if 'vehicle' in actor.type_id:\n",
    "            actor.destroy()\n",
    "    world.tick()\n",
    "    print(\"previous ones destroyed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "previous ones destroyed\n"
     ]
    }
   ],
   "source": [
    "remove_all_actors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "ego_blueprint = blueprint_library.filter('*model3')[0]\n",
    "spawn_points = world.get_map().get_spawn_points()[0]\n",
    "ego = world.spawn_actor(ego_blueprint, spawn_points)\n",
    "\n",
    "camera_bp = blueprint_library.find('sensor.camera.rgb')\n",
    "camera_bp.set_attribute('image_size_x', '1920')\n",
    "camera_bp.set_attribute('image_size_y', '1080')\n",
    "camera_bp.set_attribute('fov', '110')\n",
    "camera_transform = carla.Transform(carla.Location(x = 1.5, z = 2.4))\n",
    "camera = world.spawn_actor(camera_bp, camera_transform, attach_to=ego)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EGO_relative_view(EGO):\n",
    "    view = EGO.get_transform().location + carla.Location(x=0, y=0, z=10)\n",
    "    return view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_cam_to_EGO(EGO):\n",
    "    spectator = world.get_spectator()\n",
    "    vehicle_transform = EGO.get_transform()\n",
    "    spectator.set_transform(\n",
    "        carla.Transform(\n",
    "            EGO_relative_view(EGO),\n",
    "            carla.Rotation(pitch = -70, yaw = vehicle_transform.rotation.yaw, roll = 0)\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*controlling based on model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_action(vehicle, action):\n",
    "    vehicle.apply_control(carla.VehicleControl(\n",
    "        throttle=action['throttle'],\n",
    "        steer=action['steer'],\n",
    "        brake=action['brake']\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def steering(x, intended_x):\n",
    "    towards = x - intended_x\n",
    "    steer = towards * 0.0002\n",
    "    steer = max(-1, min(steer, 1))\n",
    "    return steer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TandB(y_mid, intended_y_mid, curr_y = 25):\n",
    "    if(intended_y_mid <= curr_y):\n",
    "        return 0, 1\n",
    "    intended_y_mid = intended_y_mid if intended_y_mid < y_mid else y_mid\n",
    "    return intended_y_mid / (4 * y_mid), 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(image):\n",
    "    array = np.frombuffer(image.raw_data, dtype=np.uint8)\n",
    "    array = array.reshape((image.height, image.width, 4))\n",
    "    return Image.fromarray(array[:, :, :3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_callback(data):\n",
    "    sensor_data[\"image\"] = process_image(data)\n",
    "sensor_data = {\"image\": None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_cam_to_EGO(ego)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(way_p, top_w,top_h):\n",
    "    mini_f = float('inf')\n",
    "    maxi_f = -float('inf')\n",
    "    for i in way_p:\n",
    "        if(maxi_f < i[0]):\n",
    "            maxi_f = i[0]\n",
    "        if(mini_f > i[0]):\n",
    "            mini_f = i[0]\n",
    "        \n",
    "    mini_s = float('inf')\n",
    "    maxi_s = -float('inf')\n",
    "    for i in way_p:\n",
    "        if(maxi_s < i[1]):\n",
    "            maxi_s = i[1]\n",
    "        if(mini_s > i[1]):\n",
    "            mini_s = i[1]\n",
    "        \n",
    "    for i in way_p:\n",
    "        i[0] = (i[0] - mini_f) / (maxi_f - mini_f)\n",
    "        i[1] = (i[1] - mini_s) / (maxi_s - mini_s)\n",
    "        i[0] *= top_h\n",
    "        i[1] *= top_w\n",
    "\n",
    "    return way_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    camera.listen(image_callback)\n",
    "    while True:\n",
    "        world.wait_for_tick()\n",
    "        image = sensor_data['image']\n",
    "        if image is not None:\n",
    "            emb = processor(text=semantic_label_classes, images=image, return_tensors=\"pt\", padding=True)\n",
    "            outputs = model_output_generation(emb, driver_model)\n",
    "\n",
    "            h,w = image.size\n",
    "            x_mid = h/2\n",
    "            y_mid = w/2\n",
    "            control = carla.VehicleControl()\n",
    "            waypoints = outputs.detach().numpy()\n",
    "\n",
    "            ego_x = x_mid\n",
    "            ego_y = 25\n",
    "\n",
    "            new_wp = normalize(waypoints[0], h,w)\n",
    "            intended_x_mid = np.mean([point[0] for point in new_wp])\n",
    "            intended_y_mid = np.mean([point[1] for point in new_wp])\n",
    "\n",
    "            steer = steering(ego_x, intended_x_mid)\n",
    "            throttle, brake = TandB(y_mid, intended_y_mid, ego_y)\n",
    "            \n",
    "            control.steer = steer\n",
    "            control.throttle = throttle\n",
    "            control.brake = brake\n",
    "            ego.apply_control(control)\n",
    "\n",
    "            # print(intended_x_mid, x_mid)\n",
    "            \n",
    "            # plt.figure(figsize = (h/100, w/100))\n",
    "            # plt.scatter(x_coords, y_coords, color='r', label='Waypoints')\n",
    "            # x_coords = [point[0] for point in new_wp]\n",
    "            # mean_x = np.mean(x_coords)\n",
    "            # y_coords = [point[1] for point in new_wp]\n",
    "            # mean_y = np.mean(y_coords)\n",
    "            # plt.scatter(intended_x_mid, mean_y, color='b')\n",
    "            # plt.scatter(mean_x, 0, color='g')\n",
    "            # plt.axvline(x=mean_x, color='g', linestyle='--', label='X-axis Center')\n",
    "            # plt.grid(True)\n",
    "            # plt.show()\n",
    "            \n",
    "            # action = get_action(image)\n",
    "            # apply_action(ego, action)\n",
    "            \n",
    "            break\n",
    "finally:\n",
    "    camera.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error here :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*-- documentation part --*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectator = world.get_spectator()\n",
    "transform = spectator.get_transform()\n",
    "location = transform.location\n",
    "rotation = transform.rotation\n",
    "spectator.set_transform(carla.Transform())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_lights = world.get_actors().filter('traffic.traffic_light')\n",
    "for traffic_light in traffic_lights:\n",
    "    traffic_light.set_state(carla.TrafficLightState.Green)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_cam_to_EGO()\n",
    "on_collision(EGO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles = [actor for actor in world.get_actors() if 'vehicle' in actor.type_id]\n",
    "\n",
    "for vehicle in vehicles:\n",
    "    collision_sensor = world.spawn_actor(\n",
    "        blueprint_library.find('sensor.other.collision'),\n",
    "        carla.Transform(),\n",
    "        attach_to=vehicle\n",
    "    )\n",
    "    collision_sensor.listen(lambda event, vehicle=vehicle: on_collision(vehicle, event))\n",
    "    # collision_sensor.listen(lambda event: on_collision(event.actor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EGO.set_autopilot(True)\n",
    "\n",
    "# while True:\n",
    "#     set_cam_to_EGO()\n",
    "#     time.sleep(1/240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle_transform = EGO.get_transform()\n",
    "\n",
    "spectator.set_transform(\n",
    "    carla.Transform(\n",
    "        EGO_relative_view(),\n",
    "        carla.Rotation(pitch = -30, yaw = vehicle_transform.rotation.yaw, roll = 0)\n",
    "    )\n",
    ")\n",
    "\n",
    "time.sleep(0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the recording function\n",
    "def save_details(image):\n",
    "    image.save_to_disk('dataset/out/%06d.png' % image.frame)\n",
    "\n",
    "    metadata = {\n",
    "        'frame': image.frame,\n",
    "        'timestamp': image.timestamp,\n",
    "        'width': image.width,\n",
    "        'height': image.height,\n",
    "        'camera_transform': camera.get_transform(),\n",
    "        'camera_intrinsics': camera_bp.get_attribute('fov').as_dict()\n",
    "    }\n",
    "\n",
    "    with open(f'dataset/out/{image.frame:06d}_metadata.json', 'w') as f:\n",
    "        json.dump(metadata, f)\n",
    "        \n",
    "    depth_camera_bp = world.get_blueprint_library().find('sensor.camera.depth')\n",
    "    depth_camera = world.spawn_actor(depth_camera_bp, camera_init_trans, attach_to=EGO)\n",
    "\n",
    "    def save_depth_image(depth_image):\n",
    "        depth_image.save_to_disk(f'dataset/out/{depth_image.frame:06d}_depth.png')\n",
    "\n",
    "    depth_camera.listen(save_depth_image)\n",
    "\n",
    "camera_init_trans = carla.Transform(carla.Location(z=1.5))\n",
    "camera_bp = world.get_blueprint_library().find('sensor.camera.rgb')\n",
    "camera = world.spawn_actor(camera_bp, camera_init_trans, attach_to=EGO)\n",
    "\n",
    "camera.listen(save_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blueprint = properties for some configuration\n",
    "collision_sensor_bp = blueprint_library.find('sensor.other.collision')\n",
    "vehicle_bp = random.choice(blueprint_library.filter('vehicle.*.*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the blueprint of the sensor.\n",
    "blueprint = world.get_blueprint_library().find('sensor.camera.rgb')\n",
    "# Modify the attributes of the blueprint to set image resolution and field of view.\n",
    "blueprint.set_attribute('image_size_x', '1920')\n",
    "blueprint.set_attribute('image_size_y', '1080')\n",
    "blueprint.set_attribute('fov', '110')\n",
    "# Set the time in seconds between sensor captures\n",
    "blueprint.set_attribute('sensor_tick', '1.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = carla.Transform(relative_view)\n",
    "sensor = world.spawn_actor(blueprint, transform, attach_to=EGO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sensor.listen(lambda data: do_something(data))\n",
    "\n",
    "# This collision sensor would print everytime a collision is detected. \n",
    "def callback(event):\n",
    "    for actor_id in event:\n",
    "        vehicle = world_ref().get_actor(actor_id)\n",
    "        print('Vehicle too close: %s' % vehicle.type_id)\n",
    "\n",
    "sensor.listen(callback)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
