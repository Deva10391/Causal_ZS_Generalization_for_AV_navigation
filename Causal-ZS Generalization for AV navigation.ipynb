{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ALGORITHM 1: VL pretrained-M fine tuned with semi-disentangled outputs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import torch\n",
    "import os\n",
    "import kagglehub\n",
    "import pickle\n",
    "import math\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "from sklearn.decomposition import PCA\n",
    "from skimage.morphology import skeletonize\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import CLIPProcessor, CLIPModel, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from torchvision import transforms as A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = kagglehub.dataset_download(\"albertozorzetto/carla-densely-annotated-driving-dataset\")\n",
    "# print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Creation (IMAGE form to EMBEDDING form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Download the CARLA dataset using the Kaggle API\n",
    "# dataset_name = \"albertozorzetto/carla-densely-annotated-driving-dataset\"\n",
    "# destination_path = \"/content/datasets\"\n",
    "\n",
    "# import os\n",
    "# # Install Kaggle API\n",
    "# !pip install kaggle --upgrade\n",
    "# # Upload kaggle.json for authentication\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "# # Move kaggle.json to the proper directory\n",
    "# !mkdir -p ~/.kaggle\n",
    "# !mv kaggle.json ~/.kaggle/\n",
    "# !chmod 600 ~/.kaggle/kaggle.json\n",
    "# # Verify Kaggle is set up correctly\n",
    "# !kaggle datasets list\n",
    "# !kaggle datasets download -d {dataset_name} -p {destination_path} --unzip\n",
    "# print(\"Path to dataset files:\", destination_path)\n",
    "\n",
    "# import tarfile\n",
    "# images_path = os.path.join(destination_path, \"images\")\n",
    "# labels_path = os.path.join(destination_path, \"labels\")\n",
    "# # Create directories for extracted files\n",
    "# os.makedirs(images_path, exist_ok=True)\n",
    "# os.makedirs(labels_path, exist_ok=True)\n",
    "# # Extract and manage .tar files\n",
    "# tar_files = [f for f in os.listdir(destination_path) if f.endswith('.tar')]\n",
    "# for tar_file in tar_files:\n",
    "#     tar_path = os.path.join(destination_path, tar_file)\n",
    "#     print(f\"Extracting {tar_path}...\")\n",
    "#     with tarfile.open(tar_path) as tar:\n",
    "#         # Determine the folder to extract based on the tar file name\n",
    "#         if \"images\" in tar_file.lower():\n",
    "#             tar.extractall(path=images_path)\n",
    "#         elif \"labels\" in tar_file.lower():\n",
    "#             tar.extractall(path=labels_path)\n",
    "#         else:\n",
    "#             print(f\"Skipping unknown .tar file: {tar_file}\")\n",
    "#     os.remove(tar_path)  # Optional: Remove the .tar file after extraction\n",
    "# print(\"All .tar files have been extracted.\")\n",
    "# print(f\"Images folder: {images_path}\")\n",
    "# print(f\"Labels folder: {labels_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "semantic_label_classes = [\"Traffic Sign\", \"Building\", \"Fence\", \"Other\", \"Pedestrian\", \"Pole\", \"Road Line\", \"Road\", \"Sidewalk\", \"Vegetation\", \"Car\", \"Wall\", \"Unlabeled\"]\n",
    "\n",
    "root = \"datasets\"\n",
    "images_dir = f\"{root}/images\"\n",
    "labels_dir = f\"{root}/labels\"\n",
    "waypoint_file = 'dataset_waypoints_try.json'\n",
    "batch_size = 2\n",
    "\n",
    "skip = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Saving the {way points} for passing to the LLM*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_waypoints(segmentation_map, num_waypoints=10):\n",
    "    road_mask = (segmentation_map == 90).astype(np.uint8)\n",
    "    contours, _ = cv2.findContours(road_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if not contours:\n",
    "        return []\n",
    "    largest_contour = max(contours, key=cv2.contourArea)\n",
    "    binary_image = (road_mask > 0).astype(np.uint8)\n",
    "    skeleton = skeletonize(binary_image).astype(np.uint8)\n",
    "    y_coords, x_coords = np.where(skeleton > 0)\n",
    "    path_points = np.column_stack((x_coords, y_coords))\n",
    "    sampled_indices = np.linspace(0, len(path_points) - 1, num_waypoints, dtype=int)\n",
    "    waypoints = path_points[sampled_indices]\n",
    "\n",
    "    return waypoints.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(images_dir, output_dir, num_waypoints=10):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    dataset_waypoints = []\n",
    "\n",
    "    for i in range(0, 28):\n",
    "        k = \"0\" if i <= 9 else \"\"\n",
    "        curr_folder_images = f\"{images_dir}/Video_0{k}{i}\"\n",
    "        images_path = [file for file in os.listdir(curr_folder_images) if file.endswith(\".png\")]\n",
    "        images_path.sort()\n",
    "        # images_path = images_path[::skip]\n",
    "\n",
    "        for image_file in images_path:\n",
    "            image_path = os.path.join(curr_folder_images, image_file)\n",
    "            image = Image.open(image_path)\n",
    "            image = image.convert(\"L\")\n",
    "            image_np = np.array(image)\n",
    "            waypoints = generate_waypoints(image_np, num_waypoints=num_waypoints)\n",
    "            output_image_path = os.path.join(output_dir, image_file)\n",
    "            dataset_waypoints.append({\n",
    "                \"image\": output_image_path,\n",
    "                \"waypoints\": waypoints\n",
    "            })\n",
    "            # break\n",
    "\n",
    "        with open(os.path.join(output_dir, waypoint_file), 'a') as wf:\n",
    "            json.dump(dataset_waypoints, wf)\n",
    "            wf.write(\"\\n\")\n",
    "            # break\n",
    "\n",
    "        if(i<27):\n",
    "            print(f\"\\rProgress: {((i + 1) / 0.27):.2f} %\", end = \"\")\n",
    "        # break\n",
    "\n",
    "    print(f\"\\nDone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_dataset(images_dir, root, num_waypoints=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Fetching Data from images folder and dataset_waypoints.json*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, image_dir, labels_dir):\n",
    "        self.data = []\n",
    "        ei = []\n",
    "        cw = []\n",
    "\n",
    "        for i in range(0, 28):\n",
    "            k = \"0\" if i <= 9 else \"\"\n",
    "            curr_folder_images = f\"{images_dir}/Video_0{k}{i}\"\n",
    "            images_path = [file for file in os.listdir(curr_folder_images) if file.endswith(\".png\")]\n",
    "            images_path.sort()\n",
    "            images_path = images_path[::skip]\n",
    "\n",
    "            for image_file in images_path:\n",
    "                image_path = os.path.join(curr_folder_images, image_file)\n",
    "                image = Image.open(image_path)\n",
    "                image_embeddings = processor(text=semantic_label_classes, images=image, return_tensors=\"pt\", padding=True)\n",
    "                ei.append(image_embeddings['pixel_values'])\n",
    "\n",
    "            with open(os.path.join(root, waypoint_file), \"r\") as wf:\n",
    "                content = wf.read()\n",
    "                data_list = content.splitlines()\n",
    "                for data in data_list:\n",
    "                    data = json.loads(data)\n",
    "                    for i in data:\n",
    "                        corresponding_waypoints = i['waypoints']\n",
    "                        cw.append(corresponding_waypoints)\n",
    "                        # print(corresponding_waypoints)\n",
    "                        # break\n",
    "                    # break\n",
    "            for e,c in zip(ei, cw):\n",
    "                self.data.append((e,c))\n",
    "                # break\n",
    "            break\n",
    "        # print(self.data[0][1])\n",
    "        # print(self.data[0][0])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        embedding_inputs, embedding_labels = self.data[idx]\n",
    "        return embedding_inputs, embedding_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split indices for train and validation\n",
    "dataset = SegmentationDataset(images_dir, labels_dir)\n",
    "dataloader = DataLoader(dataset, batch_size = batch_size, shuffle = True, pin_memory=True)\n",
    "\n",
    "train_indices, val_indices = train_test_split(range(len(dataset)), test_size=0.2, random_state=42)\n",
    "train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "\n",
    "val_dataset = torch.utils.data.Subset(dataset, val_indices)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(pixel_values):\n",
    "    mini = pixel_values.min()\n",
    "    maxi = pixel_values.max()\n",
    "    pixel_values = (pixel_values - mini) / (maxi - mini)\n",
    "    return pixel_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OUR MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaypointModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WaypointModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(512)\n",
    "        self.conv3 = nn.Conv2d(512, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "        self.conv5 = nn.Conv2d(512, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 512)\n",
    "        self.fc2 = nn.Linear(512, 40)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        x = torch.relu(self.conv1(pixel_values))\n",
    "        x = self.bn1(x)\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        \n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.bn2(x)\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = self.bn3(x)\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "\n",
    "        x = torch.relu(self.conv4(x))\n",
    "        x = self.bn4(x)\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "\n",
    "        x = torch.relu(self.conv5(x))\n",
    "        x = self.bn5(x)\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        output = self.fc2(x)\n",
    "        return output.view(-1, 10, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_float(val):\n",
    "    return val.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_output_generation(pixel_values, model):\n",
    "    pixel_values = pixel_values.squeeze(1)\n",
    "    outputs = model(pixel_values)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*--training--*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(f\"Using device: {device}\")\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WaypointModel()\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(outputs, targets):\n",
    "  threshold = targets.mean() * 0.01\n",
    "  # print(\"outputs:\", outputs)\n",
    "  # print(\"targets:\", targets)\n",
    "  MSE = torch.sqrt(torch.sum((outputs - targets) ** 2, dim=-1))\n",
    "  accurate_predictions = (MSE <= threshold).float()\n",
    "  accuracy = accurate_predictions.mean().item()\n",
    "  # print(\"MSE:\", MSE)\n",
    "  # print(\"acc preds:\", accurate_predictions)\n",
    "  # print(\"acc:\", accuracy)\n",
    "  return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wayp_to_targets(way_p):\n",
    "    # print(way_p)\n",
    "    return to_float(torch.stack([torch.stack(pair, dim = 0) for pair in way_p]).permute(2,0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataloader, optimizer, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_accuracy = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (p_val, way_p) in enumerate(dataloader_1):\n",
    "            target = wayp_to_targets(way_p)\n",
    "            print(target)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model_output_generation(p_val[0], model)\n",
    "            print(outputs)\n",
    "            loss = criterion(outputs, target)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            accuracy = compute_accuracy(outputs, target)\n",
    "            running_accuracy += accuracy\n",
    "            \n",
    "            # print(f\"\\r{running_loss:.1f}, {running_accuracy:.1f}\")\n",
    "            break\n",
    "\n",
    "test(model, dataloader_1, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_accuracy = 0.0\n",
    "\n",
    "    for i, (p_val, way_p) in enumerate(dataloader):\n",
    "        target = wayp_to_targets(way_p)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model_output_generation(p_val[0], model)\n",
    "        loss = criterion(outputs, target)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        accuracy = compute_accuracy(outputs, target)\n",
    "        running_accuracy += accuracy\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # break\n",
    "\n",
    "    return running_loss / len(dataloader), running_accuracy / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, dataloader, optimizer, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_accuracy = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (p_val, way_p) in enumerate(dataloader):\n",
    "            target = wayp_to_targets(way_p)\n",
    "\n",
    "            outputs = model_output_generation(p_val[0], model)\n",
    "            loss = criterion(outputs, target)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            accuracy = compute_accuracy(outputs, target)\n",
    "            running_accuracy += accuracy\n",
    "            # break\n",
    "\n",
    "    return running_loss / len(dataloader), running_accuracy / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WaypointModel()\n",
    "criterion = nn.MSELoss()\n",
    "num_epochs = 50\n",
    "lr = 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split indices for train and validation\n",
    "dataset_1 = SegmentationDataset(images_dir, labels_dir)\n",
    "dataloader_1 = DataLoader(dataset_1, batch_size = batch_size, shuffle = True, pin_memory=True)\n",
    "\n",
    "train_indices, val_indices = train_test_split(range(len(dataset_1)), test_size=0.2, random_state=42)\n",
    "train_dataset_1 = torch.utils.data.Subset(dataset_1, train_indices)\n",
    "train_loader_1 = DataLoader(train_dataset_1, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "\n",
    "val_dataset_1 = torch.utils.data.Subset(dataset_1, val_indices)\n",
    "val_loader_1 = DataLoader(val_dataset_1, batch_size=batch_size, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = None\n",
    "for i in range(0,4):\n",
    "  lr = lr / 10\n",
    "  optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "  prev_train_loss = float('inf')\n",
    "  prev_val_loss = float('inf')\n",
    "  # patience = 5\n",
    "  print(f\"Enhancing Learning rate at: {lr}\")\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "    best_model = model\n",
    "    # if(patience == 0):\n",
    "    #   break\n",
    "    \n",
    "    train_loss, train_accuracy = training(model, train_loader_1, optimizer, criterion)\n",
    "    val_loss, val_accuracy = validation(model, val_loader_1, optimizer, criterion)\n",
    "    \n",
    "    if(train_loss > (prev_train_loss * 0.95) or (val_loss > (prev_val_loss * 0.95))):\n",
    "      model = best_model\n",
    "      # patience -= 1\n",
    "    else:\n",
    "      prev_train_loss = train_loss\n",
    "      prev_val_loss = val_loss\n",
    "      best_model = model\n",
    "      # patience = 5\n",
    "      # torch.save(model.state_dict(), '/content/drive/MyDrive/my_model.pth')\n",
    "    print(f\"\\rEpoch {epoch + 1}/{num_epochs}, train-Loss: {train_loss:.1f}, val-Loss: {val_loss:.1f}     \", end = \"\")#, patience: {patience}\", end = \"\")\n",
    "    # break\n",
    "  print(\"\\n\")\n",
    "  # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_model.state_dict(), 'best_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "redundent code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import torch\n",
    "import os\n",
    "import kagglehub\n",
    "import pickle\n",
    "import math\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "from sklearn.decomposition import PCA\n",
    "from skimage.morphology import skeletonize\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import CLIPProcessor, CLIPModel, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from torchvision import transforms as A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaypointModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WaypointModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(512)\n",
    "        self.conv3 = nn.Conv2d(512, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "        self.conv5 = nn.Conv2d(512, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 512)\n",
    "        self.fc2 = nn.Linear(512, 40)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        x = torch.relu(self.conv1(pixel_values))\n",
    "        x = self.bn1(x)\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        \n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.bn2(x)\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = self.bn3(x)\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "\n",
    "        x = torch.relu(self.conv4(x))\n",
    "        x = self.bn4(x)\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "\n",
    "        x = torch.relu(self.conv5(x))\n",
    "        x = self.bn5(x)\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        output = self.fc2(x)\n",
    "        return output.view(-1, 10, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_float(val):\n",
    "    return val.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_output_generation(pixel_values, model):\n",
    "    pixel_values = pixel_values.squeeze(1)\n",
    "    outputs = model(pixel_values)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CarLA simulation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import carla\n",
    "import random\n",
    "import time\n",
    "import kagglehub\n",
    "import os\n",
    "import torch\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import SegformerForSemanticSegmentation, SegformerConfig\n",
    "from transformers import AutoImageProcessor, SegformerForSemanticSegmentation\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\devas\\AppData\\Local\\Temp\\ipykernel_18452\\334242544.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  driver_model.load_state_dict(torch.load('best_model.pth', map_location=torch.device('cpu')))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver_model = WaypointModel()\n",
    "driver_model.load_state_dict(torch.load('best_model.pth', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_output_generation(pixel_values, model):\n",
    "    pixel_values = pixel_values.squeeze(1)\n",
    "    outputs = model(pixel_values)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "semantic_label_classes = [\"Traffic Sign\", \"Building\", \"Fence\", \"Other\", \"Pedestrian\", \"Pole\", \"Road Line\", \"Road\", \"Sidewalk\", \"Vegetation\", \"Car\", \"Wall\", \"Unlabeled\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = carla.Client('localhost', 2000)\n",
    "world = client.get_world()\n",
    "blueprint_library = world.get_blueprint_library()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "netstat -an | findstr 2000\n",
    "ping localhost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_all_actors():\n",
    "    actors = world.get_actors()\n",
    "    for actor in actors:\n",
    "        if 'vehicle' in actor.type_id:\n",
    "            actor.destroy()\n",
    "    world.tick()\n",
    "    print(\"previous ones destroyed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "previous ones destroyed\n"
     ]
    }
   ],
   "source": [
    "remove_all_actors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ego_blueprint = blueprint_library.filter('*model3')[0]\n",
    "spawn_points = world.get_map().get_spawn_points()[0]\n",
    "ego = world.spawn_actor(ego_blueprint, spawn_points)\n",
    "\n",
    "camera_bp = blueprint_library.find('sensor.camera.rgb')\n",
    "camera_bp.set_attribute('image_size_x', '1920')\n",
    "camera_bp.set_attribute('image_size_y', '1080')\n",
    "camera_bp.set_attribute('fov', '110')\n",
    "camera_transform = carla.Transform(carla.Location(x = 1.5, z = 2.4))\n",
    "camera = world.spawn_actor(camera_bp, camera_transform, attach_to=ego)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EGO_relative_view(EGO):\n",
    "    view = EGO.get_transform().location + carla.Location(x=0, y=0, z=10)\n",
    "    return view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_cam_to_EGO(EGO, angle = 55):\n",
    "    spectator = world.get_spectator()\n",
    "    vehicle_transform = EGO.get_transform()\n",
    "    spectator.set_transform(\n",
    "        carla.Transform(\n",
    "            EGO_relative_view(EGO),\n",
    "            carla.Rotation(pitch = -angle, yaw = vehicle_transform.rotation.yaw, roll = 0)\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*functions to control*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_action(vehicle, action):\n",
    "    vehicle.apply_control(carla.VehicleControl(\n",
    "        throttle=action['throttle'],\n",
    "        steer=action['steer'],\n",
    "        brake=action['brake']\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def steering(x, intended_x):\n",
    "    towards = intended_x - x\n",
    "    steer = towards * 0.0004\n",
    "    steer = max(-1, min(steer, 1))\n",
    "    return steer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TandB(y_mid, intended_y_mid, curr_y = 25):\n",
    "    if(intended_y_mid <= curr_y):\n",
    "        return 0, 1\n",
    "    intended_y_mid = intended_y_mid if intended_y_mid < y_mid else y_mid\n",
    "    return intended_y_mid / (1.75 * y_mid), 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(image):\n",
    "    array = np.frombuffer(image.raw_data, dtype=np.uint8)\n",
    "    array = array.reshape((image.height, image.width, 4))\n",
    "    return Image.fromarray(array[:, :, :3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_callback(data):\n",
    "    sensor_image = process_image(data)\n",
    "sensor_image = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_emb(image):\n",
    "    global emb, image_1\n",
    "    image_1 = image\n",
    "    image = process_image(image)\n",
    "    emb = processor(text=semantic_label_classes, images=image, return_tensors=\"pt\", padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(way_p_1, top_w,top_h):\n",
    "    way_p = way_p_1\n",
    "\n",
    "    mini_f = float('inf')\n",
    "    maxi_f = -float('inf')\n",
    "    for i in way_p[0]:\n",
    "        if(maxi_f < i[0]):\n",
    "            maxi_f = i[0]\n",
    "        if(mini_f > i[0]):\n",
    "            mini_f = i[0]\n",
    "        \n",
    "    mini_s = float('inf')\n",
    "    maxi_s = -float('inf')\n",
    "    for i in way_p[1]:\n",
    "        if(maxi_s < i[1]):\n",
    "            maxi_s = i[1]\n",
    "        if(mini_s > i[1]):\n",
    "            mini_s = i[1]\n",
    "        \n",
    "    for i in way_p:\n",
    "        i[0] = (i[0] - mini_f) / (maxi_f - mini_f)\n",
    "        i[1] = (i[1] - mini_s) / (maxi_s - mini_s)\n",
    "        i[0] *= top_h\n",
    "        i[1] *= top_w\n",
    "\n",
    "    return way_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_cam_to_EGO(ego)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*controlling based on model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = None\n",
    "prev_emb = None\n",
    "image_1 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "control = carla.VehicleControl()\n",
    "control.steer = 0.0\n",
    "control.throttle = 0.4\n",
    "control.brake = 0.0\n",
    "ego.apply_control(control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.listen(image_to_emb)\n",
    "\n",
    "while emb is None:\n",
    "    world.tick()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_emb = emb.clone() if isinstance(emb, torch.Tensor) else emb\n",
    "while True:\n",
    "    ego.apply_control(control)\n",
    "    if not torch.equal(prev_emb, emb):\n",
    "        world.wait_for_tick()\n",
    "        if prev_emb and not torch.equal(prev_emb, emb):\n",
    "            i +=1\n",
    "            print(f\"\\rchanged {i} times\", end = \"\")\n",
    "            prev_emb = emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_controls(control):\n",
    "    print(f\"\\r{control.steer}, {control.throttle}, {control.brake}\", end = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running: at: (960.0, 25.0), mid: (960.0, 540.0), going towards: (673.8, -27.8), with b:1, t:0.0, s:-0.11"
     ]
    }
   ],
   "source": [
    "# debug start\n",
    "\n",
    "control = carla.VehicleControl()\n",
    "control.steer = 0.0\n",
    "control.throttle = 1\n",
    "control.brake = 0.0\n",
    "ego.apply_control(control)\n",
    "\n",
    "start_time = time.time()\n",
    "pe = {}\n",
    "e = {}\n",
    "idx = 0\n",
    "\n",
    "set_cam_to_EGO(ego)\n",
    "camera.listen(image_to_emb) #changes emb, image_1 globally\n",
    "\n",
    "while emb is None:\n",
    "    world.tick()\n",
    "\n",
    "if prev_emb:\n",
    "    for i in prev_emb.keys():\n",
    "        pe[i] = prev_emb[i]\n",
    "for i in emb.keys():\n",
    "    e[i] = emb[i]\n",
    "\n",
    "while True:\n",
    "    world.wait_for_tick()\n",
    "    e = emb['pixel_values']\n",
    "    # print(f\"unique emb['pixel_values]: {e}\")\n",
    "    if prev_emb:\n",
    "        pe = prev_emb['pixel_values']\n",
    "        # print(f\"unique prev_emb['pixel_values]: {pe}\")\n",
    "        if not torch.equal(e,pe):\n",
    "\n",
    "            outputs = model_output_generation(e, driver_model)\n",
    "            waypoints = outputs.detach().numpy()\n",
    "\n",
    "            scene_h, scene_w = process_image(image_1).size\n",
    "            ego_x_mid = scene_h/2\n",
    "            ego_y_mid = scene_w/2\n",
    "\n",
    "            ego_x_position = ego_x_mid\n",
    "            ego_y_position = 25\n",
    "\n",
    "            new_wp = normalize(waypoints, scene_w, scene_h)\n",
    "            intended_x_mid = np.mean([point[1] for point in waypoints])\n",
    "            intended_y_mid = np.mean([point[0] for point in waypoints])\n",
    "\n",
    "            steer = steering(ego_x_position, intended_x_mid)\n",
    "            throttle, brake = TandB(ego_y_mid, intended_y_mid, ego_y_position)\n",
    "\n",
    "            # print(outputs, waypoints, new_wp)\n",
    "            print(f\"\\rrunning: at: ({ego_x_position:.1f}, {ego_y_position:.1f}), mid: ({ego_x_mid:.1f}, {ego_y_mid:.1f}), going towards: ({intended_x_mid:.1f}, {intended_y_mid:.1f}), with b:{brake}, t:{throttle:.1f}, s:{steer:.1f}\", end = \"\")\n",
    "\n",
    "            control = carla.VehicleControl()\n",
    "            waypoints = outputs.detach().numpy()\n",
    "            control.steer = steer\n",
    "            control.throttle = throttle\n",
    "            control.brake = brake\n",
    "            ego.apply_control(control)\n",
    "            set_cam_to_EGO(ego)\n",
    "            \n",
    "    if(time.time() >= start_time + 50):\n",
    "        break\n",
    "    prev_emb = emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopped\n",
      "previous ones destroyed\n"
     ]
    }
   ],
   "source": [
    "# debug stop\n",
    "print(\"stopped\")\n",
    "\n",
    "camera.stop()\n",
    "remove_all_actors()\n",
    "\n",
    "ego_blueprint = blueprint_library.filter('*model3')[0]\n",
    "spawn_points = world.get_map().get_spawn_points()[0]\n",
    "ego = world.spawn_actor(ego_blueprint, spawn_points)\n",
    "\n",
    "camera_bp = blueprint_library.find('sensor.camera.rgb')\n",
    "camera_bp.set_attribute('image_size_x', '1920')\n",
    "camera_bp.set_attribute('image_size_y', '1080')\n",
    "camera_bp.set_attribute('fov', '110')\n",
    "camera_transform = carla.Transform(carla.Location(x = 1.5, z = 2.4))\n",
    "camera = world.spawn_actor(camera_bp, camera_transform, attach_to=ego)\n",
    "\n",
    "set_cam_to_EGO(ego, 75)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
